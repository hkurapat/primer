[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Preceptor’s Primer for Bayesian Data Science: Using the Cardinal Virtues for Inference",
    "section": "",
    "text": "Welcome\n\n\nThe Primer is being re-written. Consider it all to be a draft, at least for now. There are three major changes in progress. First, I am switching from rstanarm to brms. (If you disagree with this change, please let me know!) Second, I am standardizing the usage of the Cardinal Virtues. Third, I am using AI tools like ChatGPT and Claude to help with both writing and coding.\n\n\n\n\n\n\n\n\n \nThis isn’t the book you’re looking for.\n\nFirst, the book is for students in my classes. Everything about the book is designed to make the experience of those students better. I hope that some of the material here may be useful to people outside of this class.\nSecond, the book changes all the time. It is as up-to-date as possible.\nThird, I am highly opinionated about what matters and what does not. You might not share my views.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "preamble.html",
    "href": "preamble.html",
    "title": "Preamble",
    "section": "",
    "text": "Dedication\nAnd what is romantic, Kay —\nAnd what is love?\nNeed we ask anyone to tell us these things?",
    "crumbs": [
      "Preamble"
    ]
  },
  {
    "objectID": "preamble.html#acknowledgements",
    "href": "preamble.html#acknowledgements",
    "title": "Preamble",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis work builds on the contributions of many people in the R and Open Source communities. In particular, I would like to acknowledge extensive material taken from Diez, Barr, and Çetinkaya-Rundel (2014), Grolemund and Wickham (2017), Irizarry (2019), Kim and Ismay (2019), Bryan (2019), Diez, Barr, and Çetinkaya-Rundel (2014), Downey (2012), Grolemund and Wickham (2017), Kuhn and Silge (2020), Timbers, Campbell, and Lee (2021), and Legler and Roback (2019).\nAlboukadel Kassambara, Andrew Tran, Thomas Mock and others kindly allowed for the re-use and/or modification of their work.\n\n\n\n\n\n\n\n\nThanks to contributions from students and colleagues at Harvard and elsewhere, as well as from random people I met on the internet: Albert Rivero, Nicholas Dow, Celine Vendler, Sophia Zheng, Maria Burzillo, Robert McKenzie, Deborah Gonzalez, Beau Meche, Evelyn Cai, Miro Bergam, Jessica Edwards, Emma Freeman, Cassidy Bargell, Yao Yu, Vivian Zhang, Ishan Bhatt, Mak Famulari, Tahmid Ahmed, Eliot Min, Hannah Valencia, Asmer Safi, Erin Guetzloe, Shea Jenkins, Thomas Weiss, Diego Martinez, Andy Wang, Tyler Simko, Jake Berg, Connor Rust, Liam Rust, Alla Baranovsky, Carine Hajjar, Diego Arias, and Stephanie Yao.\n\n\n\n\n\n\n\n\nAlso, Becca Gill, Ajay Malik, Heather Li, Nosa Lawani, Stephanie Saab, Nuo Wen Lei, Anmay Gupta and Dario Anaya.\n\n\n\n\n\n\n\n\nAlso, Kevin Xu, Anmay Gupta, Sophia Zhu, Arghayan Jeiyasarangkan, Yuhan Wu, Ryan Southward, George Pentchev, Ahmet Atilla Colak, Mahima Malhotra, and Shreeram Patkar.\n\n\n\n\n\n\n\n\nAlso, Tejas Mundhe, Jackson Roe, Varun Dommeti, Soham Gunturu and Felix Cai.\n\n\n\n\n\n\n\n\nAlso Melissa Ban, Srihith Garlapati, Miriam Heiss, Matthew Ru, Mann Talati, Alex Kuai, Anish Bellamkonda, Krish Saluja, Aryan Kancherla, Zayan Farooq, Rajarshi Mandal, Pranav Chivukula, and Pratham Kancherla.\nI would like to gratefully acknowledge funding from The Derek Bok Center for Teaching and Learning at Harvard University, via its Digital Teaching Fellows and Learning Lab Undergraduate Fellows programs.\n\n\n\n\n\n\n\n\nDavid Kane",
    "crumbs": [
      "Preamble"
    ]
  },
  {
    "objectID": "preamble.html#license",
    "href": "preamble.html#license",
    "title": "Preamble",
    "section": "License",
    "text": "License\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\n\n\n\nBryan, Jenny. 2019. STAT 545: Data Wrangling, Exploration, and Analysis with r. https://stat545.com/.\n\n\nDiez, David M, Christopher D Barr, and Mine Çetinkaya-Rundel. 2014. Introductory Statistics with Randomization and Simulation. First. Scotts Valley, CA: CreateSpace Independent Publishing Platform. https://www.openintro.org/stat/textbook.php?stat_book=isrs.\n\n\nDowney, Allen. 2012. Think Bayes: Bayesian Statistics Made Simple. Green Tea Press.\n\n\nGrolemund, Garrett, and Hadley Wickham. 2017. R for Data Science. First. Sebastopol, CA: O’Reilly Media. https://r4ds.had.co.nz/.\n\n\nIrizarry, Rafael A. 2019. Introduction to Data Science: Data Analysis and Prediction Algorithms with r. First. Boca Raton, FL: CRC Press.\n\n\nKim, Albert Y., and Chester Ismay. 2019. Statistical Inference via Data Science: A ModernDive into r and the Tidyverse. First. Boca Raton, FL: CRC Press.\n\n\nKuhn, Max, and Julia Silge. 2020. Tidy Modeling with r.\n\n\nLegler, Julie, and Paul Roback. 2019. Broadening Your Statistical Horizons: Generalized Linear Models and Multilevel Models.\n\n\nTimbers, Tiffany-Anne, Trevor Campbell, and Melissa Lee. 2021. Data Science: A First Introduction. https://ubc-dsci.github.io/introduction-to-datascience/.",
    "crumbs": [
      "Preamble"
    ]
  },
  {
    "objectID": "getting-started.html",
    "href": "getting-started.html",
    "title": "Getting Started",
    "section": "",
    "text": "Installing R and RStudio\nWe use R via RStudio. R is to RStudio as a car’s engine is to its dashboard.\nMore precisely, R is a programming language that runs computations, while RStudio is an integrated development environment (IDE) that provides an interface with many convenient features. Just as having access to a speedometer, rearview mirrors, and a navigation system makes driving much easier, using RStudio’s interface makes using R much easier.\nDownload and install R and RStudio (Desktop version) on your computer.\nIf you are using Windows, download and install R Tools. Again, this is only for Windows users. The installer associated with “R-release” is what you want.\nIf you are using a Chromebook, follow this advice.\nIf you are having trouble, just sign up for a free account on Posit Cloud. (See here for our instructions.) This will give you 25 hours of usage, which should be enough to get started.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "getting-started.html#summary",
    "href": "getting-started.html#summary",
    "title": "Getting Started",
    "section": "Summary",
    "text": "Summary\nYou should have done the following:\n\nInstalled the latest versions of R and RStudio.\nInstalled, from CRAN, the remotes, tutorial.helpers and tidyverse packages:\n\n\noptions(\"pkgType\" = \"binary\")\ninstall.packages(\"remotes\")\ninstall.packages(\"tutorial.helpers\")\ninstall.packages(\"tidyverse\")\n\n\nInstalled, from Github, the r4ds.tutorials and primer.tutorials packages:\n\n\nremotes::install_github(\"PPBDS/r4ds.tutorials\")\nremotes::install_github(\"PPBDS/primer.tutorials\")\n\n\nCompleted the Getting Started tutorial from the tutorial.helpers package.\n\nLet’s get started.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "rubin-causal-model.html",
    "href": "rubin-causal-model.html",
    "title": "1  Rubin Causal Model",
    "section": "",
    "text": "1.1 Preceptor Table\nWe would not need data science if we (and our bosses, colleagues, and clients) did not have questions. Every data science project starts with a question. Examples:\nWhat is the average height of a student at New College of Florida?\nWhat are the chances that, out of the next four New College students we meet, one will be taller than 183 centimeters?\nA Preceptor Table1 is the smallest possible table with rows and columns such that, if none of the data is missing, then the things we want to know are easy to calculate. Consider:\nPreceptor Table\n\n\nID\nOutcome\n\n\nHeight (cm)\n\n\n\n\nStudent 1\n190\n\n\nStudent 2\n160\n\n\n...\n...\n\n\nStudent 47\n172\n\n\nStudent 48\n176\n\n\n...\n...\n\n\nStudent 325\n180\n\n\nStudent 326\n162\n\n\n...\n...\n\n\nStudent 670\n185\nIf we had a table with the height of every New College student, then statistics like the average would be easy to calculate. It would also be straightforward, using simulation, to estimate the the chances of various scenarios, as in our second question.\nEven the simplest Preceptor Table will have two columns. The first is an ID column which serves to label each unit. The second column is the outcome of interest, the variable we are trying to predict/understand/influence. The rows of the Preceptor Table are the units, the objects on which the outcome is measured.\nTo answer questions, we need data. Consider:\nData for New College Students\n\n\nID\nOutcome\n\n\nHeight (cm)\n\n\n\n\nStudent 11\n160\n\n\nStudent 32\n189\n\n\nStudent 47\n172\n\n\nStudent 68\n170\n\n\nStudent 425\n175\n\n\nStudent 436\n162\n\n\nStudent 570\n188\nThe notion of time is important, both in our Preceptor Table and in our data. To what moment in time do the questions refer? At what moment in time was our data collected? Those two moments are almost always different. Our data comes from some point in the past, even if it was collected yesterday. Our questions usually refer to now or to an indeterminate moment in the future.\nIn order to use this data to answer our questions, we need to consider the concept of validity. Is the data we have valid for answering our questions? Height, as an outcome, seems to make this simple, but, even here, complications can arise. For example, was height measured with shoes on or off? Which measure do we want?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Rubin Causal Model</span>"
    ]
  },
  {
    "objectID": "rubin-causal-model.html#population-table",
    "href": "rubin-causal-model.html#population-table",
    "title": "1  Rubin Causal Model",
    "section": "\n1.2 Population Table",
    "text": "1.2 Population Table\nWe create the Population Table2 by combining the Preceptor Table and the data. The aim of the Population Table is to illustrate the broader population in which we are interested. This table has three sources of data: the data for units we want to have (the Preceptor Table), the data for units which we actually have (our data), and the data for units we do not care about (the rest of the population, not included in the data or the Preceptor Table).\n\nWe are trying to answer questions about the height of New College students in 2024, so the “year” entries for these rows will read “2024.” In this case, the actual data comes from a survey of New College students in 2015.\n\nThe “Source” column refers to the source of the rows in the Population Table. The rows with no “Source” are from the larger population from which, by assumption, both the Preceptor Table and the data are drawn. As such, all values, other than year, are missing.\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation Table\n\n\nSource\nYear\nHeight\n\n\n\n\n…\n2010\n…\n\n\n…\n…\n…\n\n\nData\n2015\n180\n\n\nData\n2015\n163\n\n\n…\n…\n…\n\n\n…\n2018\n…\n\n\n…\n…\n…\n\n\nPreceptor Table\n2024\n?\n\n\nPreceptor Table\n2024\n?\n\n\n…\n…\n…\n\n\n…\n2030\n…\n\n\n…\n2030\n…\n\n\n\n\n\n\n\nThe question marks indicate the data which we do not know but which we need in order to answer the questions.\nImplicit in the Preceptor Table is a notion of time. Now that we can see our actual data compared with our greater population and our desired data, we must expand our observations. That is to say that, given our data is sourced from 2015 and our desired data is from 2024, we should include, by assumption, a greater time span in our population.\nNever forget that time is always a lie in the Population Table. Indeed, any time variable is suspect in data science more broadly.\n\nA moment in time is rarely measured accurately. In this example, we refer to the data being recorded in 2015. But that isn’t true! We didn’t record student heights across a year. We recorded them once, on a specific date like August 9, 2015 and at a specific time like 3:16 PM.\nIdentical values often aren’t truly identical. Even though the data measures for different students all refer to 2015, they almost certainly occurred at different moments in time, even if all measurements were taken in 2015. In fact, measurements can and do occur on different days or even months. Using 2015 a the value for Year hides that variation.\nThe value for the time variable for rows corresponding to the Preceptor Table is always hazy. Does it refer to now? Tomorrow? Whenever we finish the analysis? In general, we don’t know the time period for which we are answering our question until we actually get to the point of answering.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Rubin Causal Model</span>"
    ]
  },
  {
    "objectID": "rubin-causal-model.html#causal-effect",
    "href": "rubin-causal-model.html#causal-effect",
    "title": "1  Rubin Causal Model",
    "section": "\n1.3 Causal effect",
    "text": "1.3 Causal effect\n\n\n\n\nThis study, which shows the impact of exposure to Spanish-speaking individuals on attitudes towards immigration, was conducted by Ryan Enos.\n\n\n\nThe Rubin Causal Model (RCM) is based on the idea of potential outcomes. For example, Enos (2014) measured attitudes toward immigration among Boston commuters. Individuals were exposed to one of two possible conditions, and then their attitudes towards immigrants were recorded. One condition was waiting on a train platform near individuals speaking Spanish. The other was being on a train platform without Spanish-speakers. To calculate the causal effect of having Spanish-speakers nearby, we need to compare the outcome for an individual in one possible state of the world (with Spanish-speakers) to the outcome for that same individual in another state of the world (without Spanish-speakers). However, it is impossible to observe both potential outcomes at once. One of the potential outcomes is always missing, since a unit cannot travel back in time, and experience both treatments. This dilemma is the Fundamental Problem of Causal Inference.\nIn most circumstances, we are interested in comparing two experimental manipulations, one generally termed “treatment” and the other “control.” The difference between the potential outcome under treatment and the potential outcome under control is a “causal effect” or a “treatment effect.” According to the RCM, the causal effect of being on the platform with Spanish-speakers is the difference between what your attitude would have been under “treatment” (with Spanish-speakers) and under “control” (no Spanish-speakers).\nThe commuter survey consisted of three questions, each measuring agreement on a 1 to 5 integer scale, with 1 being liberal and 5 being conservative. For each person, the three answers were summed, generating an overall measure of attitude toward immigration which ranged from 3 (very liberal) to 15 (very conservative). If your attitude towards immigrants would have been a 13 after being exposed to Spanish-speakers and a 9 with no such exposure, then the causal effect of being on a platform with Spanish-speakers is a 4-point increase in your score.\n\nWe will use the symbol \\(Y\\) to represent potential outcomes, the variable we are interested in understanding and modeling. \\(Y\\) is called the response or outcome variable. It is the variable we want to “explain.” In our case this would be the attitude score. If we are trying to understand a causal effect, we need two symbols so that the value with treatment and the value with control can be represented separately: \\(Y_t\\) and \\(Y_c\\).\n\n1.3.1 Potential outcomes\nSuppose that Yao is one of the commuters surveyed in this experiment. If we were omniscient, we would know the outcomes for Yao under both treatment (with Spanish-speakers) and control (no Spanish-speakers), and we’d be able to ignore the Fundamental Problem of Causal Inference. We can show this using a Preceptor Table. Calculating the number we are interested in is trivial because none of the data is missing.\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\nOutcomes\n\n\nAttitude if Treated\nAttitude if Control\n\n\n\nYao\n13\n9\n\n\n\n\n\n\nRegardless of what the causal effect is for other subjects, the causal effect for Yao of being on the train platform with Spanish-speakers is a shift towards a more conservative attitude.\nUsing the response variable — the actual symbol rather than a written description — makes for a more concise Preceptor Table.\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\nOutcomes\n\n\n$$Y_t$$\n$$Y_c$$\n\n\n\nYao\n13\n9\n\n\n\n\n\n\nThe “causal effect” is the difference between Yao’s potential outcome under treatment and his potential outcome under control.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\nOutcomes\nCausal Effect\n\n\n$$Y_t$$\n$$Y_c$$\n$$Y_t - Y_c$$\n\n\n\nYao\n13\n9\n+4\n\n\n\n\n\n\n\nRemember that, in the real world, we will have a bunch of missing data! We can not use simple arithmetic to calculate the causal effect on Yao’s attitude toward immigration. Instead, we will be required to estimate it. An estimand is some unknown variable in the real world that we are trying to measure. In this case, it is \\(Y_{t}-Y_{c}\\), not \\(+4\\). An estimand is not the value you calculated, but is rather the unknown variable you want to estimate.\n\n\n\n\nDon Rubin was a Professor of Statistics at Harvard.\n\n\n\n\n1.3.2 Causal and predictive models\nCausal inference is often compared with prediction. In prediction, we want to know an outcome, \\(Y\\). In causal inference, we want to know a function of potential outcomes, such as the treatment effect: \\(Y_t - Y_c\\).\nThese are both missing data problems. Prediction involves estimating an outcome variable that we don’t have, and thus is missing, whether because it is in the future or because it is from data that we are unable to collect. Thus, prediction is the term for using statistical inference to fill in missing data for individual outcomes. Causal inference, however, involves filling in missing data for more than one potential outcome. This is unlike prediction, where only one outcome can ever be observed, even in principle.\n\nKey point: In a predictive model, there is only one \\(Y\\) value for each unit. This is very different to the RCM where there are (at least) two potential outcomes (treatment and control). There is only one outcome column in a predictive model, whereas there are two or more in a causal model.\n\nWith a predictive model, we cannot infer what would happen to the outcome \\(Y\\) if we changed \\(X\\) for a given unit. We can only compare two units, one with one value of \\(X\\) and another with a different value of \\(X\\).\nIn a sense, all models are predictive. However, only a subset of models are causal, meaning that, for a given individual, you can change the value \\(X\\) and observe a change in outcome, \\(Y(u)\\), and from that calculate a causal effect.\n\n1.3.3 No causation without manipulation\nIn order for a potential outcome to make sense, it must be possible, at least a priori. For example, if there is no way for Yao, under any circumstance, to ever be in the train study, then \\(Y_{t}\\) is impossible for him. It can never happen. And if \\(Y_{t}\\) can never be observed, even in theory, then the causal effect of treatment on Yao’s attitude is undefined.\nThe causal effect of exposure to Spanish-speakers is well defined because it is the simple difference of two potential outcomes, both of which might happen. In this case, we (or something else) can manipulate the world, at least conceptually, so that it is possible that one thing or a different thing might happen.\nThis definition of causal effects becomes much more problematic if there is no way for one of the potential outcomes to happen, ever. For example, what is the causal effect of Yao’s height on his weight? It might seem we would just need to compare two potential outcomes: Yao’s weight under the treatment (where treatment is defined as being 3 inches taller) and Yao’s weight under the control (where control is defined as his current height).\nA moment’s reflection highlights the problem: we can’t increase Yao’s height. There is no way to observe, even conceptually, what Yao’s weight would be if he were taller because there is no way to make him taller. We can’t manipulate Yao’s height, so it makes no sense to investigate the causal effect of height on weight. Hence the slogan: No causation without manipulation.\nThis then raises the question of what can and cannot be manipulated. If something cannot be manipulated, we should not consider it causal. So can race ever be considered causal? What about sex? A genetic condition like color-blindness? Can we manipulate these characteristics? In the modern world these questions are not simple.\nTake color-blindness for example. Say we are interested in how color-blindness impacts ability to complete a jig-saw puzzle. Because color-blindness is genetic some might argue it cannot be manipulated. But advances in technology like gene-therapy might allow us to actually change someone’s genes. Could we then claim the ability to manipulate color-blindness? If yes, we could then measure the causal effect of color-blindness on ability to complete jig-saw puzzles.\nThe slogan of “No causation without manipulation” may at first seem straight-forward, but it is clearly not so simple. Questions about race, sex, and genetics are very complex and should be considered with care.\n\n1.3.4 Multiple units\n\nGenerally, a study has many individuals (or, more broadly, “units”) who each have their own potential outcomes. More notation is needed to allow us to differentiate between different units.\nIn other words, there needs to be a distinction between \\(Y_t\\) for Yao, and \\(Y_t\\) for Emma. We use the variable \\(u\\) (\\(u\\) for “unit”) to indicate that the outcome under control and the outcome under treatment can differ for each individual unit (person).\nInstead of \\(Y_t\\), we will use \\(Y_t(u)\\) to represent “Attitude if Treated.” If you want to talk about only Emma, you could say “Emma’s Attitude if Treated” or “\\(Y_t(u = Emma)\\)” or “the \\(Y_t(u)\\) for Emma”, but not just \\(Y_t\\). That notation is too ambiguous when there is more than one subject.\nLet’s look at a Preceptor Table with more subjects using our new notation:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\nOutcomes\nCausal Effect\n\n\n$$Y_t(u)$$\n$$Y_c(u)$$\n$$Y_t(u) - Y_c(u)$$\n\n\n\n\nYao\n13\n9\n+4\n\n\nEmma\n14\n11\n+3\n\n\nCassidy\n11\n6\n+5\n\n\nTahmid\n9\n12\n-3\n\n\nDiego\n3\n4\n-1\n\n\n\n\n\n\n\nFrom this Preceptor Table, there are many possible estimands we might be interested in. Consider some examples, along with their true values:\n\n\n\nA potential outcome for one person, e.g., Yao’s potential outcome under treatment: \\(13\\).\nA causal effect for one person, such as for Emma. This is the difference between the potential outcomes: \\(14 - 11 = +3\\).\nThe most positive causal effect: \\(+5\\), for Cassidy.\nThe most negative causal effect: \\(-3\\), for Tahmid.\nThe median causal effect: \\(+3\\).\nThe median percentage change: \\(+27.2\\%\\). To see this, calculate the percentage change for each person. You’ll get 5 percentages: \\(+44.4\\%\\), \\(+27.2\\%\\), \\(+83.3\\%\\), \\(-25.0\\%\\), and \\(-25.0\\%\\).\n\n\nSimilar concepts can also be applied to the Population Table:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation Table\n\n\nSource\nYear\nID\nOutcomes\nCausal Effect\n\n\n$$Y_t(u)$$\n$$Y_c(u)$$\n$$Y_t(u) - Y_c(u)$$\n\n\n\n\n…\n2010\n?\n?\n?\n?\n\n\n…\n…\n…\n…\n…\n…\n\n\nData\n2015\nYao\n5\n3\n+2\n\n\nData\n2015\nCassidy\n3\n6\n-3\n\n\n…\n…\n…\n…\n…\n…\n\n\n…\n2018\n?\n?\n?\n?\n\n\n…\n…\n…\n…\n…\n…\n\n\nPreceptor Table\n2024\nYao\n13\n9\n+4\n\n\n…\n…\n…\n…\n…\n…\n\n\n…\n2030\n?\n?\n?\n?\n\n\n\n\n\n\n\nFor example, we get a much better picture of all our data, as it all combines into one nice looking Population Table. We can take a look a past data about Yao, or Cassidy, and their previous outcomes and causal effects. We can also see the rest of the units which fall under our desired population, but we don’t have any data about, hence the question makes.\nConsider these examples:\n\nDifference in potential outcome for one person, eg., the difference between Yao’s \\(Y_t(u)\\) values: \\(-8\\)\n\nDifference in causal effect for one person, for Yao it would be \\(-2\\): \\(+2- +4\\)\n\n\nAll of the variables calculated in the Preceptor and Population Tables are examples of estimands we might be interested in. One estimand is important enough that it has its own name: the average treatment effect, often abbreviated as ATE. The average treatment effect is the mean of all the individual causal effects. Here, the mean is \\(+1.6\\).\nWhat does our real-world Preceptor Table look like?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCausal Preceptor Table\n\n\nID\nOutcomes\nCausal Effect\n\n\n$$Y_t(u)$$\n$$Y_c(u)$$\n$$Y_t(u) - Y_c(u)$$\n\n\n\n\nYao\n13\n?\n?\n\n\nEmma\n14\n?\n?\n\n\nCassidy\n?\n6\n?\n\n\nTahmid\n?\n12\n?\n\n\nDiego\n3\n?\n?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictive Preceptor Table\n\n\nID\n$$Y_t(u)$$\n\n\n\n\nYao\n13\n\n\nEmma\n14\n\n\nCassidy\n6\n\n\nTahmid\n12\n\n\nDiego\n3",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Rubin Causal Model</span>"
    ]
  },
  {
    "objectID": "rubin-causal-model.html#assumptions",
    "href": "rubin-causal-model.html#assumptions",
    "title": "1  Rubin Causal Model",
    "section": "\n1.4 Assumptions",
    "text": "1.4 Assumptions\nIn this section, we will explore four topics: validity, stability, representativeness and unconfoundedness.\nOur earlier Population Table familiarized us with the three sources of data for which we are making inferences: the Preceptor Table, our data, and the greater population from which both are drawn. Consider a new Population Table.\n\n\n\n\n\n\n\n\n\nPopulation Table\n\n\nSource\nOutcomes\nYear\nCovariates\nCausal Effect\n\n\n$$Y_t(u)$$\n$$Y_c(u)$$\nSex\n$$Y_t(u) - Y_c(u)$$\n\n\n\n\n...\n?\n?\n2010\n?\n?\n\n\n...\n?\n?\n2010\n?\n?\n\n\n...\n...\n...\n...\n...\n...\n\n\nData\n13\n?\n2012\nMale\n?\n\n\nData\n?\n9\n2012\nFemale\n?\n\n\n...\n...\n...\n...\n...\n...\n\n\nPreceptor Table\n?\n?\n2024\nFemale\n?\n\n\nPreceptor Table\n?\n?\n2024\nFemale\n?\n\n\n...\n...\n...\n...\n...\n...\n\n\n...\n?\n?\n2024\n?\n?\n\n\n...\n?\n?\n2024\n?\n?\n\n\n\n\n\n\n\nThe rows from our data have covariates and two potential outcomes. (By definition, we only know the value for one of the potential outcomes for each unit.) The rows from the Preceptor Table include covariates, but not outcomes. (We have no outcome data for 2024, but we will, in theory, be given the sex of any units about whom we need to make inferences.) The rows from our greater population include no data, as we know nothing about these units.\n\n\n\n1.4.1 Validity\n\nValidity is the consistency, or lack there of, in the columns of your data set and the corresponding columns in your Preceptor Table. In order to consider the two data sets to be drawn from the same population, the columns from one must have a valid correspondence with the columns in the other. Validity, if true (or at least reasonable), allows us to construct the Population Table.\nConsider the potential outcomes: attitude toward immigration under treatment and control. How is this measured? In this case, we use answers to survey questions. Our we going to use the exact same questions in 2024 as were used in our data from 2012? Probably not. Even if we used the same wording, will the meaning of the words be constant. Almost certainly not! The word “undocumented” could mean something very different in the America before the election of Donald Trump in 2016. Consider the treatment: exposure to Spanish speakers on a train platform. The speakers used in 2012 will not be available in 2024 if we were to redo the experiment.\nValidity is an assumption which allows us to “stack” the data and the Preceptor Table on top of one another, into a single Population Table. The variables from each source, while not identical in meaning, are similar enough that we can treat them as if they are the same thing.\n\n1.4.2 Stability\nStability means that the relationship between the columns in the Population Table is the same for three categories of rows: the data, the Preceptor Table, and the larger population from which both are drawn.\nWe will be constructing models, mathematical relationships between different variables. For example, perhaps the treatment has a greater effect on women than on men. However, we can only directly observe that relationship in the data which we have, from 2012. We must assume stability in order to use the model created from the data to make inferences about other rows in the Population Table, specifically the rows corresponding to the Preceptor Table.\nMust the world be stable? No! In fact, the world is always changing! The stability assumption, in this case, is a claim that the world has not changes that much between 2012 and 2024. Stability applies, not just to the data and the Preceptor Table rows, but to the entire Population Table. Stability allows us to ignore the passage of time.\n\n1.4.3 Representativeness\n\n\nRepresentativeness, or the lack thereof, concerns two relationship, among the rows in the Population Table. The first is between the Preceptor Table and the other rows. The second is between our data and the other rows. Ideally, we would like both the Precepor Table and our data to be random samples from the population. Sadly, this is almost never the case.\nDoes the train experiment allow us to calculate a causal effect for people who commute by cars? Can we calculate the causal effect for people in New York City? Before we generalize to a broader population we have to consider if our experimental estimates are applicable beyond our experiment. Maybe we think that commuters in Boston and New York are similar enough to generalize our findings. We could also conclude that people who commute by car are fundamentally different than people who commute by train. If that was true, then we could not say our estimate is true for all commuters because our sample does not accurately represent the broader group to which we want to generalize.\n\n\n\n\n\n1.4.4 Unconfoundedness\nA fourth assumption we use when working with causal models — but not with predictive models — is “unconfoundedness.” If whether or not a unit received treatment or control is random, we write that treatment assignment is not “confounded.” If, however, treatment assignment depends on the value of a potential outcome, then treatment assignment is confounded. Our lives are easiest if we can (reasonably!) assume unconfoundedness. In that case, we can estimate the average treatment effect by subtracting the average outcome for control units from the average outcome for treated units, as we do above.\nConsider the “Perfect Doctor” as an example of the problems caused by confounded treatment assignments. Imagine we have this omniscient doctor who knows how any patient will respond to a certain drug. She has perfect knowledge of the entire Preceptor Table. Using this information, she always assign each patient the treatment with the best outcome, whether that is treatment or control. Consider:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHoly Grail of Information\n\n\nID\nBlood Pressure Outcomes\nCausal Effect\n\n\n$$Y_t(u)$$\n$$Y_c(u)$$\n$$Y_t(u) - Y_c(u)$$\n\n\n\n\nYao\n130\n105\n+25\n\n\nEmma\n120\n140\n-20\n\n\nCassidy\n100\n170\n-70\n\n\nTahmid\n115\n125\n-10\n\n\nDiego\n135\n100\n35\n\n\nMEAN\n120\n128\n-8\n\n\n\n\n\n\n\nThe Perfect Doctor would assign the treatment to Emma, Cassidy and Tahmid. She would assign control to Yao and Diego. And that is good! This is what the doctor should do. This is the best treatment assignment for the patients. But it is not a good assignment mechanism for estimating the average causal effect because treatment assignment is confounded by the values of the potential outcomes.\nWe, the non-Perfect Doctors, do not have access to the entire Preceptor Table. We can only see this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkewed Holy Grail of Information\n\n\nID\nBlood Pressure Outcomes\nCausal Effect\n\n\n$$Y_t(u)$$\n$$Y_c(u)$$\n$$Y_t(u) - Y_c(u)$$\n\n\n\n\nYao\n?\n105\n?\n\n\nEmma\n120\n?\n?\n\n\nCassidy\n100\n?\n?\n\n\nTahmid\n115\n?\n?\n\n\nDiego\n?\n100\n?\n\n\nMEAN\n111.66\n102.5\n9.16\n\n\n\n\n\n\n\nThe true causal effect of the treatment, as we can see in the first table, is -8. In other words, the treatment lowers blood pressure on average. But, using just the data we have access to if the Perfect Doctor performs the treatment assignment, we would estimate — if we mistakenly assume random assignment — that the causal effect is positive, that treatment increases blood pressure.\nThe best way to ensure unconfoundedness is to randomize the treatment across units. Don’t let the doctor decide who gets the treatment and who gets the control. Randomize assignment. As long as you use randomization as your assignment mechanism, you are good. There is the possibility that you can’t use pure randomization due to ethical or practical reasons, so we are forced to use a non-random assignment mechanisms. Many statistical methods have been developed for causal inference when there is a non-random assignment mechanism. Those methods, however, are largely beyond the scope of this book.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Rubin Causal Model</span>"
    ]
  },
  {
    "objectID": "rubin-causal-model.html#simple-models",
    "href": "rubin-causal-model.html#simple-models",
    "title": "1  Rubin Causal Model",
    "section": "\n1.5 Simple models",
    "text": "1.5 Simple models\n\nHow can we fill in the question marks? Because of the Fundamental Problem of Causal Inference, we can never know the missing values. Because we can never know the missing values, we must make assumptions. “Assumption” just means that we need a “model,” and all models have parameters.\n\n1.5.1 A single value for tau\nOne model might be that the causal effect is the same for everyone. There is a single parameter, \\(\\tau\\), which we then estimate. (\\(\\tau\\) is a Greek letter, written as “tau” and rhyming with “cow.”) Once we have an estimate, we can fill in the Preceptor Table because, knowing it, we can estimate what the unobserved potential outcome is for each person. We use our assumption about \\(\\tau\\) to estimate the counterfactual outcome for each unit.\nRemember what our Preceptor Table looks like with all of the missing data:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\nOutcomes\nCausal Effect\n\n\n$$Y_t(u)$$\n$$Y_c(u)$$\n$$Y_t(u) - Y_c(u)$$\n\n\n\n\nYao\n13\n?\n?\n\n\nEmma\n14\n?\n?\n\n\nCassidy\n?\n6\n?\n\n\nTahmid\n?\n12\n?\n\n\nDiego\n3\n?\n?\n\n\n\n\n\n\n\nIf we assume \\(\\tau\\) is the treatment effect for everyone, how do we fill in the table? We are using \\(\\tau\\) as an estimate for the causal effect. By definition: \\(Y_t(u) - Y_c(u) = \\tau\\). Using simple algebra, it is then clear that \\(Y_t(u) = Y_c(u) + \\tau\\) and \\(Y_c(u) = Y_t(u) - \\tau\\). In other words, you could add it to the observed value of every observation in the control group (or subtract it from the observed value of every observation in the treatment group), and thus fill in all the missing values.\nAssuming there is a constant treatment effect, \\(\\tau\\), for everyone, filling in the missing values would look like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\nOutcomes\nCausal Effect\n\n\n$$Y_t(u)$$\n$$Y_c(u)$$\n$$Y_t(u) - Y_c(u)$$\n\n\n\n\nYao\n13\n\\[13 - \\tau\\]\n\\[\\tau\\]\n\n\nEmma\n14\n\\[14 - \\tau\\]\n\\[\\tau\\]\n\n\nCassidy\n\\[6 + \\tau\\]\n6\n\\[\\tau\\]\n\n\nTahmid\n\\[12 + \\tau\\]\n12\n\\[\\tau\\]\n\n\nDiego\n3\n\\[3 - \\tau\\]\n\\[\\tau\\]\n\n\n\n\n\n\n\nNow we need to find an estimate for \\(\\tau\\) in order to fill in the missing values. One approach is to subtract the average of the observed control values from the average of the observed treated values. \\[((13 + 14 + 3) / 3) - ((6 + 12) /  2)\\] \\[10 - 9 = +1\\]\nOr, in other words, we use this formula:\n\\[\\frac{\\Sigma Y_t(u)}{n_t} + \\frac{\\Sigma Y_c(u)}{n_c} = \\widehat{ATE}\\]\n\\(\\Sigma\\) represents the sum of the treated/control values, and \\(n_t\\)/\\(n_c\\) represents the number of values within the treated and control groups. This formula is for something called \\(\\widehat{ATE}\\), which we will discuss in more depth in a later section.\nContinuing with the example, calculating the ATE or the causal effect, gives us an estimate of \\(+1\\) for \\(\\tau\\). Let’s fill in our missing values by adding \\(\\tau\\) to the observed values under control and by subtracting \\(\\tau\\) from the observed value under treatment like so:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\nOutcomes\nCausal Effect\n\n\n$$Y_t(u)$$\n$$Y_c(u)$$\n$$Y_t(u) - Y_c(u)$$\n\n\n\n\nYao\n13\n\\[13 - (+1)\\]\n+1\n\n\nEmma\n14\n\\[14 - (+1)\\]\n+1\n\n\nCassidy\n\\[6 + (+1)\\]\n6\n+1\n\n\nTahmid\n\\[12 + (+1)\\]\n12\n+1\n\n\nDiego\n3\n\\[3 - (+1)\\]\n+1\n\n\n\n\n\n\n\nWhich gives us:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\nOutcomes\nCausal Effect\n\n\n$$Y_t(u)$$\n$$Y_c(u)$$\n$$Y_t(u) - Y_c(u)$$\n\n\n\n\nYao\n13\n12\n+1\n\n\nEmma\n14\n13\n+1\n\n\nCassidy\n7\n6\n+1\n\n\nTahmid\n13\n12\n+1\n\n\nDiego\n3\n2\n+1\n\n\n\n\n\n\n\nIf we make the assumption that there is a single value for \\(\\tau\\) and that \\(1\\) is a good estimate of that value, then we can determine the missing potential outcomes. The Preceptor Table no longer has any missing values, so we can use it to easily answer (almost) any conceivable question.\n\n1.5.2 Two values for tau\nA second model might assume that the causal effect is different between levels of a category but the same within those levels. For example, perhaps there is a \\(\\tau_F\\) for females and \\(\\tau_M\\) for males where \\(\\tau_F != \\tau_M\\). We are making this assumption to give us a different model with which we can fill in the missing values in our Preceptor Table. We can’t make any progress unless we make some assumptions. That is an inescapable consequence of the Fundamental Problem of Causal Inference.\nConsider a model in which causal effects differ based on sex. When we are looking at a “category” of units — for instance, sex — we call this a covariate. Possible covariates include, but are not limited to, sex, age, political party and almost everything else which might be associated with attitudes toward immigration.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\nOutcomes\nCovariate\nCausal Effect\n\n\n$$Y_t(u)$$\n$$Y_c(u)$$\nSex\n$$Y_t(u) - Y_c(u)$$\n\n\n\n\nYao\n13\n\\[13 - \\tau_M\\]\nMale\n\\[\\tau_M\\]\n\n\nEmma\n14\n\\[14 - \\tau_F\\]\nFemale\n\\[\\tau_F\\]\n\n\nCassidy\n\\[6 + \\tau_F\\]\n6\nFemale\n\\[\\tau_F\\]\n\n\nTahmid\n\\[12 + \\tau_M\\]\n12\nMale\n\\[\\tau_M\\]\n\n\nDiego\n3\n\\[3 - \\tau_M\\]\nMale\n\\[\\tau_M\\]\n\n\n\n\n\n\n\nWe would have two different estimates for \\(\\tau\\).\n\\(\\tau_M\\) would be \\[(13+3)/2 - 12 = -4\\] \\(\\tau_F\\) would be \\[(14-6 = +8)\\]\nUsing those values, we would fill out our new table like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\nOutcomes\nCausal Effect\n\n\n$$Y_t(u)$$\n$$Y_c(u)$$\n$$Y_t(u) - Y_c(u)$$\n\n\n\n\nYao\n13\n\\[13 - (-4)\\]\n-4\n\n\nEmma\n14\n\\[14 - (+8)\\]\n+8\n\n\nCassidy\n\\[6 + (+8)\\]\n6\n+8\n\n\nTahmid\n\\[12 + (-4)\\]\n12\n-4\n\n\nDiego\n3\n\\[3 - (-4)\\]\n-4\n\n\n\n\n\n\n\nWhich gives us:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\nOutcomes\nCausal Effect\n\n\n$$Y_t(u)$$\n$$Y_c(u)$$\n$$Y_t(u) - Y_c(u)$$\n\n\n\n\nYao\n13\n17\n-4\n\n\nEmma\n14\n6\n+8\n\n\nCassidy\n14\n6\n+8\n\n\nTahmid\n8\n12\n-4\n\n\nDiego\n3\n7\n-4\n\n\n\n\n\n\n\nWe now have two different estimates for Emma (and for everyone else in the table). When we estimate \\(Y_c(Emma)\\) using an assumption of constant treatment effect (a single value for \\(\\tau\\)), we get \\(Y_c(Emma) = 13\\). When we estimate assuming treatment effect is constant for each sex, we calculate that \\(Y_c(Emma) = 8\\). This difference between our estimates for Emma highlights the difficulties of inference. Models drive inference. Different models will produce different inferences.\n\n1.5.3 Heterogenous treatment effects\nIs the assumption of a constant treatment effect, \\(\\tau\\), usually true? No! It is never true. People vary. The effect of a pill on you will always be different from the effect of a pill on your friend, at least if we measure outcomes accurately enough. Treatment effects are always heterogeneous, meaning that they vary across individuals.\nReality looks like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\nOutcomes\nCausal Effect\n\n\n$$Y_t(u)$$\n$$Y_c(u)$$\n$$Y_t(u) - Y_c(u)$$\n\n\n\n\nYao\n13\n\\[13 - \\tau_{yao}\\]\n\\[\\tau_{yao}\\]\n\n\nEmma\n14\n\\[14 - \\tau_{emma}\\]\n\\[\\tau_{emma}\\]\n\n\nCassidy\n\\[6 + \\tau_{cassidy}\\]\n6\n\\[\\tau_{cassidy}\\]\n\n\nTahmid\n\\[12 + \\tau_{tahmid}\\]\n12\n\\[\\tau_{tahmid}\\]\n\n\nDiego\n3\n\\[3 - \\tau_{diego}\\]\n\\[\\tau_{diego}\\]\n\n\n\n\n\n\n\nCan we solve for \\(\\tau_{yao}\\)? No! That is the Fundamental Problem of Causal Inference. So how can we make any progress from here if we are unwilling to assume there is at least some structure to the causal effect across different individuals? Instead of worrying about the causal effect for specific individuals, we, instead, focus on the causal effect for the entire population.\n\n1.5.4 Average treatment effect\nThe average treatment effect (ATE) is the average difference in potential outcomes between the treated group and the control groups. Because averaging is a linear operator, the average difference is the same as the difference between the averages. The distinction between this estimand and estimands like \\(\\tau\\), \\(\\tau_M\\) and \\(\\tau_F\\), is that, in this case, we do not care about using the average treatment effect to fill in missing values in each row. The average treatment effect is useful because we don’t have to assume anything about each individuals’ \\(\\tau\\), like \\(\\tau_{yao}\\), but can still understand something about the average causal effect across the whole population.\nAs we did before, the simplest way to estimate the ATE is to take the mean of the treated group (\\(10\\)) and the mean of the control group (\\(9\\)) and then take the difference in those means (\\(1\\)). If we use this method to an estimate of the ATE, we’ll call it \\(\\widehat{ATE}\\), pronounced “ATE-hat.”\nIf we already did this exact same calculation above, why are we talking about it again? Remember that we are unwilling to assume treatment effect is constant in our study population, and we cannot solve for \\(\\tau\\) if \\(\\tau\\) is different for different individuals. This is where \\(\\widehat{ATE}\\) is helpful.\nSome estimands may not require filling in all the question marks in the Preceptor Table. We can get a good estimate of the average treatment effect without filling in every question mark — the average treatment effect is just a single number. Rarely in a study do we care about what happens to individuals. In our case, we don’t care about what specifically would happen to Cassidy’s attitude if treated. Instead, we care generally about how our experiment impacts people’s attitudes towards immigrants. This is why an average estimate, like \\(\\widehat{ATE}\\), can be helpful.\nAs we noted before, this is a popular estimand. Why?\n\nThere’s an obvious estimator for this estimand: the mean difference of the observed outcomes between the treated group and the control group: \\(\\overline{Y_t(u)} - \\overline{Y_c(u)}\\).\nIf treatment is randomly assigned, the estimator is unbiased: you can be fairly confident in the estimate if you have a large enough treatment and control groups.\nAs we did earlier, if you are willing to assume that the causal effect is the same for everyone (a big assumption!), you can use your estimate of the ATE, \\(\\widehat{ATE}\\), to fill in the missing individual values in your Preceptor Table.\n\nJust because the ATE is often a useful estimand doesn’t mean that it always is.\nConsider point #3. For example, let’s say the treatment effect does vary dependent on sex. For males there is a small negative effect (-4), but for females there is a larger positive effect (+8). However, the average treatment effect for the whole sample, even if you estimate it correctly, will be a single positive number (+1) – since the positive effect for females is larger than the negative effect for males.\nEstimating the average treatment effect, by calculating \\(\\widehat{ATE}\\), is easy. But is our \\(\\widehat{ATE}\\) a good estimate of the actual ATE? After all, if we knew all the missing values in the Preceptor Table, we could calculate the ATE perfectly. But those missing values may be wildly different from the observed values. Consider this Preceptor Table:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\nOutcomes\nCausal Effect\n\n\n$$Y_t(u)$$\n$$Y_c(u)$$\n$$Y_t(u) - Y_c(u)$$\n\n\n\n\nYao\n13\n10\n+3\n\n\nEmma\n14\n11\n+3\n\n\nCassidy\n9\n6\n+3\n\n\nTahmid\n15\n12\n+3\n\n\nDiego\n3\n0\n+3\n\n\n\n\n\n\n\nIn this example, there is indeed a constant treatment effect for everyone: \\(+3\\). Note that the observed values are all the same, but the unobserved values were such that our estimated ATE, \\(+1\\), is pretty far from the actual ATE, \\(+3\\). If we think we have a reasonable estimate of ATE, using that value as a constant for \\(\\tau\\) might be our best guess.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Rubin Causal Model</span>"
    ]
  },
  {
    "objectID": "rubin-causal-model.html#summary",
    "href": "rubin-causal-model.html#summary",
    "title": "1  Rubin Causal Model",
    "section": "\n1.6 Summary",
    "text": "1.6 Summary\n\n\n\n\n\n\nGlossary\n\n\n\n\nA Preceptor Table is a table that includes all rows and columns such that, if no data is missing, it is easy to calculate our quantity of interest. Unfortunately, there is always data missing.\nA Population Table has three sources of data: the Preceptor Table, the data set, and the greater population from which both are drawn.\nA Potential Outcome is the outcome for an individual depending on if they receive the treatment or not.\nA Causal Effect is the difference between two potential outcomes.\nThe Fundamental Problem of Causal Inference is that it is impossible to observe the causal effect on a single unit because we can never observe both potential outcomes for that unit. We must make assumptions in order to estimate causal effects.\nWhen confronting a causal question, first identify the units, treatments and outcomes.\n\n\n\nThe fundamental components of every problem in causal inference are units, treatments and outcomes. The units are the rows in the table. The treatments are (some of) the columns. The outcomes are the values under the treatment columns. (There are also covariate columns and the values within them.) Whenever you confront a problem in causal inference, start by identifying the units, treatments and outcomes.\nA causal effect is the difference between one potential outcome and another. How different would your life be if you missed the train?\n\n\nA Preceptor Table includes all rows and columns such that, if no data is missing, it is easy to calculate our quantity of interest. Unfortunately, data is always missing in causal models because, at most, we can only observe one potential outcome for each unit. The causal effect of a treatment on a single unit at a point in time is the difference between the value of the outcome variable with the treatment and without the treatment. The Fundamental Problem of Causal Inference is that it is impossible to observe the causal effect on a single unit. We must make assumptions — i.e, we must make models — in order to estimate causal effects.\nThe Population Table has three sources of data: the Preceptor Table, the data set, and the greater population from which both are drawn.\nThe assumption of validity, if met, allows us to create the Population Table because it ensures that the columns of data from the different sources can be put into a single table. If the relationships among the data are the same (or at least same’ish), over time, then we can assume stability. A model estimated on our data will also apply to our Preceptor Table. Representativeness examines the rows we have relative to the rows in the Population Table which we might have had. Unconfoundedness, which only matters in causal settings, means that either the treatment was randomly assigned or that we can act as if it was.\nRandom assignment of treatments to units is the best experimental set up for estimating causal effects. Other assignment mechanisms are subject to confounding. If the treatment assigned is correlated with the potential outcomes, it is very hard to estimate the true treatment effect. (As always, we use the terms “causal effects” and “treatment effects” interchangeably.) With random assignment, we can, mostly safely, estimate the average treatment effect (ATE) by looking at the difference between the average outcomes of the treated and control units.\nBe wary of claims made in situations without random assignment: Here be dragons!\n\n\n\n\nEnos, Ryan D. 2014. “Causal Effect of Intergroup Contact on Exclusionary Attitudes.” Proceedings of the National Academy of Sciences 111 (10): 3699–3704. https://doi.org/10.1073/pnas.1317670111.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Rubin Causal Model</span>"
    ]
  },
  {
    "objectID": "rubin-causal-model.html#footnotes",
    "href": "rubin-causal-model.html#footnotes",
    "title": "1  Rubin Causal Model",
    "section": "",
    "text": "The term Preceptor Table is, perhaps unsurprisingly, unique to this textbook. We hope you find it useful.↩︎\nThe term Population Table is, perhaps unsurprisingly, unique to this textbook. We hope you find it useful.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Rubin Causal Model</span>"
    ]
  },
  {
    "objectID": "probability.html",
    "href": "probability.html",
    "title": "2  Probability",
    "section": "",
    "text": "2.1 Distributions\nA variable in a tibble is a column, a vector of values. We sometimes refer to this vector as a “distribution.” This is somewhat sloppy in that a distribution can be many things, most commonly a mathematical formula. But, strictly speaking, a “frequency distribution” or an “empirical distribution” is a list of values, so this usage is not unreasonable.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#sec-distributions",
    "href": "probability.html#sec-distributions",
    "title": "2  Probability",
    "section": "",
    "text": "2.1.1 Scaling distributions\nConsider the vector which is the result of rolling one dice 10 times.\n\n\nShow the code\nten_rolls &lt;- c(5, 5, 1, 5, 4, 2, 6, 2, 1, 5)\n\n\nThere are other ways of storing the data in this vector. Instead of reporting every observation, we could record the number of times each value appears or the percentage of the total which this number accounts for.\n\n\n\n\n\n\n\n\n\nDistribution of Ten Rolls of a Fair Dice\n\n\nCounts and percentages reflect the same information\n\n\nOutcome\nCount\nPercentage\n\n\n\n\n1\n2\n20%\n\n\n2\n2\n20%\n\n\n4\n1\n10%\n\n\n5\n4\n40%\n\n\n6\n1\n10%\n\n\n\n\n\n\n\n\nIn this case, with only 10 values, it is actually less efficient to store the data like this. But what happens when we have 1,000 rolls?\n\n\n\n\n\n\n\n\n\nDistribution of One Thousand Rolls of a Fair Dice\n\n\nCounts and percentages reflect the same information\n\n\nOutcome\nCount\nPercentage\n\n\n\n\n1\n190\n19%\n\n\n2\n138\n14%\n\n\n3\n160\n16%\n\n\n4\n173\n17%\n\n\n5\n169\n17%\n\n\n6\n170\n17%\n\n\n\n\n\n\n\n\nInstead of keeping around a vector of length 1,000, we can just keep 12 values — the 6 possible outcomes and their frequency — without losing any information.\nTwo distributions can be identical even if they are of very different lengths. Let’s compare our original distribution of 10 rolls of the dice with another distribution which just features 100 copies of those 10 rolls.\n\n\nShow the code\nmore_rolls &lt;- rep(ten_rolls, 100)\n\n\n\n\n\n\n\n\n\n\n\nThe two graphs have the exact same shape because, even though the vectors are of different lengths, the relative proportions of the outcomes are identical. In some sense, both vectors are from the same distribution. Relative proportions, not the total counts, are what matter.\n\n\n2.1.2 Normalizing distributions\nIf two distributions have the same shape, then they only differ by the labels on the y-axis. There are various ways of “normalizing” distributions so as to place them all the same scale. The most common scale is one in which the area under the distribution adds to 1, e.g., 100%. For example, we can transform the above plots:\n\n\n\n\n\n\n\n\n\nWe sometimes refer to a distribution as “unnormalized” if the area under the curve does not add up to 1.\n\n\n2.1.3 Simulating distributions\nThere are two distinct concepts: a distribution and a set values drawn from that distribution. But, in typical usage, we employ “distribution” for both. When given a distribution (meaning a vector of numbers), we often use geom_histogram() or geom_density() to display it. But, sometimes, we don’t want to look at the whole thing. We just want some summary measures which report the key aspects of the distribution. The two most important attributes of a distribution are its center and its variation around that center.\n\nWe use summarize() to calculate statistics for a variable, a column, a vector of values, or a distribution. Note the language sloppiness. For the purposes of this book, “variable,” “column,” “vector,” and “distribution” all mean the same thing. Other popular statistical functions include: mean(), median(), min(), max(), n() and sum(). Functions which may be new to you include three measures of the “spread” of a distribution: sd() (the standard deviation), mad() (the scaled median absolute deviation) and quantile(), which is used to calculate an interval which includes a specified proportion of the values.\n\nThink of the distribution of a variable as an urn from which we can pull out, at random, values for that variable. Drawing a thousand or so values from that urn, and then looking at a histogram, can show where the values are centered and how they vary. Because people are sloppy, they will use the word distribution to refer to at least three related entities:\n\nthe (imaginary!) urn from which we are drawing values.\nall the values in the urn\nall the values which we have drawn from the urn, whether that be 10 or 1,000\n\nSloppiness in the usage of the word distribution is universal. However, keep three distinct ideas separate:\n\nThe unknown true distribution which, in reality, generates the data which we see. Outside of stylized examples in which we assume that a distribution follows a simple mathematical formula, we will never have access to the unknown true distribution. We can only estimate it. This unknown true distribution is often referred to as the data generating mechanism, or DGM. It is a function or black box or urn which produces data. We can see the data. We can’t see the urn.\nThe estimated distribution which, we think, generates the data which we see. Again, we can never know the unknown true distribution. But, by making some assumptions and using the data we have, we can estimate a distribution. Our estimate may be very close to the true distribution. Or it may be far away. The main task of data science to to create and use these estimated distributions. Almost always, these distributions are instantiated in computer code. Just as there is a true data generating mechanism associated with the (unknown) true distribution, there is an estimated data generating mechanism associated with the estimated ditribution.\nA vector of numbers drawn from the estimated distribution. Both true and estimated distributions can be complex animals, difficult to describe accurately and in detail. But a vector of numbers drawn from a distribution is easy to understand and use. So, in general, we work with vectors of numbers. When someone — either a colleague or a piece of R code — creates a distribution which we want to use to answer a question, we don’t really want the distribution itself. Rather, we want a vectors of “draws” from that distribution. Vectors are easy to work with! Complex computer code is not.\n\nAgain, people (including us!) will often be sloppy and use the same word, “distribution,” without making it clear whether they are talking about the true distribution, the estimated distribution, or a vector of draws from the estimated distribution. The same sloppiness applies to the use of the term data generating mechanism. Try not to be sloppy.\nMuch of the rest of the Primer involves learning how to work with distributions, which generally means working with the draws from those distributions. Fortunately, the usual rules of arithmetic apply. You can add/subtract/multiply/divide distributions by working with draws from those distributions, just as you can add/subtract/multiply/divide regular numbers.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#probability-distributions",
    "href": "probability.html#probability-distributions",
    "title": "2  Probability",
    "section": "2.2 Probability distributions",
    "text": "2.2 Probability distributions\n\n\n\n\n\nBruno de Finetti, an Italian statistician who wrote a famous treatise on the theory of probability that began with the statement “PROBABILITY DOES NOT EXIST.”\n\n\n\n\nFor the purposes of this Primer, a probability distribution is a mathematical object which maps a set of outcomes to probabilities, where each distinct outcome has a chance of occurring between 0 and 1 inclusive. The probabilities must sum to 1. The set of possible outcomes, i.e., the sample space — heads or tails for the coin, 1 through 6 for a single dice, 2 through 12 for the sum of a pair of dice — can be either discrete or continuous. Remember, discrete data can only take on certain values. Continuous data, like height and weight, can take any value within a range. The set of outcomes is the domain of the probability distribution. The range is the associated probabilities.\nAssume that a probability distribution is created by a probability function, a set function which maps outcomes to probabilities. The concept of a “probability function” is often split into two categories: probability mass functions (for discrete random variables) and probability density functions (for continuous random variables). As usual, we will be a bit sloppy, using the term probability distribution for both the mapping itself and for the function which creates the mapping.\nWe discuss three types of probability distributions: empirical, mathematical, and posterior.\nThe key difference between a distribution, as we have explored them in Section 2.1, and a probability distribution is the requirement that the sum of the probabilities of the individual outcomes must be exactly 1. There is no such requirement for a distribution in general. But any distribution can be turned into a probability distribution by “normalizing” it. In this context, we will often refer to a distribution which is not (yet) a probability distribution as an “unnormalized” distribution.\n\nPay attention to notation. Recall that when we are talking about a specific probability (represented by a single value), we will use \\(\\rho\\) (the Greek letter “rho”) with a subscript which specifies the exact outcome of which it is the probability. For instance, \\(\\rho_h = 0.5\\) denotes the probability of getting heads on a coin toss when the coin is fair. \\(\\rho_t\\) — spoken as “PT” or “P sub T” or “P tails” — denotes the probability of getting tails on a coin toss. However, when we are referring to the entire probability distribution over a set of outcomes, we will use \\(P()\\). For example, the probability distribution of a coin toss is \\(P(\\text{coin})\\). That is, \\(P(\\text{coin})\\) is composed of the two specific probabilities (50% and 50%) mapped from the two values in the domain (Heads and Tails). Similarly, \\(P(\\text{sum of two dice})\\) is the probability distribution over the set of 11 outcomes (2 through 12) which are possible when you take the sum of two dice. \\(P(\\text{sum of two dice})\\) is made up of 11 numbers — \\(\\rho_2\\), \\(\\rho_3\\), …, \\(\\rho_{12}\\) — each representing the unknown probability that the sum will equal their value. That is, \\(\\rho_2\\) is the probability of rolling a 2.\nA distribution is a function that shows the possible values of a variable and how often they occur.\n\n2.2.1 Flipping a coin\nData science problems start with a question. Example:\nWhat are the chances of getting three heads in a row when flipping a fair coin?\nQuestions are answered with the help of probability distributions.\nAn empirical distribution is based on data. Think of this as the probability distribution created by collecting data in the real world or by running a simulation on your computer. In theory, if we increase the number of coins we flip (either in reality or via simulation), the empirical distribution will look more and more similar to the mathematical distribution. The mathematical distribution is the Platonic form. The empirical distribution will often look like the mathematical probability distribution, but it will rarely be exactly the same.\nIn this simulation, there are 44 heads and 56 tails. The outcome will vary every time we run the simulation, but the proportion of heads to tails should not be too different if the coin is fair.\n\n\n\n\n\n\n\n\n\nA mathematical distribution is based on a mathematical formula. Assuming that the coin is perfectly fair, we should, on average, get heads as often as we get tails.\n\n\n\n\n\n\n\n\n\nThe distribution of a single observation is described by this formula.\n\\[ P(Y = y) = \\begin{cases} 1/2 &\\text{for }y= \\text{Heads}\\\\ 1/2 &\\text{for }y= \\text{Tails} \\end{cases}\\] We sometimes do not know that the probability of heads and the probability of tails both equal 50%. In that case, we might write:\n\\[ P(Y = y) = \\begin{cases} \\rho_H &\\text{for }y= \\text{Heads}\\\\ \\rho_T &\\text{for }y= \\text{Tails} \\end{cases}\\]\nYet, we know that, by definition, \\(\\rho_H + \\rho_T = 1\\), so we can rewrite the above as:\n\\[ P(Y = y) = \\begin{cases} \\rho_H &\\text{for }y= \\text{Heads}\\\\ 1- \\rho_H &\\text{for }y= \\text{Tails} \\end{cases}\\]\nCoin flipping (and related scenarios with only two possible outcomes) are such common problems, that the notation is often simplified further, with \\(\\rho\\) understood, by convention, to be the probability of heads. In that case, we can write the mathematical distribution is two canonical forms:\n\\[P(Y) = Bernoulli(\\rho)\\] and\n\\[y_i \\sim Bernoulli(\\rho)\\] All five of these versions mean the same thing! The first four describe the mathematical probability distribution for a fair coin. The capital \\(Y\\) within the \\(P()\\) indicates a random variable. The fifth highlights one “draw” from that random variable, hence the lower case \\(y\\) and the subscript \\(i\\).\nMost probability distributions do not have special names, which is why we will use the generic symbol \\(P\\) to refer to them. But some common probability distributions do have names, like “Bernoulli” in this case.\nIf the mathematical assumptions are correct, then, as your sample size increases, the empirical probability distribution will look more and more like the mathematical distribution.\nA posterior distribution is based on beliefs and expectations. It displays your beliefs about things you can’t see right now. You may have posterior distributions for outcomes in the past, present, or future.\nIn the case of the coin toss, the posterior distribution changes depending on your beliefs. For instance, let’s say your friend brought a coin to school and asked to bet you. If the result is heads, you have to pay them $5. In that case, your posterior probability distribution might look like this:\n\n\n\n\n\n\n\n\n\nThe fact that your friend wants to bet on heads suggests to you that the coin is not fair. Does it prove that the coin is unfair? No! Much depends on the sort of person you think your friend is. Your posterior probability distribution is your opinion, based on your experiences and beliefs. My posterior probability distribution will often be (very) different from yours.\nThe full terminology is mathematical (or empirical or posterior) probability distribution. But we will often shorten this to just mathematical (or empirical or posterior) distribution. The word “probability” is understood, even if it is not present.\n\nRecall the question with which we started this section: What are the chances of getting three heads in a row when flipping a fair coin? To answer this question, we need to use a probability distribution as our data generating mechanism. Fortunately, the rbinom() function allows us to generate the results for coin flips. For example:\n\n\nShow the code\nrbinom(n = 10, size = 1, prob = 0.5)\n\n\n [1] 1 1 0 1 1 0 0 0 0 0\n\n\ngenerates the results of 10 coin flips, where a result of heads is presented as 1 and tails as 0. With this tool, we can generate 1,000 draws from our experiment:\n\n\nShow the code\ntibble(toss_1 = rbinom(n = 1000, size = 1, prob = 0.5),\n       toss_2 = rbinom(n = 1000, size = 1, prob = 0.5),\n       toss_3 = rbinom(n = 1000, size = 1, prob = 0.5))\n\n\n# A tibble: 1,000 × 3\n   toss_1 toss_2 toss_3\n    &lt;int&gt;  &lt;int&gt;  &lt;int&gt;\n 1      0      1      1\n 2      0      1      1\n 3      0      1      0\n 4      0      0      1\n 5      1      1      0\n 6      1      0      1\n 7      1      0      0\n 8      1      0      1\n 9      0      0      1\n10      0      1      1\n# ℹ 990 more rows\n\n\nBecause the flips are independent, we can consider each row to be a draw from the experiment. Then, we simply count up the proportion of experiments in which resulted in three heads.\n\n\nShow the code\ntibble(toss_1 = rbinom(n = 1000, size = 1, prob = 0.5),\n       toss_2 = rbinom(n = 1000, size = 1, prob = 0.5),\n       toss_3 = rbinom(n = 1000, size = 1, prob = 0.5)) |&gt; \n  mutate(three_heads = toss_1 + toss_2 + toss_3 == 3) |&gt; \n  summarize(chance = mean(three_heads))\n\n\n# A tibble: 1 × 1\n  chance\n   &lt;dbl&gt;\n1  0.104\n\n\nThis is close to the “correct” answer of \\(1/8\\)th. If we increase the number of draws, we will get closer to the “truth.” The reason for the quotation marks around “correct” and “truth” is that we are uncertain. We don’t know the true probability distribution for this coin. If this coin is a trick coin — like the one we expect our friend to have brought to school — then the odds of three heads in a row would be much higher:\n\n\nShow the code\ntibble(toss_1 = rbinom(n = 1000, size = 1, prob = 0.95),\n       toss_2 = rbinom(n = 1000, size = 1, prob = 0.95),\n       toss_3 = rbinom(n = 1000, size = 1, prob = 0.95)) |&gt; \n  mutate(three_heads = toss_1 + toss_2 + toss_3 == 3) |&gt; \n  summarize(chance = mean(three_heads))\n\n\n# A tibble: 1 × 1\n  chance\n   &lt;dbl&gt;\n1   0.87\n\n\nThis is our first example of using a data generating mechanism — meaning rbinom() — to answer a question. We will see many more in the chapters to come.\n\n\n2.2.2 Rolling two dice\nData science begins with a question:\nWhat is the probability of rolling a 7 or an 11 with a pair of dice?\nWe get an empirical distribution by rolling two dice a hundred times, either by hand or with a computer simulation. The result is not identical to the mathematical distribution because of the inherent randomness of the real world and/or of simulation.\n\n\n\n\n\n\n\n\n\nWe might consider labeling the y-axis in plots of empirical distributions as “Proportion” rather than “Probability” since it is an actual proportion, calculated from real (or simulated) data. We will keep it as “Probability” since we want to emphasize the parallels between mathematical, empirical and posterior probability distributions.\nOur mathematical distribution tells us that, with a fair dice, the probability of getting 1, 2, 3, 4, 5, and 6 are equal: there is a 1/6 chance of each. When we roll two dice at the same time and sum the numbers, the values closest to the middle are more common than values at the edge because there are more combinations of numbers that add up to the middle values.\n\n\n\n\n\n\n\n\n\n\\[ P(Y = y) = \\begin{cases} \\dfrac{y-1}{36} &\\text{for }y=1,2,3,4,5,6 \\\\ \\dfrac{13-y}{36} &\\text{for }y=7,8,9,10,11,12 \\\\ 0 &\\text{otherwise} \\end{cases} \\]\nThe posterior distribution for rolling two dice a hundred times depends on your beliefs. If you take the dice from your Monopoly set, you have reason to believe that the assumptions underlying the mathematical distribution are true. However, if you walk into a crooked casino and a host asks you to play craps, you might be suspicious, just as in the coin flip example above. The word “suspicious” means that you suspect that the data generating mechanism for these dice is not like that for honest dice. For example, in craps, a “come-out” roll of 7 and 11 is a “natural,” resulting in a win for the “shooter” and a loss for the casino. You might expect those numbers to occur less often than they would with fair dice. Meanwhile, a come-out roll of 2, 3 or 12 is a loss for the shooter. You might also expect values like 2, 3 and 12 to occur more frequently. Your posterior distribution might look like this:\n\n\n\n\n\n\n\n\n\nSomeone less suspicious of the casino would have a posterior distribution which looks more like the mathematical distribution.\nWe began this section with a question about the probability (or odds) of rolling a 7 or 11 — i.e., a “natural” — with a pair of dice. The answer to the question depends on whether or not we think the dice are fair. In other words, we need to know which distribution to use to answer the question.\nAssume that the dice are fair. In that case, we can create a data generating mechanism by hand. (Alas, there is not a built-in R function for dice like there is for coin flips with rbinom().)\n\n\nShow the code\nset.seed(7)\n\n# Creating a variable like rolls makes our code easier to read and modify. Of\n# course, we could just hard code the 4 into the size argument for each of the\n# two calls to sample, but that is much less convenient.\n\nrolls &lt;- 4\n\n# The details of the code matter. If we don't have replace = TRUE, sample will\n# only use each of the 6 possible values once. That might be OK if we are just\n# rolling the dice 4 times, but it won't work for thousands of rolls.\n\ntibble(dice_1 = sample(x = 1:6, size = rolls, replace = TRUE),\n       dice_2 = sample(x = 1:6, size = rolls, replace = TRUE)) |&gt; \n  mutate(result = dice_1 + dice_2) |&gt; \n  mutate(natural = ifelse(result %in% c(7, 11), TRUE, FALSE))\n\n\n# A tibble: 4 × 4\n  dice_1 dice_2 result natural\n   &lt;int&gt;  &lt;int&gt;  &lt;int&gt; &lt;lgl&gt;  \n1      2      2      4 FALSE  \n2      3      6      9 FALSE  \n3      4      3      7 TRUE   \n4      2      6      8 FALSE  \n\n\nThis code is another data generating mechanism or dgm. It allows us to simulate the distribution of the results from rolling a pair of fair dice. To answer our question, we simply increase the number of rolls and calculate the proportion of rolls which result in a 7 or 11.\n\n\nShow the code\nrolls &lt;- 100000\n\n# We probably don't need 100,000 rolls, but this code is so fast that it does\n# not matter. Generally 1,000 (or even 100) draws from the data generating\n# mechanism is enough for most practical purposes.\n\ntibble(dice_1 = sample(x = 1:6, size = rolls, replace = TRUE),\n       dice_2 = sample(x = 1:6, size = rolls, replace = TRUE)) |&gt; \n  mutate(result = dice_1 + dice_2) |&gt; \n  summarize(natural_perc = mean(result %in% c(7, 11)))\n\n\n# A tibble: 1 × 1\n  natural_perc\n         &lt;dbl&gt;\n1        0.221\n\n\nThe probability of rolling either a 7 or an 11 with a pair of fair dice is about 22%.\n\n\n2.2.3 Presidential elections\nData science begins with a question:\nWhat is the probability that the Democratic candidate will win the Presidential election?\nConsider the probability distribution for a political event, like a presidential election. We want to know the probability that Democratic candidate wins X electoral votes, where X comes from the range of possible outcomes: 0 to 538. (The total number of electoral votes in US elections since 1964 is 538.)\n\nThe empirical distribution in this case would involve counting the number of electoral votes that the Democratic candidate won in each of the Presidential elections in the last 50 years or so. For the empirical distribution, we create a tibble with electoral vote results from past elections. Looking at elections since 1964, we can observe that the number of electoral votes that the Democratic candidate received in each election is different.\n\n\n\n\n\n\n\n\n\nGiven that we only have 15 observations, it is difficult to draw conclusions or make predictions based off of this empirical distribution. But “difficult” does not mean “impossible.” For example, if someone, more than a year before the election, offered to bet us 50/50 that the Democratic candidate was going to win more than 475 electoral votes, we would take the bet. After all, this outcome has only happened once in the last 15 elections, so a 50/50 bet seems like a great deal.\nWe can build a mathematical distribution for X which assumes that the chances of the Democratic candidate winning any given state’s electoral votes is 0.5 and that the results from each state are independent.\n\n\n\n\n\n\n\n\n\nIf our assumptions about this mathematical distribution are correct — they are not! — then, as the sample size increase, the empirical distribution should look more and more similar to our mathematical distribution.\nHowever, the data from past elections is more than enough to demonstrate that the assumptions of our mathematical probability distribution do not work for electoral votes. The model assumes that the Democrats have a 50% chance of receiving each of the 538 votes. Just looking at the mathematical probability distribution, we can observe that receiving 13 or 17 or 486 votes out of 538 would be extreme and almost impossible if the mathematical model were accurate. However, our empirical distribution shows that such extreme outcomes are quite common. Presidential elections have resulted in much bigger victories or defeats than this mathematical distribution seems to allow for, thereby demonstrating that our assumptions are false.\nThe posterior distribution of electoral votes is a popular topic, and an area of strong disagreement, among data scientists. Consider this posterior from FiveThirtyEight.\n\n\n\n\n\n\n\n\n\nBelow is a posterior probability distribution from the FiveThirtyEight website for August 13, 2020. This was created using the same data as the above distribution, but is displayed differently. For each electoral result, the height of the bar represents the probability that a given event will occur. However, there are no labels on the y-axis telling us what the specific probability of each outcome is. And that is OK! The specific values are not that useful. If we removed the labels on our own y-axes, would it matter? Probably not. Anytime there are many possible outcomes — 539, in this case — we stop looking at specific outcomes and, instead, look at where most of the “mass” of the distribution lies.\n\n\n\n\n\n\n\n\n\nBelow is the posterior probability distribution from The Economist, also from August 13, 2020. This looks confusing at first because they chose to combine the axes for Republican and Democratic electoral votes. The Economist was less optimistic, relative to FiveThirtyEight, about Trump’s chances in the election.\n\n\n\n\n\n\n\n\n\nThese two models, built by smart people using similar data sources, have reached fairly different conclusions. Data science is difficult! There is not one “right” answer. Real life is not a problem set.\n\n\n\n\n\nWatch the makers of these two models throw shade at each other on Twitter! Eliot Morris is one of the primary authors of the Economist model. Nate Silver is in charge of 538. They don’t seem to be too impressed with each other’s work! More smack talk here and here.\n\n\n\n\nThere are many questions you could explore with posterior distributions. They can relate to the past, present, or future.\n\nPast: How many electoral votes would Hilary Clinton have won if she had picked a different VP?\nPresent: What are the total campaign donations from Harvard faculty?\nFuture: How many electoral votes will the Democratic candidate for president win in 2024?\n\n\n\n\n2.2.4 Height\n\nQuestion: What is the height of the next adult male we will meet?\nThe three examples above are all discrete probability distributions, meaning that the outcome variable can only take on a limited set of values. A coin flip has two outcomes. The sum of a pair of dice has 11 outcomes. The total electoral votes for the Democratic candidate has 539 possible outcomes. In the limit, we can also create continuous probability distributions which have an infinite number of possible outcomes. For example, the average height for an American male could be any real number between 0 inches and 100 inches. (Of course, an average value anywhere near 0 or 100 is absurd. The point is that the average could be 68.564, 68.5643, 68.56432 68.564327, or any real number.)\nAll the characteristics for discrete probability distributions which we reviewed above apply just as much to continuous probability distributions. For example, we can create mathematical, empirical and posterior probability distributions for continuous outcomes just as we did for discrete outcomes.\nThe empirical distribution involves using data from the National Health and Nutrition Examination Survey (NHANES).\n\n\n\n\n\n\n\n\n\nMathematical distribution is completely based on mathematical formula and assumptions, as in the coin flip example. In the coin-flip example, we assumed that the coin was perfectly fair, meaning that the probability of landing on heads or tails was equal. In this case, we make three assumptions. First, a male height follows a Normal distribution. Second, the average height of men is 175 cm. Third, the standard deviation for male height is 9 cm. We can create a Normal distribution using the rnorm() function with these two parameter values.\n\n\n\n\n\n\n\n\n\nAgain, the Normal distribution which is a probability distribution that is symmetric about the mean described by this formula.\n\\[y_i \\sim N(\\mu, \\sigma^2)\\]\n\nEach value \\(y_i\\) is drawn from a Normal distribution with parameters \\(\\mu\\) for the mean and \\(\\sigma\\) for the standard deviation. If the assumptions are correct, then, as our sample size increases, the empirical probability distribution will look more and more like the mathematical distribution.\nThe posterior distribution for heights depends on the context. Are we considering all the adult men in America? In that case, our posterior would probably look a lot like the empirical distribution using NHANES data. If we are being asked about the distribution of heights among players in the NBA, then our posterior might look like:\n\n\n\n\n\n\n\n\n\nCaveats:\n\nContinuous variables are a myth. Nothing that can be represented on a computer is truly continuous. Even something which appears continuous, like height, actually can only take on a (very large) set of discrete variables.\nThe math of continuous probability distributions can be tricky. Read a book on mathematical probability for all the messy details. Little of that matters in applied work.\nThe most important difference is that, with discrete distributions, it makes sense to estimate the probability of a specific outcome. What is the probability of rolling a 9? With continuous distributions, this makes no sense because there are an infinite number of possible outcomes. With continuous variables, we only estimate intervals.\n\n\nDon’t worry about the distinctions between discrete and continuous outcomes, or between the discrete and continuous probability distributions which we will use to summarize our beliefs about those outcomes. The basic intuition is the same in both cases.\n\n\n2.2.5 Joint distributions\n\nRecall that \\(P(\\text{coin})\\) is the probability distribution for the result of a coin toss. It includes two parts, the probability of heads (\\(\\rho_h\\)) and the probability of tails (\\(\\rho_t\\)). This is a univariate distribution because there is only one outcome, which can be heads or tails. If there is more than one outcome, then we have a joint distribution.\n\nJoint distributions are also mathematical objects that cover a set of outcomes, where each distinct outcome has a chance of occurring between 0 and 1 and the sum of all chances must equal 1. The key to a joint distribution is that it measures the chance that both outcome \\(a\\) from the set of events A and outcome \\(b\\) from the set of events B will occur. The notation is \\(P(A, B)\\).\nLet’s say that you are rolling two six-sided dice simultaneously. Dice 1 is weighted so that there is a 50% chance of rolling a 6 and a 10% chance of each of the other values. Dice 2 is weighted so there is a 50% chance of rolling a 5 and a 10% chance of rolling each of the other values. Let’s roll both dice 1,000 times. In previous examples involving two dice, we cared about the sum of results and not the outcomes of the first versus the second dice of each simulation. With a joint distributions, the outcomes for individual dice matter; so instead of 11 possible outcomes on the x-axis of our distribution plot (ranging from 2 to 12), we have 36 outcomes. Furthermore, a 2D probability distribution is not sufficient to represent all of the variables involved, so the joint distribution for this example is displayed using a 3D plot.\n\n\n\n\n\n\n\n\n\n\n\n2.2.6 Conditional distrubutions\nImagine that 60% of people in a community have a disease. A doctor develops a test to determine if a random person has the disease. However, this test isn’t 100% accurate. There is an 80% probability of correctly returning positive if the person has the disease and 90% probability of correctly returning negative if the person does not have the disease.\nThe probability of a random person having the disease is 0.6. Since each person either has the disease or doesn’t (those are the only two possibilities), the probability that a person does not have the disease is \\(1 - 0.6 =  0.4\\).\n\n\n\n\n\n\n\n\n\n\nIf a person has the disease, then we go up the top branch. The probability of an infected person testing positive is 0.8 because the test is 80% sure of correctly returning positive when the person has the disease.\nBy the same logic, if a person does not have the disease, we go down the bottom branch. The probability of the person incorrectly testing positive is 0.1.\n\nWe decide to go down the top branch if our random person has the disease. We go down the bottom branch if they do not. This is conditional probability. The probability of testing positive is dependent on whether the person has the disease.\nHow would you express this in statistical notation? \\(P(A|B)\\) is the same thing as the probability of A given B. \\(P(A|B)\\) means the probability of A if we know for sure the value of B. Note that \\(P(A|B)\\) is not the same thing as \\(P(B|A)\\).\nThere are three main categories of probability distributions: univariate, joint and condictional. \\(p(A)\\) is the probability distribution for event A. This is a univariate probability distribution because there is only one random variable. \\(p(A, B)\\) is the joint probability distribution of A and B. \\(p(A | B)\\) is the conditional probability distribution of A given that B has taken on a specific value. This is often written as \\(p(A | B = b)\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#sec-rlist-columns-and-map-functions",
    "href": "probability.html#sec-rlist-columns-and-map-functions",
    "title": "2  Probability",
    "section": "2.3 List-columns and map functions",
    "text": "2.3 List-columns and map functions\n\nWe need to expand our collection of R tricks by learning about list-columns and map_* functions. Recall that a list is different from an atomic vector. In atomic vectors, each element of the vector has one value. Lists, however, can contain vectors, and even more complex objects, as elements.\n\n\nShow the code\nx &lt;- list(c(4, 16, 9), c(\"A\", \"Z\"))\nx\n\n\n[[1]]\n[1]  4 16  9\n\n[[2]]\n[1] \"A\" \"Z\"\n\n\nx is a list with two elements. The first element is a numeric vector of length 3. The second element is a character vector of length 2. We use [[]] to extract specific elements.\n\n\nShow the code\nx[[1]]\n\n\n[1]  4 16  9\n\n\nThere are a number of built-in R functions that output lists. For example, ggplot objects store all of the plot information in a list. Any function that returns multiple values can be used to create a list output by wrapping that returned object with list().\n\n\nShow the code\nx &lt;- rnorm(10)\n\n# range() returns the min and max of the argument \n\nrange(x)\n\n\n[1] -0.7743019  2.3285564\n\n\nShow the code\n# We can create a tibble which includes the results of range(x)\n\ntibble(col_1 = list(range(x))) \n\n\n# A tibble: 1 × 1\n  col_1    \n  &lt;list&gt;   \n1 &lt;dbl [2]&gt;\n\n\nNotice this is a 1-by-1 tibble with one observation, which is a list of one element. Voila! You have just created a list-column.\nIf a function returns multiple values as a vector, like range() does, you must use list() as a wrapper if you want to create a list-column.\nA list column is a column of your data which is a list rather than an atomic vector. As with stand-alone list objects, you can pipe to str() to examine the column.\n\n\nShow the code\n# tibble() is what we use to generate a tibble, it acts sort of like the\n# mutate(), but mutate() needs a data frame to which it can add a new column,\n# tibble can survive by itself.\n\ntibble(col_1 = list(range(x))) |&gt;\n  str()\n\n\ntibble [1 × 1] (S3: tbl_df/tbl/data.frame)\n $ col_1:List of 1\n  ..$ : num [1:2] -0.774 2.329\n\n\nWe can use map_* functions to both create a list-column and then, much more importantly, work with that list-column afterwards.\n\n\nShow the code\n# .x is col_1 from tibble and ~ sum(.) is the formula\n\ntibble(col_1 = list(range(x))) |&gt;\n  mutate(col_2 = map_dbl(col_1, ~ sum(.))) |&gt; \n  str()\n\n\ntibble [1 × 2] (S3: tbl_df/tbl/data.frame)\n $ col_1:List of 1\n  ..$ : num [1:2] -0.774 2.329\n $ col_2: num 1.55\n\n\nmap_* functions, like map_dbl() in this example, take two key arguments, .x (the data which will be acted on) and .f (the function which will act on this data). Here, .x is the data in col_1, which is a list-column. .f is the function sum(). However, we can not simply write map_dbl(col_1, sum). Instead, each use of map_* functions requires the use of a tilde — a ~ — to indicate the start of the function and the use of a dot — a . — to specify where the data goes in the function.\nmap_* functions are a family of functions, with the suffix specifying the type of the object to be returned. map() itself returns a list. map_dbl() returns a double. map_int() returns an integer. map_chr() returns a character, and so on. Example:\n\n\nShow the code\n# Note that map_chr() requires that the relevant function return a character,\n# hence the use of as.character as a wrapper around sum(.).\n\ntibble(ID = 1) |&gt; \n  mutate(col_1 = map(ID, ~range(rnorm(10)))) |&gt;\n  mutate(col_2 = map_dbl(col_1, ~ sum(.))) |&gt; \n  mutate(col_3 = map_int(col_1, ~ length(.))) |&gt; \n  mutate(col_4 = map_chr(col_1, ~ as.character(sum(.)))) |&gt; \n  str()\n\n\ntibble [1 × 5] (S3: tbl_df/tbl/data.frame)\n $ ID   : num 1\n $ col_1:List of 1\n  ..$ : num [1:2] -2.233 0.862\n $ col_2: num -1.37\n $ col_3: int 2\n $ col_4: chr \"-1.37120168285811\"\n\n\nConsider a more detailed example:\n\n\nShow the code\n# This simple example demonstrates the workflow which we will often follow.\n# Start by creating a tibble which will be used to store the results. (Or start\n# with a tibble which already exists and to which you will be adding more\n# columns.) It is often convenient to get all the code working with just a few\n# rows. Once it is working, we increase the number of rows to a thousand or\n# million or whatever we need.\n\ntibble(ID = 1:3) |&gt; \n  \n  # The big convenience is being able to store a list in each row of the tibble.\n  # Note that we are not using the value of ID in the call to rnorm(). (That is\n  # why we don't have a \".\" anywhere.) But we are still using ID as a way of\n  # iterating through each row; ID is keeping count for us, in a sense.\n  \n  mutate(draws = map(ID, ~ rnorm(10))) |&gt; \n  \n  # Each succeeding step of the pipe works with columns already in the tibble\n  # while, in general, adding more columns. The next step calculates the max\n  # value in each of the draw vectors. We use map_dbl() because we know that\n  # max() will returns a single number.\n  \n  mutate(max = map_dbl(draws, ~ max(.))) |&gt; \n  \n  # We will often need to calculate more than one item from a given column like\n  # draws. For example, in addition to knowing the max value, we would like to\n  # know the range. Because the range is a vector, we need to store the result\n  # in a list column. map() does that for us automatically.\n  \n  mutate(min_max = map(draws, ~ range(.)))\n\n\n# A tibble: 3 × 4\n     ID draws        max min_max  \n  &lt;int&gt; &lt;list&gt;     &lt;dbl&gt; &lt;list&gt;   \n1     1 &lt;dbl [10]&gt; 0.982 &lt;dbl [2]&gt;\n2     2 &lt;dbl [10]&gt; 2.62  &lt;dbl [2]&gt;\n3     3 &lt;dbl [10]&gt; 2.12  &lt;dbl [2]&gt;\n\n\nThis flexibility is only possible via the use of list-columns and map_* functions. This workflow is extremely common. We start with an empty tibble, using ID to specify the number of rows. With that skeleton, each step of the pipe adds a new column, working off a column which already exists.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#two-models",
    "href": "probability.html#two-models",
    "title": "2  Probability",
    "section": "2.4 Two models",
    "text": "2.4 Two models\n\n\n\nThe simplest possible setting for inference involves two models — meaning two possible states of the world — and two outcomes from an experiment. Imagine that there is a disease — Probophobia, an irrational fear of probability — which you either have or don’t have. We don’t know if you have the diseases, but we do assume that there are only two possibilities.\nWe also have a test which is 99% accurate when given to a person who has Probophobia. Unfortunately, the test is only 50% accurate for people who do not have Probophobia. In this experiment, there only two possible outcomes: a positive and a negative result on the test.\nQuestion: If you test positive, what is the probability that you have Probophobia?\nMore generally, we are estimating a conditional probability. Conditional on the outcome of a postive test, what is the probability that you have Probophobia? Mathematically, we want:\n\\[ P(\\text{Probophobia | Test = Postive} ) \\]\nTo answer this question, we need to use the tools of joint and conditional probability from earlier in the Chapter. We begin by building, by hand, the joint distribution of the possible models (you have the Probophobia or you do not) and of the possible outcomes (you test positive or negative). Building the joint distribution involves assuming that each model is true and then creating the distribution of outcomes which might occur if that assumption is true.\nFor example, assume you have Probophobia. There is then a 50% chance that you test positive and a 50% chance you test negative. Similarly, if we assume that the second model is true — that you don’t have Probophobia — then there is 1% chance you test positive and a 99% you chance negative. Of course, for you (or any individual) we do not know for sure what is happening. We do not know if you have the disease. We do not know what your test will show. But we can use these relationships to construct the joint distribution.\n\n\n\nShow the code\n# Pipes generally start with tibbles, so we start with a tibble which just\n# includes an ID variable. We don't really use ID. It is just handy for getting\n# organized. We call this object `jd_disease`, where the `jd` stands for\n# joint distribution.\n\nsims &lt;- 10000\n\njd_disease &lt;- tibble(ID = 1:sims, have_disease = rep(c(TRUE, FALSE), 5000)) |&gt;\n  mutate(positive_test =\n           if_else(have_disease,\n                   map_int(have_disease, ~ rbinom(n = 1, size = 1, p = 0.99)),\n                   map_int(have_disease, ~ rbinom(n = 1, size = 1, p = 0.5))))\n\n\n\njd_disease\n\n\n# A tibble: 10,000 × 3\n      ID have_disease positive_test\n   &lt;int&gt; &lt;lgl&gt;                &lt;int&gt;\n 1     1 TRUE                     1\n 2     2 FALSE                    1\n 3     3 TRUE                     1\n 4     4 FALSE                    1\n 5     5 TRUE                     1\n 6     6 FALSE                    0\n 7     7 TRUE                     1\n 8     8 FALSE                    1\n 9     9 TRUE                     1\n10    10 FALSE                    0\n# ℹ 9,990 more rows\n\n\nThe first step is to simply create an tibble that consists of the simulated data we need to plot our distribution. Keep in mind that in the setting we have two different probabilities and they are completely separate from each other and we want to keep the two probabilities and the disease results in two and only two columns so that we can graph using the ggplot() function. And that’s why we used the rep and seq functions when creating the table, we used the seq function to set the sequence we wants, in this case is only two numbers, 0.01 (99% accuracy for testing negative if no disease, therefore 1% for testing positive if no disease) and 0.5 (50% accuracy for testing positive/negative if have disease), then we used the rep functions to repeat the process 10,000 times for each probability, in total 20,000 times. Note that this number “20,000” also represent the number of observations in our simulated data, we simulated 20,000 results from testing, where 10,000 results from the have-disease group and 10,000 for the no-disease group, we often use the capital N to represent the population, in this simulated data \\(N=20,000\\).\nPlot the joint distribution:\n\n\n\n\n\n\n\n\n\nBelow is a joint distribution displayed in 3D. Instead of using the “jitter” feature in R to unstack the dots, we are using a 3D plot to visualize the number of dots in each box. The number of people who correctly test negative is far greater than of the other categories. The 3D plot shows the total number of cases for each section (True positive, True negative, False positive, False negative),the 3D bar coming from those combinations. Now,pay attention to the two rows of the 3D graph, if you trying to add up the length of the 3D bar for the top two sections and the bottom two sections, they should be equal to each other, where each have 10,000 case. This is because we simulate the experience in two independent and separate world one in the have-disease world and one in the no-disease world.\n\n\n\n\n\n\n\n\n\n\n\nThis Section is called “Two Models” because, for each person, there are two possible states of the world: have the disease or not have the disease. By assumption, there are no other outcomes. We call these two possible states of the world “models,” even though they are very simple models.\nIn addition to the two models, we have two possible results of our experiment on a given person: test positive or test negative. Again, this is an assumption. We do not allow for any other outcome. In coming sections, we will look at more complex situations where we consider more than two models and more than two possible results of the experiment. In the meantime, we have built the unnormalized joint distribution for models and results. This is a key point! Look back earlier in this Chapter for discussions about both unnormalized distributions and joint distributions.\nWe want to analyze these plots by looking at different slices. For instance, let’s say that you have tested positive for the disease. Since the test is not always accurate, you cannot be 100% certain that you have it. We isolate the slice where the test result equals 1 (meaning positive).\n\n\nShow the code\njd_disease |&gt; \n  filter(positive_test == 1)\n\n\n# A tibble: 7,484 × 3\n      ID have_disease positive_test\n   &lt;int&gt; &lt;lgl&gt;                &lt;int&gt;\n 1     1 TRUE                     1\n 2     2 FALSE                    1\n 3     3 TRUE                     1\n 4     4 FALSE                    1\n 5     5 TRUE                     1\n 6     7 TRUE                     1\n 7     8 FALSE                    1\n 8     9 TRUE                     1\n 9    11 TRUE                     1\n10    12 FALSE                    1\n# ℹ 7,474 more rows\n\n\nMost people who test positive are infected This is a result for common diseases like cold. We can easily create an unnormalized conditional distribution with:\n\n\n\n\n\n\n\n\n\nfilter() transforms a joint distribution into a conditional distribution.\nTurn this unnormalized distribution into a posterior probability distribution:\n\n\n\n\n\n\n\n\n\nIf we zoom in on the plot, about 70% of people who tested positive have the disease and 30% who tested positive do not have the disease. In this case, we are focusing on one slice of the probability distribution where the test result was positive. There are two disease outcomes: positive or negative. By isolating a section, we are looking at a conditional distribution. Conditional on a positive test, you can visualize the likelihood of actually having the disease versus not.\nNow recalled the question we asked at the start of the session:\nIf you test positive, what is the probability that you have Probophobia?\nBy looking at the posterior graph we just create, we can answer this question easily:\nWith a positive test, you can be almost 70% sure that you have Probophobia, however there is a good chance about 30% that you receive a false positive, so don’t worry too much there is still about a third of hope that you get the wrong result\nNow let’s consider the manipulation of this posterior, here is another question. Question : 10 people walks up to testing center, 5 of them tested negative, 5 of them tested positive, what is the probability of at least 6 people are actually healthy? \n\n\nShow the code\ntibble(test = 1:100000) |&gt;\n  mutate(person1 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.3))) |&gt;\n  mutate(person2 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.3))) |&gt;\n  mutate(person3 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.3))) |&gt;\n  mutate(person4 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.3))) |&gt;\n  mutate(person5 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.3))) |&gt;\n  mutate(person6 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.7))) |&gt;\n  mutate(person7 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.7))) |&gt;\n  mutate(person8 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.7))) |&gt;\n  mutate(person9 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.7))) |&gt;\n  mutate(person10 = map_int(test, ~ rbinom(n = 1, size = 1, p = 0.7))) |&gt;\n  select(!test) |&gt; \n  \n  # The tricky part of this code is that we want to sum the outcomes across the\n  # rows of the tibble. This is different from our usual approach of summing\n  # down the columns, as with summarize(). The way to do this is to, first, use\n  # rowwise() to tell R that we want to work with rows in the tibble and then,\n  # second, use c_across() to indicate which variables we want to work with.\n  \n  rowwise() |&gt; \n  mutate(total = sum(c_across(person1:person10))) |&gt;\n  \n  ggplot(aes(total)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   binwidth = 1,\n                   color = \"white\") +\n    scale_x_continuous(breaks = c(0:10)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#three-models",
    "href": "probability.html#three-models",
    "title": "2  Probability",
    "section": "2.5 Three models",
    "text": "2.5 Three models\n\n\n\n\n\n\n\n\n\n\nImagine that your friend gives you a bag with two marbles. There could either be two white marbles, two black marbles, or one of each color. Thus, the bag could contain 0% white marbles, 50% white marbles, or 100% white marbles. The proportion, \\(p\\), of white marbles could be, respectively, 0, 0.5, or 1.\nQuestion: What is the chance of the bag contains exactly two white marbles, given that when we selected the marbles three times, every time we select a white marble?\n\\[ P(\\text{2 White Marbles in bag | White Marbles Sampled = 3} ) \\] Just as during the Probophobia models, in order to answer this question, we need to start up with the simulated data and then graphing out the joint distribution of this scenerio because we need to considered all possible outcomes of this model, and then based on the joint distribution we can slice out the the part we want (the conditional distribution) in the end making a posterior graph as well as normalizing it to see the probability.\nStep 1: Simulate the data into an tibble\nLet’s say you take a marble out of the bag, record whether it’s black or white, then return it to the bag. You repeat this three times, observing the number of white marbles you see out of three trials. You could get three whites, two whites, one white, or zero whites as a result of this trial. We have three models (three different proportions of white marbles in the bag) and four possible experimental results. Let’s create 3,000 draws from this joint distribution:\n\n\nShow the code\n# Create the joint distribution of the number of white marbles in the bag\n# (in_bag) and the number of white marbles pulled out in the sample (in_sample),\n# one-by-one. in_bag takes three possible values: 0, 1 and 2, corresponding to\n# zero, one and two white marbles potentially in the bag.\n\nset.seed(3)\nsims &lt;- 10000\n\n# We also start off with a tibble. It just makes things easier\n\njd_marbles &lt;- tibble(ID = 1:sims) |&gt; \n  \n  # For each row, we (randomly!) determine the number of white marbles in the\n  # bag. We do not know why the `as.integer()` hack is necessary. Shouldn't\n  # `map_int()` automatically coerce the result of `sample()` into an integer?\n  \n  mutate(in_bag = map_int(ID, ~ as.integer(sample(c(0, 1, 2), \n                                                  size = 1)))) |&gt;\n  \n  # Depending on the number of white marbles in the bag, we randomly draw out 0,\n  # 1, 2, or 3 white marbles in our experiment. We need `p = ./2` to transform\n  # the number of white marbles into the probability of drawing out a white\n  # marble in a single draw. That probability is either 0%, 50% or 100%.\n  \n  mutate(in_sample = map_int(in_bag, ~ rbinom(n = 1, \n                                              size = 3, \n                                              p = ./2))) \n\njd_marbles\n\n\n# A tibble: 10,000 × 3\n      ID in_bag in_sample\n   &lt;int&gt;  &lt;int&gt;     &lt;int&gt;\n 1     1      0         0\n 2     2      1         3\n 3     3      2         3\n 4     4      1         1\n 5     5      2         3\n 6     6      2         3\n 7     7      1         0\n 8     8      2         3\n 9     9      0         0\n10    10      1         2\n# ℹ 9,990 more rows\n\n\nStep 2: Plot the joint distribution:\n\n\nShow the code\n# The distribution is unnormalized. All we see is the number of outcomes in each\n# \"bucket.\" Although it is never stated clearly, we are assuming that there is\n# an equal likelihood of 0, 1 or 2 white marbles in the bag.\n\njd_marbles |&gt;\n  ggplot(aes(x = in_sample, y = in_bag)) +\n    geom_jitter(alpha = 0.5) +\n    labs(title = \"Black and White Marbles\",\n         subtitle = \"More white marbles in bag mean more white marbles selected\",\n         x = \"White Marbles Selected\",\n         y = \"White Marbles in the Bag\") +\n    scale_y_continuous(breaks = c(0, 1, 2)) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\nHere is the 3D visualization:\n\n\n\n\n\n\n\n\n\nThe y-axes of both the scatterplot and the 3D visualization are labeled “Number of White Marbles in the Bag.” Each value on the y-axis is a model, a belief about the world. For instance, when the model is 0, we have no white marbles in the bag, meaning that none of the marbles we pull out in the sample will be white.\nNow recalls the question, we essentially only care about the fourth column in the joint distribution (x-axis=3) because the question is asking us to create a conditional distribution given that fact that 3 marbles were selected. Therefore, we could isolate the slice where the result of the simulation involves three white marbles and zero black ones. Here is the unnormalized probability distribution.\nStep 3: Plot the unnormalized conditional distribution.\n\n\nShow the code\n# The key step is the filter. Creating a conditional distribution from a joint\n# distribution is the same thing as filtering that joint distribution for a\n# specific value. A conditional distribution is a \"slice\" of the joint\n# distribution, and we take that slice with filter().\n\njd_marbles |&gt; \n  filter(in_sample == 3) |&gt; \n  ggplot(aes(in_bag)) +\n    geom_histogram(binwidth = 0.5, color = \"white\") +\n    labs(title = \"Unnormalized Conditional Distribution\",\n         subtitle = \"Number of white marbles in bag given that three were selected in the sample\",\n         x = \"Number of White Marbles in the Bag\",\n         y = \"Count\") +\n    coord_cartesian(xlim = c(0, 2)) +\n    scale_x_continuous(breaks = c(0, 1, 2)) +\n    theme_classic()\n\n\n\n\n\n\n\n\n\nStep 4: Plot the normalize posterior distribution. Next, let’s normalize the distribution.\n\n\nShow the code\njd_marbles |&gt; \n  filter(in_sample == 3) |&gt; \n  ggplot(aes(in_bag)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))), \n                   binwidth = 0.5, \n                   color = \"white\") +\n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"Number of white marbles in bag given that three were selected in the sample\",\n         x = \"Number of White Marbles in the Bag\",\n         y = \"Probability\") +\n    coord_cartesian(xlim = c(0, 2)) +\n    scale_x_continuous(breaks = c(0, 1, 2)) +\n    scale_y_continuous(labels = \n                         scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\n\n\n\n\n\nThis plot makes sense because when all three marbles you draw out of the bag are white, there is a pretty good chance that there are no black marbles in the bag. But you can’t be certain! It is possible to draw three white even if the bag contains one white and one black. However, it is impossible that there are zero white marbles in the bag.\nLastly let’s answer the question: What is the chance of the bag contains exactly two white marbles, given that when we selected the white marbles three times, everytime we select a white marble?\nAnswer: As the Posterior Probability Distribution shows (x-axis=2), the chance of the bag contains exactly two white marbles given that we select 3 white marbles out of three tries is about 85%.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#sec-n-models",
    "href": "probability.html#sec-n-models",
    "title": "2  Probability",
    "section": "2.6 N models",
    "text": "2.6 N models\n\n\n\n\n\n\n\n\n\n\n\n\nAssume that there is a coin with \\(\\rho_h\\). We guarantee that there are only 11 possible values of \\(\\rho_h\\): \\(0, 0.1, 0.2, ..., 0.9, 1\\). In other words, there are 11 possible models, 11 things which might be true about the world. This is just like situations we have previously discussed, except that there are more models to consider.\nWe are going to run an experiment in which you flip the coin 20 times and record the number of heads. What does this result tell you about the value of \\(\\rho_h\\)? Ultimately, we will want to calculate a posterior distribution of \\(\\rho_h\\), which is written as p(\\(\\rho_h\\)).\nQuestion: What is the probability of getting exactly 8 heads out of 20 tosses?\nTo start, it is useful to consider all the things which might happen if, for example, \\(\\rho_h = 0.4\\). Fortunately, the R functions for simulating random variables makes this easy.\n\n\n\n\n\n\n\n\n\nFirst, notice that many different things can happen! Even if we know, for certain, that \\(\\rho_h = 0.4\\), many outcomes are possible. Life is remarkably random. Second, the most likely result of the experiment is 8 heads, as we would expect. Third, we have transformed the raw counts of how many times each total appeared into a probability distribution. Sometimes, however, it is convenient to just keep track of the raw counts. The shape of the figure is the same in both cases.\n\n\n\n\n\n\n\n\n\nEither way, the figures show what would have happened if that model — that \\(\\rho_h = 0.4\\) — were true.\nWe can do the same thing for all 11 possible models, calculating what would happen if each of them were true. This is somewhat counterfactual since only one of them can be true. Yet this assumption does allow us to create the joint distribution of models which might be true and of data which our experiment might generate. Let’s simplify this is p(models, data), although you should keep the precise meaning in mind.\n\n\n\n\n\n\n\n\n\nHere is the 3D version of the same plot.\n\n\n\n\n\n\n\n\n\nIn both of these diagrams, we see 11 models and 21 outcomes. We don’t really care about the p(\\(models\\), \\(data\\)), the joint distribution of the models-which-might-be-true and the data-which-our-experiment-might-generate. Instead, we want to estimate \\(p\\), the unknown parameter which determines the probability that this coin will come up heads when tossed. The joint distribution alone can’t tell us that. We created the joint distribution before we had even conducted the experiment. It is our creation, a tool which we use to make inferences. Instead, we want the conditional distribution, p(\\(models\\) | \\(data = 8\\)). We have the results of the experiment. What do those results tell us about the probability distribution of \\(p\\)?\nTo answer this question, we simply take a vertical slice from the joint distribution at the point of the x-axis corresponding to the results of the experiment.\nThis animation shows what we want to do with joint distributions. We take a slice (the red one), isolate it, rotate it to look at the conditional distribution, normalize it (change the values along the current z-axis from counts to probabilities), then observe the resulting posterior.\n\nThis is the only part of the joint distribution that we care about. We aren’t interested in what the object looks like where, for example, the number of heads is 11. That portion is irrelevant because we observed 8 heads, not 11. By using the filter function on the simulation tibble we created, we can conclude that there are a total of 465 times in our simulation in which 8 heads were observed.\nAs we would expect, most of the time when 8 coin tosses came up heads, the value of \\(p\\) was 0.4. But, on numerous occasions, it was not. It is quite common for a value of \\(p\\) like 0.3 or 0.5 to generate 8 heads. Consider:\n\n\n\n\n\n\n\n\n\nYet this is a distribution of raw counts. It is an unnormalized density. To turn it into a proper probability density (i.e., one in which the sum of the probabilities across possible outcomes sums to one) we just divide everything by the total number of observations.\n\n\n\n\n\n\n\n\n\nSolution:\nThe most likely value of \\(\\rho_h\\) is 0.4, as before. But, it is much more likely that \\(p\\) is either 0.3 or 0.5. And there is about an 8% chance that \\(\\rho_h \\ge 0.6\\).\nYou might be wondering: what is the use of a model? Well, let’s say we toss the coin 20 times and get 8 heads again. Given this result, we can ask: What is the probability that future samples of 20 flips will result in 10 or more heads?\nThere are three main ways you could go about solving this problem with simulations.\nThe first wrong way to do this is assuming that \\(\\rho_h\\) is certain because we observed 8 heads after 20 tosses. We would conclude that 8/20 gives us 0.4. The big problem with this is that you are ignoring your uncertainty when estimating \\(\\rho_h\\). This would lead us to the following code.\n\n\nShow the code\nsims &lt;- 10000000\n\nodds &lt;- tibble(sim_ID = 1:sims) |&gt;\n  mutate(heads = map_int(sim_ID, ~ rbinom(n = 1, size = 20, p = .4))) |&gt; \n  mutate(above_ten = if_else(heads &gt;= 10, TRUE, FALSE))\n\nodds\n\n\n# A tibble: 10,000,000 × 3\n   sim_ID heads above_ten\n    &lt;int&gt; &lt;int&gt; &lt;lgl&gt;    \n 1      1    10 TRUE     \n 2      2     5 FALSE    \n 3      3     2 FALSE    \n 4      4    10 TRUE     \n 5      5     5 FALSE    \n 6      6    10 TRUE     \n 7      7     7 FALSE    \n 8      8    11 TRUE     \n 9      9     9 FALSE    \n10     10     9 FALSE    \n# ℹ 9,999,990 more rows\n\n\n\n\nShow the code\nodds |&gt;\n  ggplot(aes(x=heads,fill=above_ten))+\n           geom_histogram(aes(y = after_stat(count/sum(count))),bins = 50)+\n  scale_fill_manual(values = c('grey50', 'red'))+\n  labs(title = \"Posterior Probability Distribution (Wrong Way)\",\n         subtitle = \"Number of heads in 20 tosses\",\n         x = \"Number of heads\",\n         y = \"Probability\",\n         fill = \"Above ten heads\") + \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\n\n\n\n\n\nUsing this Posterior distribution derived from the (wrong way) simulated data, the probability results in 10 or more head is\n\n\nShow the code\nodds |&gt;\n  summarize(success = sum(above_ten)/sims)\n\n\n# A tibble: 1 × 1\n  success\n    &lt;dbl&gt;\n1   0.245\n\n\nabout 24.5%.\nThe second method involves sampling the whole posterior distribution vector we previously created. This would lead to the following correct code.\n\n\nShow the code\np_draws &lt;- tibble(p = rep(seq(0, 1, 0.1), 1000)) |&gt;\n  mutate(heads = map_int(p, ~ rbinom(n = 1, size = 20, p = .))) |&gt;\n  filter(heads == 8)\n  \nodds_2nd &lt;- tibble(p = sample(p_draws$p, size = sims, replace = TRUE)) |&gt;\n  mutate(heads = map_int(p, ~ rbinom(n = 1, size = 20, p = .))) |&gt; \n  mutate(above_ten = if_else(heads &gt;= 10, TRUE, FALSE)) \n\nodds_2nd\n\n\n# A tibble: 10,000,000 × 3\n       p heads above_ten\n   &lt;dbl&gt; &lt;int&gt; &lt;lgl&gt;    \n 1   0.4     7 FALSE    \n 2   0.3     8 FALSE    \n 3   0.5    13 TRUE     \n 4   0.4    10 TRUE     \n 5   0.4     6 FALSE    \n 6   0.5     8 FALSE    \n 7   0.5     9 FALSE    \n 8   0.5    12 TRUE     \n 9   0.5    10 TRUE     \n10   0.4     8 FALSE    \n# ℹ 9,999,990 more rows\n\n\n\n\nShow the code\nodds_2nd |&gt;\n  ggplot(aes(x = heads,fill = above_ten))+\n           geom_histogram(aes(y = after_stat(count/sum(count))),bins = 50)+\n  scale_fill_manual(values = c('grey50', 'red'))+\n  labs(title = \"Posterior Probability Distribution (Right Way)\",\n         subtitle = \"Number of heads in 20 tosses\",\n         x = \"Number of heads\",\n         y = \"Probability\",\n         fill = \"Above ten heads\") + \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\n\n\n\n\n\nUsing this Posterior distribution derived from the (right way 1st) simulated data, the probability results in 10 or more head is\n\n\nShow the code\nodds_2nd |&gt;\n  summarize(success = sum(above_ten)/sims)\n\n\n# A tibble: 1 × 1\n  success\n    &lt;dbl&gt;\n1   0.351\n\n\nabout 32.8%\nAs you may have noticed, if you calculated the value using the first method, you would believe that getting 10 or more heads is less likely than it really is. If you were to run a casino based on these assumptions, you will lose all your money. It is very important to be careful about the assumptions you are making. We tossed a coin 20 times and got 8 heads. However, you would be wrong to assume that \\(\\rho_h\\) = 0.4 just based on this result.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#working-with-probability-distributions",
    "href": "probability.html#working-with-probability-distributions",
    "title": "2  Probability",
    "section": "2.7 Working with probability distributions",
    "text": "2.7 Working with probability distributions\n\n\n2.7.1 Random variables\n\n2.7.1.1 sample()\nThe most common distributions you will work with are empirical or frequency distributions, the values of age in the trains tibble, the values of poverty in the kenya tibble, and so on. But we can also create our own data by making “draws” from a distribution which we have concocted.\nConsider the distribution of the possible values from rolling a fair die. We can use the sample() function to create draws from this distribution, meaning it will change (or sometimes stay the same) for every subsequent draw.\n\n\nShow the code\ndie &lt;- c(1, 2, 3, 4, 5, 6)\n\nsample(x = die, size = 1)\n\n\n[1] 3\n\n\nThis produces one “draw” from the distribution of the possible values of one roll of fair six-sided die.\nNow, suppose we wanted to roll this die 10 times. One of the arguments of the sample() function is replace. We must specify it as TRUE if values can appear more than once. Since, when rolling a die 10 times, we expect that a value like 3 can appear more than once, we need to set replace = TRUE.\n\n\nShow the code\nsample(x = die, size = 10, replace = TRUE)\n\n\n [1] 5 6 3 3 3 4 3 6 4 5\n\n\nIn other words, rolling a 1 on the first roll should not preclude you from rolling a 1 on a later roll.\nWhat if the die is not “fair,” meaning that some sides are more likely to appear than others? The final argument of the sample() function is the prob argument. It takes a vector (of the same length as the initial vector x) that contains all of the probabilities of selecting each one of the elements of x. Suppose that the probability of rolling a 1 was 0.5, and the probability of rolling any other value is 0.1. (These probabilities should sum to 1. If they don’t sample() will automatically re-scale them.)\n\n\nShow the code\nsample(x = die, \n       size = 10, \n       replace = TRUE, \n       prob = c(0.5, 0.1, 0.1, 0.1, 0.1, 0.1))\n\n\n [1] 1 1 3 1 2 1 1 6 1 1\n\n\n\nRemember: There is no real data here. We have not actually rolled a die. We have just made some assumptions about what would happen if we were to roll a die. With those assumptions we have built an urn — a data generating mechanism — from which we can draw as many values as we like. Let’s roll the unfair die 10,000 times.\n\n\nShow the code\ntibble(result = sample(x = die, \n                       size = 10000, \n                       replace = TRUE, \n                       prob = c(0.5, rep(0.1, 5)))) |&gt; \n  ggplot(aes(x = result)) +\n    geom_bar() +\n    labs(title = \"Distribution of Results of an Unfair Die\",\n         x = \"Result of One Roll\",\n         y = \"Count\") +\n    scale_x_continuous(breaks = 1:6,\n                       labels = as.character(1:6)) +\n    scale_y_continuous(labels = scales::comma_format())\n\n\n\n\n\n\n\n\n\nsample() is just one of many functions for creating draws — or, more colloquially, “drawing” — from a distribution. Three of the most important functions are: runif(), rbinom(), and rnorm().\n\n\n2.7.1.2 runif()\nConsider a “uniform” distribution. This is the case in which every outcome in the range of possible outcomes has the same chance of occurring. The function runif() (spoken as “r-unif”) enables us to draw from a uniform contribution. runif() has three arguments: n, min, and max. runif() will produce n draws from between min and max, with each value having an equal chance of occurring.\n\n\nShow the code\nrunif(n = 10, min = 4, max = 6)\n\n\n [1] 5.594800 4.981960 4.601993 5.738417 4.879622 5.155263 4.319479 5.627666\n [9] 4.800110 5.634794\n\n\nMathematically, we would notate:\n\\[y_i \\sim U(4, 6)\\]\nThis means that the each value for \\(y\\) is drawn from a uniform distribution between four and six.\n\n\n2.7.1.3 rbinom()\nConsider binomial distribution, the case in which the probability of some Boolean variable (for instance success or failure) is calculated for repeated, independent trials. One common example would be the probability of flipping a coin and landing on heads. The function rbinom() allows us to draw from a binomial distribution. This function takes three arguments, n, size, and prob.\nn is the number of values we seek to draw. size is the number of trials for each n. *prob is the probability of success on each trial.\nSuppose we wanted to flip a fair coin one time, and let landing on heads represent success.\n\n\nShow the code\nrbinom(n = 1 , size = 1, prob = 0.5)\n\n\n[1] 0\n\n\nDo the same thing 100 times:\n\n\nShow the code\ntibble(heads = rbinom(n = 100, size = 1, prob = 0.5)) |&gt; \n  ggplot(aes(x = heads)) +\n    geom_bar() +\n    labs(title = \"Flipping a Fair Coin 100 Times\",\n         x = \"Result\",\n         y = \"Count\") +\n    scale_x_continuous(breaks = c(0, 1),\n                       labels = c(\"Tails\", \"Heads\"))\n\n\n\n\n\n\n\n\n\nIn our graph above, we use the function scale_x_continuous() because our x-axis variable is continuous, meaning it can take on any real values. The breaks argument to scale_x_continuous() converts our x-axis to having two different “tick marks”. There is a fairly even distribution of Tails and Heads. More draws would typically result in an even more equal split.\nRandomness creates (inevitable) tension between distribution as a “thing” and distribution as a vector of draws from that thing. In this case, the vector of draws is not balanced between Tails and Heads. Yet, we “know” that it should be since the coin is, by definition, fair. In a sense, the mathematics require an even split. Yet, randomness means that the vector of draws will rarely match the mathematically “true” result. And that is OK! First, randomness is an intrinsic property of the real world. Second, we can make the effect of randomness be as small as we want by increasing the number of draws.\n\nSuppose instead we wanted to simulate an unfair coin, where the probability of landing on Heads was 0.75 instead of 0.25.\n\n\nShow the code\ntibble(heads = rbinom(n = 100, size = 1, prob = 0.75)) |&gt; \n  ggplot(aes(x = heads)) +\n    geom_bar() +\n    labs(title = \"Flipping a Fair Coin 100 Times\",\n         x = \"Result\",\n         y = \"Count\") +\n    scale_x_continuous(breaks = c(0, 1),\n                       labels = c(\"Tails\", \"Heads\"))\n\n\n\n\n\n\n\n\n\nThe distribution — the imaginary urn — from which we draw the results of a coin flip for a fair coin is a different distribution — a different imaginary urn — from the distribution for a biased coin. In fact, there are an infinite number of distributions. Yet as long as we can draw values from a distribution, we can work with it. Mathematics:\n\\[y_i \\sim B(n, p)\\]\nEach value for \\(y\\) is drawn from a binomial distribution with parameters \\(n\\) for the number of trials and \\(p\\) for the probability of success.\nInstead of each n consisting of a single trial, we could have situation in which we are, 10,000 times, flipping a coin 10 times and summing up, for each experiment, the number of heads. In that case:\n\n\nShow the code\nset.seed(9)\ntibble(heads = rbinom(n = 10000, size = 10, prob = 0.5)) |&gt; \n  ggplot(aes(x = heads)) +\n    geom_bar() +\n    labs(title = \"Flipping a Fair Coin 10 Times\",\n         subtitle = \"Extreme results are possible with enough experiments\",\n         x = \"Total Number of Heads in Ten Flips\",\n         y = \"Count\") +\n    scale_x_continuous(breaks = 0:10)\n\n\n\n\n\n\n\n\n\n\n\n2.7.1.4 rnorm()\nThe most important distribution is the normal distribution. Mathematics:\n\\[y_i \\sim N(\\mu, \\sigma^2)\\]\nEach value \\(y_i\\) is drawn from a normal distribution with parameters \\(\\mu\\) for the mean and \\(\\sigma\\) for the standard deviation.\nThis bell-shaped distribution is defined by two parameters: (1) the mean \\(\\mu\\) (spoken as “mu”) which locates the center of the distribution and (2) the standard deviation \\(\\sigma\\) (spoken as “sigma”) which determines the variation of values around that center. In the figure below, we plot three normal distributions where:\n\nThe solid normal curve has mean \\(\\mu = 5\\) & standard deviation \\(\\sigma = 2\\).\nThe dotted normal curve has mean \\(\\mu = 5\\) & standard deviation \\(\\sigma = 5\\).\nThe dashed normal curve has mean \\(\\mu = 15\\) & standard deviation \\(\\sigma = 2\\).\n\n\n\n\n\n\nThree normal distributions.\n\n\n\n\nNotice how the solid and dotted line normal curves have the same center due to their common mean \\(\\mu\\) = 5. However, the dotted line normal curve is wider due to its larger standard deviation of \\(\\sigma = 5\\). On the other hand, the solid and dashed line normal curves have the same variation due to their common standard deviation \\(\\sigma = 2\\). However, they are centered at different locations.\nWhen the mean \\(\\mu = 0\\) and the standard deviation \\(\\sigma = 1\\), the normal distribution has a special name. It’s called the standard normal distribution or the \\(z\\)-curve.\nFurthermore, if a variable follows a normal curve, there are three rules of thumb we can use:\n\n68% of values will lie within \\(\\pm\\) 1 standard deviation of the mean.\n95% of values will lie within \\(\\pm\\) 1.96 \\(\\approx\\) 2 standard deviations of the mean.\n99.7% of values will lie within \\(\\pm\\) 3 standard deviations of the mean.\n\nLet’s illustrate this on a standard normal curve. The dashed lines are at -3, -1.96, -1, 0, 1, 1.96, and 3. These 7 lines cut up the x-axis into 8 segments. The areas under the normal curve for each of the 8 segments are marked and add up to 100%. For example:\n\nThe middle two segments represent the interval -1 to 1. The shaded area above this interval represents 34% + 34% = 68% of the area under the curve. In other words, 68% of values.\nThe middle four segments represent the interval -1.96 to 1.96. The shaded area above this interval represents 13.5% + 34% + 34% + 13.5% = 95% of the area under the curve. In other words, 95% of values.\nThe middle six segments represent the interval -3 to 3. The shaded area above this interval represents 2.35% + 13.5% + 34% + 34% + 13.5% + 2.35% = 99.7% of the area under the curve. In other words, 99.7% of values.\n\n\n\n\n\n\nRules of thumb about areas under normal curves.\n\n\n\n\n\n\n\n2.7.2 Summary statistics\nA probability distribution is not always easy to work with. It is a complex object. And, in many contexts, we don’t really care about all that complexity. So, instead of providing the full probability distribution, we often just use a summary measure, a number or two or three which captures those aspects of the entire distribution which are relevant to the matter at hand. Let’s explore these issues using the 538 posterior probability distribution, as of August 13, 2020, for the number of electoral votes which will be won by Joe Biden. Here is a tibble with 1,000,000 draws from that distribution:\n\n\nShow the code\ndraws\n\n\n# A tibble: 1,000,000 × 2\n      ID electoral_votes\n   &lt;int&gt;           &lt;int&gt;\n 1     1             388\n 2     2             474\n 3     3             279\n 4     4             400\n 5     5             239\n 6     6             364\n 7     7             198\n 8     8             319\n 9     9             336\n10    10             294\n# ℹ 999,990 more rows\n\n\nA distribution and a sample of draws from that distribution are different things. But, if you squint, they are sort of the same thing, at least for our purposes. For example, if you want to know the mean of the distribution, then the mean of the draws will be a fairly good estimate, especially if the number of draws is large enough.\n\nRecall how we can draw randomly from specified probability distributions:\n\n\nShow the code\nrnorm(10)\n\n\n [1]  0.95693552  0.18860245 -2.04561627  0.41344817  0.26100764  1.18315071\n [7]  0.07832782 -1.14338230 -0.98240598  1.88883159\n\n\n\n\nShow the code\nrunif(10)\n\n\n [1] 0.30482858 0.08068409 0.65942947 0.86727544 0.54974073 0.20976843\n [7] 0.77876867 0.09990737 0.74968824 0.19082733\n\n\nThe elements of these vectors are all “draws” from the specified probability distributions. In most applied situations, our tools will produce draws rather than summary objects. Fortunately, a vector of draws is very easy to work with. Start with summary statistics:\n\n\nShow the code\n# recall mean, media, standard deviation and mad functions.\n\nkey_stats &lt;- draws |&gt; \n  summarize(mn = mean(electoral_votes),\n            md = median(electoral_votes),\n            sd = sd(electoral_votes),\n            mad = mad(electoral_votes))\n\nkey_stats\n\n\n# A tibble: 1 × 4\n     mn    md    sd   mad\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  325.   326  86.9  101.\n\n\nCalculate a 95% interval directly:\n\n\nShow the code\nquantile(draws$electoral_votes, probs = c(0.025, 0.975))\n\n\n 2.5% 97.5% \n  172   483 \n\n\nApproximate the 95% interval in two ways:\n\n\nShow the code\nc(key_stats$mn - 2 * key_stats$sd, \n  key_stats$mn + 2 * key_stats$sd)\n\n\n[1] 151.5461 499.0198\n\n\nShow the code\nc(key_stats$md - 2 * key_stats$mad, \n  key_stats$md + 2 * key_stats$mad)\n\n\n[1] 124.3664 527.6336\n\n\nIn this case, using the mean and standard deviation produces a 95% interval which is closer to the true interval. In other cases, the median and scaled median absolute deviation will do better. Either approximation is generally “good enough” for most work. But, if you need to know the exact 95% interval, you must use quantile().",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#cardinal-virtues",
    "href": "probability.html#cardinal-virtues",
    "title": "2  Probability",
    "section": "2.8 Cardinal Virtues",
    "text": "2.8 Cardinal Virtues\nThe four Cardinal Virtues are Wisdom, Justice, Courage, and Temperance. Because data science is, ultimately, a moral act, we use these virtues to guide our work. Every data science project begins with a question.\n\nWisdom starts by creating the Preceptor Table. What data, if we had it, would allow us to answer our question easily? If the Preceptor Table has one outcome, then the model is predictive. If it has more than one (potential) outcome, then the model is causal. We then explore the data we have. You can never look too closely at your data. Key question: Are the data we have close enough to the data we want (i.e., the Preceptor Table) that we can consider both as coming from the same population? If not, we can’t proceed further. Key in making that decision is the assumption of validity. Do the columns in the Preceptor Table match the columns in the data?\nJustice starts with the Population Table – the data we want to have (i.e., the Preceptor Table), the data which we actually have, and all the other data from that same population. Each row of the Population Table is defined by a unique Unit/Time combination. We explore three key issues about the Population Table. First, does the relationship among the variables demonstrate stability, meaning is the model stable across different time periods? Second, are the rows associated with the data and, separately, the rows associated with the Preceptor Table representative of all the units from the population? Third, for causal models only, we consider unconfoundedness.\nCourage allows us to explore different models. Justice gave us the Population Table. Courage creates the data generating mechanism. We begin with the basic mathematical structure of the model. With that structure in mind, we decide which variables to include. We estimate the values of the unknown parameters. We avoid hypothesis tests. We check our models for consistency with the data we have. We select one model.\nTemperance guides us in the use of the model we have created to answer the questions with which we began. We create posteriors of quantities of interest. We should be modest in the claims we make. The posteriors we create are never the “truth.” The assumptions we made to create the model are never perfect. Yet decisions made with flawed posteriors are almost always better than decisions made without them.\n\n\n2.8.1 Wisdom\n\nAll we can know is that we know nothing. And that’s the height of human wisdom. — Leo Tolstoy\n\n\n\n\n\nWisdom.\n\n\n\n\nWisdom helps us decide if we can even hope to answer our question with the data we have.\nBegin with the Preceptor Table. What rows and columns of data do you need such that, if you had them all, the calculation of the Quantity of Interest would be trivial? If you want to know the average height of an adult in India, then the Preceptor Table would include a row for each adult and a column for their height. With no missing data, the average is easy to determine, as are a wide variety of other estimands, other unknown values.\nOne key aspect of this Preceptor Table is whether or not we need more than one potential outcome in order to calculate our estimand. For example, if we want to know the causal effect of exposure to Spanish-speakers on attitude toward immigration then we need a causal model, one which estimates that attitude for each person under both treatment and control. The Preceptor Table would require two columns for the outcome. If, on the other hand, we only want to predict someone’s attitude, or compare one person’s attitude to another’s, then we would only need a Preceptor Table with one column for the outcome. Are we are modeling (just) for prediction or are we (also) modeling for causation? Predictive models care nothing about causation. Causal models are often also concerned with prediction, if only as a means of measuring the quality of the model.\nEvery model is predictive, in the sense that, if we give you new data — and it is drawn from the same population — then you can create a predictive forecast. But only a subset of those models are causal, meaning that, for a given individual, you can change the value of one input and figure out what the new output would be and then, from that, calculate the causal effect by looking at the difference between two potential outcomes.\nWith prediction, all we care about is forecasting \\(Y\\) given \\(X\\) on some as-yet-unseen data. But there is no notion of “manipulation” in such models. We don’t pretend that, for Joe, we could turn variable \\(X\\) from a value of \\(5\\) to a value of \\(6\\) by just turning some knob and, by doing so, cause Joe’s value of \\(Y\\) to change from \\(17\\) to \\(23\\). We can compare two people (or two groups of people), one with \\(X\\) equal to \\(5\\) and one with \\(X\\) equal to \\(6\\), and see how they differ in \\(Y\\). The basic assumption of predictive models is that there is only one possible \\(Y\\) for Joe. There are not, by assumption, two possible values for \\(Y\\), one if \\(X\\) equals \\(5\\) and another if \\(X\\) equals \\(6\\). The Preceptor Table has a single column under \\(Y\\) if that is all we need to answer the question.\nWith causal inference, however, we can consider the case of Joe with \\(X = 5\\) and Joe with \\(X = 6\\). The same mathematical model can be used. And both models can be used for prediction, for estimating what the value of \\(Y\\) will be for a yet-unseen observation with a specified value for \\(X\\). But, in this case, instead of only a single column in the Preceptor Table for \\(Y\\), we have at least two (and possibly many) such columns, one for each of the potential outcomes under consideration.\nThe difference between predictive models and causal models is that the former have one column for the outcome variable and the latter have more than one column.\nSecond, we look at the data we have and perform an exploratory data analysis, an EDA. You can never look at your data too much. The most important variable is the one we most want to understand/explain/predict. In the models we create in later chapters, this variable will go on the left-hand side of our mathematical equations. Some academic fields refer to this as the “dependent variable.” Others use terms like “response” or “outcome.” Whatever the terminology, we need to explore the distribution of this variable, its min/max/range, its mean and median, its standard deviation, and so on.\nGelman, Hill, and Vehtari (2020) write:\n\nMost important is that the data you are analyzing should map to the research question you are trying to answer. This sounds obvious but is often overlooked or ignored because it can be inconvenient. Optimally, this means that the outcome measure should accurately reflect the phenomenon of interest, the model should include all relevant predictors, and the model should generalize to the cases to which it will be applied.\n\n\nFor example, with regard to the outcome variable, a model of incomes will not necessarily tell you about patterns of total assets. A model of test scores will not necessarily tell you about child intelligence or cognitive development. …\n\nWe care about other variables as well, especially those that are most correlated/connected with the outcome variable. The more time that we spend looking at these variables, the more likely we are to create a useful model.\nThird, the (almost always imaginary) population is key. We need the data we want — the Preceptor Table — and the data we have to be similar enough that we can consider them as all having come from the same statistical population. From Wikipedia:\n\nIn statistics, a population is a set of similar items or events which is of interest for some question or experiment. A statistical population can be a group of existing objects (e.g. the set of all stars within the Milky Way galaxy) or a hypothetical and potentially infinite group of objects conceived as a generalization from experience (e.g. the set of all opening hands in all the poker games in Las Vegas tomorrow).\n\nMechanically, assuming that the Preceptor Table and the data are drawn from the same population is the same thing as “stacking” the two on top of each other. For that to make sense, the variables must mean the same thing — at least mostly — in both cases. This is the assumption of validity. For example, if the Preceptor Table concerns who people will vote for in the election next week and the data is from a survey taken last week, it is not obvious that we can consider the data as coming from the same population. After all, voting and survey responses are not exactly the same thing. We can only assume that they are — which is precisely what everyone who forecasts elections does — if we assume that both are “valid” measures of the same underlying construct.\nIf we assume that the data we have is drawn from the same population as the data in the Preceptor Table, then we can use information about the former to make inferences about the latter. We can combine the Preceptor Table and the data into a single Population Table. If we can’t do that, if we can’t assume that the two sources come from the same population, then we can’t use our data to answer our questions. We have no choice but to walk away. The heart of Wisdom is knowing when to walk away. As John Tukey noted:\n\nThe combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.\n\n\n\n2.8.2 Justice\n\n\n\n\n\nJustice.\n\n\n\n\nJustice begins with a Population Table. It includes rows for the data we have and the data we want to have. It will almost always have many more rows, rows from the larger population of which the Preceptor Table and our data form a portion. It has missing values, most importantly for potential outcomes which were not observed. The central problem in inference is to fill in the missing values in the Population Table.\nThere are three key issues to explore in any Population Table: stability, representativeness and unconfoundedness.\n\nStability assumes that the relationship between the outcome variable and the covariates is consistent over time and over different parts of the population. Never forget the temporal nature of almost all real data science problems. Our Preceptor Table will focus on rows for today or for the near future. The data we have will always be from before now. We must almost always assume that the future will be like the past in order to use data from the past to make predictions about the future.\nRepresentativeness is a two-sided concern. We want the data we have to be representative of the population for which we need to calculate parameters. Ideally, we would love for our data to be randomly sampled from the population, but this is almost never the case. But this is a concern, not just with our data, but also for our Preceptor Table. If the data we want is not representative of the entire population then we will need to be careful in the inferences which we draw.\n\nValidity is about the columns in our Population Table. Stability and representativeness are about the rows.\n\n\n2.8.3 Courage\n\n\n\n\n\nCourage.\n\n\n\n\n\nThe first step of Courage is to make an assumption about the structure of the data generating mechanism (DGM): the mathematical formula, and associated error term, which relates our outcome variable to our covariates.\nCourage requires math. Consider a model of coin-tossing:\n\\[ H_i  \\sim B(\\rho_h, n = 20) \\]\nThe total number \\(H\\) of Heads in experiment \\(i\\) with 20 flips of a single coin, \\(H_i\\), is distributed as a binomial with \\(n = 20\\) and an unknown probability \\(\\rho_h\\) of the coin coming up Heads.\n\nNote:\n\nThis is a cheat and a simplification! We are Bayesians but we have not specified the full Bayesian machinery. We really need priors on the unknown parameter \\(\\rho_h\\) as well. But that is too complex for an introductory textbook, so we wave our hands, accept the default sensible parameters built into the R packages we use, and point readers to more advanced books, like Gelman, Hill, and Vehtari (2020).\nDefining \\(\\rho_h\\) as the “the probability that the coin comes up Heads” is a bit of a fudge. If you calculate that by hand and then compare it to what our tools produce, they won’t be the same. Instead, the calculated value will be closer to zero. Why? \\(\\rho_h\\) is really the “long-run percentage of the time the coin comes up Heads.” It is not just the percentage from this experiment.\nIn this simple case, we are fortunate that the parameter \\(\\rho_h\\) has such a (mostly!) simple analog to a real world quantity. Most of the time, parameters are not so easy to interpret. In a more complex model, especially those with interaction terms, we focus less on parameters and more on model predictions.\n\nThe three languages of data science are words, math and code, and the most important of these is code.\nWe need to explain the structure of our model using all three languages, but we need Courage to implement the model in code.\n\nCourage requires us to take the general mathematical formula and then make it specific. Which variables should we include in the model and which do we exclude? Every data science project involves the creation of several models. For each, we specify the precise data generating mechanism. Using that formula, and some R code, we create a fitted model. All models have parameters. We can never know the true values of the parameters, but we can create, and explore, posterior distributions for those unknown true values.\nCode allows us to “fit” a model by estimating the values of the unknown parameters, like \\(\\rho_h\\). Sadly, we can never know the true values of these parameters. But, like all good statisticians, we can express our uncertain knowledge in the form of posterior probability distributions. With those distributions, we can compare the actual values of the outcome variable with the “fitted” or “predicted” results of the model. We can examine the “residuals,” the difference between the fitted and actual values.\nEvery outcome is the sum of two parts: the model and what is not in the model:\n\\[outcome = model + what\\ is\\ not\\ in\\ the\\ model\\]\nIt doesn’t matter what the outcome is. It could be the result of a coin flip, the weight of a person, the GDP of a country. Whatever outcome we are considering is always made up of two parts. The first is the model we have created. The second is all the stuff — all the blooming and buzzing complexity of the real world — which is not a part of the model.\nSome of our uncertainty is driven by our ignorance about \\(\\rho_h\\).\nA parameter is something which does not exist in the real world. (If it did, or could, then it would be data.) Instead, a parameter is a mental abstraction, a building block which we will use to to help us accomplish our true goal: To replace at least some of the questions marks in the actual Preceptor Table. Since parameters are mental abstractions, we will always be uncertain as to their value, however much data we might collect.\nBut some, often most, of the uncertainty associated with the outcome derives from forces that are, by assumption, not in the model. For example, if the coin is fair, we expect \\(H_i\\) to equal 10. But, often, it will be different, even if we are correct and \\(\\rho_h\\) equals exactly 0.5.\nRandomness is intrinsic to this fallen world.\nNull hypothesis testing is a mistake. There is only the data, the models and the summaries therefrom.\n\n\n2.8.4 Temperance\n\n\n\n\n\nTemperance.\n\n\n\n\n\n\nThere are few more important concepts in statistics and data science than the “Data Generating Mechanism.” Our data — the data that we collect and see — has been generated by the complexity and confusion of the world. God’s own mechanism has brought His data to us. Our job is to build a model of that process, to create, on the computer, a mechanism which generates fake data consistent with the data which we see. With that DGM, we can answer any question which we might have. In particular, with the DGM, we provide predictions of data we have not seen and estimates of the uncertainty associated with those predictions. We can fill in the missing values in the Preceptor Table and then, easily, calculate all Quantities of Interest.\nJustice gave us the Population Table. Courage created the DGM, the fitted model. Temperance will guide us in its use.\nHaving created (and checked) a model, we now use the model to answer questions. Models are made for use, not for beauty. The world confronts us. Make decisions we must. Our decisions will be better ones if we use high quality models to help make them.\nSadly, our models are never as good as we would like them to be. First, the world is intrinsically uncertain.\n\n\n\n\n\nDonald Rumsfeld.\n\n\n\n\n\nThere are known knowns. There are things we know we know. We also know there are known unknowns. That is to say, we know there are some things we do not know. But there are also unknown unknowns, the ones we do not know we do not know. – Donald Rumsfeld\n\nWhat we really care about is data we haven’t seen yet, mostly data from tomorrow. But what if the world changes, as it always does? If it doesn’t change much, maybe we are OK. If it changes a lot, then what good will our model be? In general, the world changes some. That means that our forecasts are more uncertain that a naive use of our model might suggest.\n\nWe need Temperance in order to study and understand the unknown unknowns in our model. Temperance is also important when we analyze the “realism” of our models. When we created the mathematical probability distribution for presidential elections, for instance, we assumed that the Democratic candidate would have a 50% chance of winning each vote in the electoral college. By comparing the mathematical model to our empirical cases, however, we recognize that the mathematical model is unlikely to be true. The mathematical model suggested that getting fewer than 100 votes is next to impossible, but many past Democratic candidates in the empirical distribution received less than 100 electoral votes.\nIn Temperance, the key distinction is between the true posterior distribution — what we will call “Preceptor’s Posterior” — and the estimated posterior distribution. Recall our discussion from Section 2.1. Imagine that every assumption we made in Wisdom and Justice were correct, that we correctly understand every aspect of how the world works. We still would not know the unknown value we are trying to estimate — recall the Fundamental Problem of Causal Inference — but the posterior we created would be perfect. That is Preceptor’s Posterior. Sadly, even if our estimated posterior is, very close to Preceptor’s Posterior, we can never be sure of that fact, because we can never know the truth, never be certain that all the assumptions we made are correct.\nEven worse, we must always worry that our estimated posterior, despite all the work we put into creating it, is far from the truth. We, therefore, must be cautious in our use of that posterior, humble in our claims about its accuracy. Using our posterior, despite its faults, is better than not using it. Yet it is, as best, a distorted map of reality, a glass through which we must look darkly. Use your posteriors with humility.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#summary",
    "href": "probability.html#summary",
    "title": "2  Probability",
    "section": "2.9 Summary",
    "text": "2.9 Summary\n\nThroughout this Chapter, we spent time going through examples of conditional distributions. However, it’s worth noting that all probability distributions are conditional on something. Even in the most simple examples, when we were flipping a coin multiple times, we were assuming that the probability of getting heads versus tails did not change between tosses.\nWe discussed the difference between empirical, mathematical, and posterior probability distributions. Even though we developed these heuristics to better understand distributions, every time we make a claim about the world, it is based on our beliefs - what we think about the world. We could be wrong. Two reasonable people can have conflicting beliefs about the fairness of a coin.\nIt is useful to understand the three types of distributions and the concept of conditional distributions, but almost every probability distribution is conditional and posterior. We can leave out both words in future discussions, as we generally will in this book. They are implicit.\nIf you are keen to learn more about probability, here is a video featuring Professor Gary King. This is a great way to review some of the concepts we covered in this chapter, albeit at a higher level of mathematics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Analytical Methods for Social Research. Cambridge University Press. https://doi.org/10.1017/9781139161879.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "sampling.html",
    "href": "sampling.html",
    "title": "3  Sampling",
    "section": "",
    "text": "3.1 Real sampling activity\nThe urn below has a certain number of red and a certain number of white beads all of equal size, mixed well together. What proportion, \\(\\rho\\), of this urn’s beads are red?\nAn urn with red and white beads.\nOne way to answer this question would be to perform an exhaustive count: remove each bead individually, count the number of red beads, count the number of white beads, and divide the number of red beads by the total number of beads. Call that ratio \\(\\rho\\), the proportion of red beads in the urn. However, this would be a long and tedious process. Therefore, we will use sampling!\nTo begin this chapter, we will look at a real sampling activity: the urn. Then, we will simulate the urn example using R code. This will help us to understand the standard error and the ways in which uncertainty factors into our predictions. We then attempt to estimate a single parameter, the proportion of red beads in the urn. Use the tidyverse package.\nShow the code\nlibrary(tidyverse)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "sampling.html#sec-sampling-activity",
    "href": "sampling.html#sec-sampling-activity",
    "title": "3  Sampling",
    "section": "",
    "text": "3.1.1 Using the shovel method once\nInstead of performing an exhaustive count, let’s insert a shovel into the urn and remove \\(5 \\cdot 10 = 50\\) beads. We are taking a sample of the total population of beads.\n\n\n\n\n\nInserting a shovel into the urn.\n\n\n\n\n\n\n\n\n\nRemoving 50 beads from the urn.\n\n\n\n\nObserve that 17 of the 50 sampled beads are red and thus \\(17/50 = 0.34 = 34\\%\\) of the shovel’s beads are red. We can view the proportion of beads that are red in this shovel as a guess of the proportion of beads that are red in the entire urn. While not as exact as doing an exhaustive count of all the beads in the urn, our guess of 34% took much less time and energy to make.\nRecall that \\(\\rho\\) is the true value of the proportion of red beads. There is only one \\(\\rho\\). Our guesses at the proportion of red beads are known as \\(\\hat{\\rho}\\) (pronounced p hat), where \\(\\hat{\\rho}\\) is the estimated value of \\(\\rho\\). There are many ways to estimate \\(\\rho\\), each leading to a (potentially) different \\(\\hat{\\rho}\\). The 34% value for \\(\\hat{\\rho}\\) came from taking this sample. But, if we used the shovel again, we would probably come up with a different \\(\\hat{\\rho}\\). There are many possible \\(\\hat{\\rho}\\)’s. You and I will often differ in our estimates. We each have a different \\(\\hat{\\rho}\\) even though we agree that there is only one \\(\\rho\\).\nStart this activity over again from the beginning, placing the 50 beads back into the urn. Would a second sample include exactly 17 red beads? Maybe, but probably not.\nWhat if we repeated this activity many times? Would our guess at the proportion of the urn’s beads that are red, \\(\\hat{\\rho}\\), be exactly 34% every time? Surely not.\nLet’s repeat this exercise with the help of 33 groups of friends to understand how the value of \\(\\hat{\\rho}\\) varies across 33 independent trials.\n\n\n3.1.2 Using the shovel 33 times\nEach of our 33 groups of friends will do the following:\n\nUse the shovel to remove 50 beads each.\nCount the number of red beads and compute the proportion of the 50 beads that are red.\nReturn the beads into the urn.\nMix the contents of the urn to not let a previous group’s results influence the next group’s.\n\nEach of our 33 groups of friends make note of their proportion of red beads from their sample collected. Each group then marks their proportion of their 50 beads that were red in the appropriate bin in a hand-drawn histogram as seen below.\n\n\n\n\n\nConstructing a histogram of proportions.\n\n\n\n\nHistograms allow us to visualize the distribution of a numerical variable. In particular, where the center of the values falls and how the values vary. A partially completed histogram of the first 10 out of 33 groups of friends’ results can be seen in the figure below.\n\n\n\n\n\nHand-drawn histogram of first 10 out of 33 proportions.\n\n\n\n\nObserve the following details in the histogram:\n\nAt the low end, one group removed 50 beads from the urn with proportion red between 0.20 and 0.25.\nAt the high end, another group removed 50 beads from the urn with proportion between 0.45 and 0.5 red.\nHowever, the most frequently occurring proportions were between 0.30 and 0.35 red, right in the middle of the distribution.\nThe distribution is somewhat bell-shaped.\n\ntactile_sample_urn saves the results from our 33 groups of friends.\n\n\n# A tibble: 33 × 4\n   group          red_beads prop_red group_ID\n   &lt;chr&gt;              &lt;dbl&gt;    &lt;dbl&gt;    &lt;int&gt;\n 1 Vignesh, Eliot        16     0.32        1\n 2 Caroline, Edna        15     0.3         2\n 3 Conrad, Vlad          16     0.32        3\n 4 Dohyun, Estel         18     0.36        4\n 5 Griffin, Mary         18     0.36        5\n 6 Mak, Sophie           17     0.34        6\n 7 Aaron, Mike           11     0.22        7\n 8 Ace, Chris            18     0.36        8\n 9 Yuki, Harry           21     0.42        9\n10 Ishan, Cass           15     0.3        10\n# ℹ 23 more rows\n\n\nFor each group, we are given their names, the number of red_beads they obtained, and the corresponding proportion out of 50 beads that were red, called prop_red. We also have an group_ID variable which gives each of the 33 groups a unique identifier. Each row can be viewed as one instance of a replicated activity: using the shovel to remove 50 beads and computing the proportion of those beads that are red.\nLet’s visualize the distribution of these 33 proportions using geom_histogram() with binwidth = 0.05. This is a computerized and complete version of the partially completed hand-drawn histogram you saw earlier.\n\n\nShow the code\ntactile_sample_urn |&gt;\n  ggplot(aes(x = prop_red)) +\n  \n  # Setting `boundary = 0.4` indicates that we want a binning scheme such that\n  # one of the bins' boundary is at 0.4. `color = \"white\"` modifies the color of\n  # the boundary for visual clarity.\n  \n  geom_histogram(binwidth = 0.05, \n                 boundary = 0.4, \n                 color = \"white\") +\n  \n  # Add scale_y_continuous with breaks by 1, as the default shows the y-axis\n  # from 1 to 10 with breaks at .5. Breaks by 1 is better for this plot, as all\n  # resulting values are integers.\n  \n  scale_y_continuous(breaks = seq(from = 0, to = 10, by = 1)) +\n  \n  # The call expression() is used to insert a mathematical expression, like\n  # p-hat. The paste after expression allows us to paste text prior to said\n  # expression.\n  \n  labs(x = expression(paste(\"Proportion, \", hat(rho), \", of 50 beads that were red\")),\n       y = \"Count\",\n       title = \"Proportions Red in 33 Samples\") \n\n\n\n\n\n\n\n\n\n\n\n3.1.3 What did we just do?\nWhat we just demonstrated in this activity is the statistical concept of sampling. We want to know the proportion of red beads in the urn, with the urn being our population. Performing an exhaustive count of the red and white beads would be too time-consuming. Therefore, it is much more practical to extract a sample of 50 beads using the shovel. Using this sample of 50 beads, we estimated the proportion of the urn’s beads that are red to be about 34%.\nMoreover, because we mixed the beads before each use of the shovel, the samples were random and independent. Because each sample was drawn at random, the samples were different from each other. This is an example of sampling variation. For example, what if instead of selecting 17 beads in our first sample we had selected just 11? Does that mean that the population proportion of the beads is 11/50 or 22%? No! Because we performed 33 trials we can look to our histogram, and see that the peak of the distribution occurs when \\(.35 &lt; \\hat{\\rho} &lt; .4\\) , so it is likely that the proportion of red beads in the entire population will also fall in or near this range.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "sampling.html#sec-virtual-sampling",
    "href": "sampling.html#sec-virtual-sampling",
    "title": "3  Sampling",
    "section": "3.2 Virtual sampling",
    "text": "3.2 Virtual sampling\nWe just performed a tactile sampling activity. We used a physical urn of beads and a physical shovel. We did this by hand so that we could develop our intuition about the ideas behind sampling. In this section, we mimic this physical sampling with virtual sampling, using a computer.\n\n3.2.1 Using the virtual shovel once\nVirtual sampling requires a virtual urn and a virtual shovel. Create a tibble named urn. The rows of urn correspond exactly to the contents of the actual urn.\n\n\nShow the code\n# set.seed() ensures that the beads in our virtual urn are always in the same\n# order. This ensures that the figures in the book match their written\n# descriptions. We want 40% of the beads to be red.\n\nset.seed(10)\n\nurn &lt;- tibble(color = c(rep(\"red\", 400), \n                        rep(\"white\", 600))) |&gt;\n  \n  # sample_frac() keeps all the rows in the tibble but rearranges their order.\n  # We don't need to do this. A virtual urn does not care about the order of the\n  # beads. But we find it aesthetically pleasing to mix them up.\n  \n  sample_frac() |&gt; \n  mutate(bead_ID = 1:1000) \n\nurn  \n\n\n# A tibble: 1,000 × 2\n   color bead_ID\n   &lt;chr&gt;   &lt;int&gt;\n 1 white       1\n 2 white       2\n 3 red         3\n 4 red         4\n 5 white       5\n 6 white       6\n 7 white       7\n 8 white       8\n 9 white       9\n10 white      10\n# ℹ 990 more rows\n\n\nObserve that urn has 1,000 rows, meaning that the urn contains 1,000 beads. The first variable bead_ID is used as an identification variable. None of the beads in the actual urn are marked with numbers. The second variable color indicates whether a particular virtual bead is red or white.\nNote that in this section, we used the variable bead_ID to keep track of each bead in our urn, while in the last section we used group_ID to keep track of the samples drawn by the 33 individual teams. This is a better strategy than naming both variables ID, as it would be much more likely for us to get them confused later on.\nOur virtual urn needs a virtual shovel. We use slice_sample() and some list-column mapping wizardry learned in Section 2.3 to take a sample of 50 beads from our virtual urn.\n\n\nShow the code\n# Define trial_ID as one instance of us sampling 50 beads from the urn. When\n# trial_ID is called within map(), we are performing slice_sample() upon our urn\n# once, and taking a sample of 50 beads. \n\ntibble(trial_ID = 1) |&gt; \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50)))\n\n\n# A tibble: 1 × 2\n  trial_ID shovel           \n     &lt;dbl&gt; &lt;list&gt;           \n1        1 &lt;tibble [50 × 2]&gt;\n\n\nAs usual, map functions and list-columns are powerful but confusing. The str() function is a good way to explore a tibble with a list-column.\n\n\nShow the code\ntibble(trial_ID = 1) |&gt; \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50))) |&gt; \n  str()\n\n\ntibble [1 × 2] (S3: tbl_df/tbl/data.frame)\n $ trial_ID: num 1\n $ shovel  :List of 1\n  ..$ : tibble [50 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ color  : chr [1:50] \"white\" \"white\" \"white\" \"red\" ...\n  .. ..$ bead_ID: int [1:50] 812 903 227 283 229 160 523 893 66 277 ...\n\n\nThere are two levels. There is one row in the tibble for each sample. So far, we have only drawn one sample. Within each row, there is a second level, the tibble which is the sample. That tibble has two variables: trial_ID and color. This is the advantage to using slice_sample(), because it selects all columns of our urn, whereas sample() can only sample from a single column. While identifying each individual bead may be irrelevant in our urn scenario, with other problems it could be very useful to have additional data about each individual.\nNow let’s add a column which indicates the number of red beads in the sample taken from the shovel.\n\n\n\nShow the code\ntibble(trial_ID = 1) |&gt; \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50))) |&gt; \n  \n  # To count the number of red beads in each shovel, we can use a lesser \n  # known property of the sum() function: By passing in a comparison \n  # expression, sum() will count the number of occurrences within a vector. \n  # In this case, we count the total number occurrences of the word red\n  # in the color column of shovel.\n\n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\")))\n\n\n# A tibble: 1 × 3\n  trial_ID shovel            numb_red\n     &lt;dbl&gt; &lt;list&gt;               &lt;int&gt;\n1        1 &lt;tibble [50 × 2]&gt;       20\n\n\nHow does this work? R evaluates if color == red, and treats TRUE values like the number 1 and FALSE values like the number 0. So summing the number of TRUEs and FALSEs is equivalent to summing 1’s and 0’s. In the end, this operation counts the number of beads where color equals “red”.\nFinally, calculate the proportion red by dividing numb_red (The number of red beads observed in the shovel), by the shovel size (We are using a shovel of size 50).\n\n\nShow the code\ntibble(trial_ID = 1) |&gt; \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50))) |&gt; \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |&gt; \n  mutate(prop_red = numb_red / 50)\n\n\n# A tibble: 1 × 4\n  trial_ID shovel            numb_red prop_red\n     &lt;dbl&gt; &lt;list&gt;               &lt;int&gt;    &lt;dbl&gt;\n1        1 &lt;tibble [50 × 2]&gt;       23     0.46\n\n\nCareful readers will note that the numb_red is changing in each example above. The reason, of course, is that each block re-runs the shovel exercise, and slice_sample will return a random number of red beads. If we wanted the same number in each block, we would need to use set.seed() each time, always providing the same seed each time.\nLet’s now perform the virtual analog of having 33 groups of students use the sampling shovel!\n\n\n3.2.2 Using the virtual shovel 33 times\nIn our tactile sampling exercise in Section 3.1, we had 33 groups of students use the shovel, yielding 33 samples of size 50 beads. We then used these 33 samples to compute 33 proportions.\nLet’s use our virtual sampling to replicate the tactile sampling activity in a virtual format. We’ll save these results in a data frame called virtual_samples.\n\n\nShow the code\nset.seed(9)\n\n virtual_samples &lt;- tibble(trial_ID = 1:33) |&gt;\n    mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50))) |&gt; \n    mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |&gt;\n    mutate(prop_red = numb_red / 50) \n\nvirtual_samples\n\n\n# A tibble: 33 × 4\n   trial_ID shovel            numb_red prop_red\n      &lt;int&gt; &lt;list&gt;               &lt;int&gt;    &lt;dbl&gt;\n 1        1 &lt;tibble [50 × 2]&gt;       21     0.42\n 2        2 &lt;tibble [50 × 2]&gt;       19     0.38\n 3        3 &lt;tibble [50 × 2]&gt;       17     0.34\n 4        4 &lt;tibble [50 × 2]&gt;       15     0.3 \n 5        5 &lt;tibble [50 × 2]&gt;       17     0.34\n 6        6 &lt;tibble [50 × 2]&gt;       21     0.42\n 7        7 &lt;tibble [50 × 2]&gt;        9     0.18\n 8        8 &lt;tibble [50 × 2]&gt;       21     0.42\n 9        9 &lt;tibble [50 × 2]&gt;       16     0.32\n10       10 &lt;tibble [50 × 2]&gt;       20     0.4 \n# ℹ 23 more rows\n\n\nLet’s visualize this variation in a histogram:\n\n\nShow the code\nvirtual_samples |&gt; \nggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, \n                 boundary = 0.4, \n                 color = \"white\") +\n  \n  # To use mathematical symbols in titles and labels, use the expression()\n  # function, as here.\n  \n  labs(x = expression(paste(\"Proportion, \", hat(rho), \", of 50 beads that were red\")),\n       y = \"Count\",\n       title = \"Distribution of 33 proportions red\") +\n  \n  # Label the y-axis in an attractive fashion. Without this code, the axis\n  # labels would include 2.5, which makes no sense because all the values are\n  # integers.\n  \n  scale_y_continuous(breaks = seq(2, 10, 2))\n\n\n\n\n\n\n\n\n\nSince binwidth = 0.05, this will create bins with boundaries at 0.30, 0.35, 0.45, and so on. Recall that \\(\\hat{\\rho}\\) is equal to the proportion of beads which are red in each sample.\nObserve that we occasionally obtained proportions red that are less than 30%. On the other hand, we occasionally obtained proportions that are greater than 45%. However, the most frequently occurring proportions were between 35% and 45%. Why do we have these differences in proportions red? Because of sampling variation.\nNow we will compare our virtual results with our tactile results from the previous section. Observe that both histograms are somewhat similar in their center and variation, although not identical. These slight differences are again due to random sampling variation. Furthermore, observe that both distributions are somewhat bell-shaped.\n\n\n\n\n\nComparing 33 virtual and 33 tactile proportions red. Note that, though the figures differ slightly, both are centered around .35 to .45. This shows that, in both sampling distributions, the most frequently occuring proportion red is between 35% and 45%.\n\n\n\n\nThis visualization allows us to see how our results differed between our tactile and virtual urn results. As we can see, there is some variation between our results. This is not a cause for concern, as there is always expected sampling variation between results.\n\n\n3.2.3 Using the virtual shovel 1,000 times\n\n\n\n\n\nSo much sampling, so little time.\n\n\n\n\nAlthough we took 33 samples from the urn in the previous section, we should never do that again! The advantage of modern technology is that we can use virtual simulation as many times as we choose, so we have no restrictions on resources. No longer are the days where we have to recruit our friends to tirelessly sample from the physical urn. We are now data scientists! 33 samples are useless to us. Instead, we use our simulations hundreds or thousands of times to create mathematical models that we can combine with our knowledge to answer our questions. In this section we’ll examine the effects of sampling from the urn 1,000 times.\nWe can reuse our code from above, making sure to replace 33 trials with 1,000.\n\n\nShow the code\nset.seed(9)\n\n virtual_samples &lt;- tibble(trial_ID = 1:1000) |&gt;\n    mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50))) |&gt; \n    mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |&gt;\n    mutate(numb_beads = map_int(shovel, ~ length(.$color))) |&gt; \n    mutate(prop_red = numb_red / numb_beads) \n\n\nNow we have 1,000 values for prop_red, each representing the proportion of 50 beads that are red in a sample. Using the same code as earlier, let’s visualize the distribution of these 1,000 replicates of prop_red in a histogram:\n\n\nShow the code\nvirtual_samples |&gt; \nggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.01, \n                 boundary = 0.4, \n                 color = \"white\") +\n  labs(x = expression(hat(rho)),\n       y = \"Count\",\n       title = \"Distribution of 1,000 proportions red\") \n\n\n\n\n\n\n\n\n\nWhy the empty spaces among the bars? Recall that, with only 50 beads, there are only 51 possible values for \\(\\hat{\\rho}\\): 0, 0.02, 0.04, …, 0.98, 1. A value of 0.31 or 0.47 is impossible, hence the gaps.\nThe most frequently occurring proportions of red beads occur, again, between 35% and 45%. Every now and then we observe proportions much higher or lower. This occurs because as we increase the number of trials, tails develop on our distribution as we are more likely to witness extreme \\(\\hat{\\rho}\\) values. The symmetric, bell-shaped distribution shown in the histogram is well approximated by the normal distribution.\nNow that we have a good understanding of virtual sampling, we can apply our knowledge to examine the effects of changing our virtual shovel size.\n\n\n3.2.4 The effect of different shovel sizes\n\n\n\n\n\nWhat happens if we use different sized shovels to sample?\n\n\n\n\nInstead of just one shovel, imagine we have three choices of shovels to extract a sample of beads with: shovels of size 25, 50, and 100. Using our newly developed tools for virtual sampling, let’s unpack the effect of having different sample sizes. Start by creating a tibble with 1,000 rows, each row representing an instance of us sampling from the urn with our chosen shovel size. Then, compute the resulting 1,000 replicates of proportion red. Finally, plot the distribution using a histogram.\n\n\nShow the code\n# Within slice_sample(), n = 25 represents our shovel of size 25. We also divide\n# by 25 to compute the proportion red.\n\nvirtual_samples_25 &lt;- tibble(trial_ID = 1:1000) |&gt; \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 25))) |&gt; \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |&gt; \n  mutate(prop_red = numb_red / 25)\n\nvirtual_samples_25 |&gt;\n  ggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.04, \n                 boundary = 0.4, \n                 color = \"white\") +\n  labs(x = expression(paste(\"Proportion, \", hat(rho), \", of 25 beads that were red\")), \n       title = \"25\") \n\n\n\n\n\n\n\n\n\nWe will repeat this process with a shovel size of 50.\n\n\nShow the code\nvirtual_samples_50 &lt;- tibble(trial_ID = 1:1000) |&gt; \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50))) |&gt; \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |&gt; \n  mutate(prop_red = numb_red / 50)\n\n\nvirtual_samples_50  |&gt;\n  ggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.04, \n                 boundary = 0.4, \n                 color = \"white\") +\n  labs(x = expression(paste(\"Proportion, \", hat(rho), \", of 50 beads that were red\")), \n       title = \"50\")  \n\n\n\n\n\n\n\n\n\nWe choose a bin width of .04 for all histograms to more easily compare different shovel sizes. Using a smaller bin size would result in gaps between the bars, as a shovel of size 50 has more possible \\(\\hat{\\rho}\\) values than a shovel of size 25.\nFinally, we will perform the same process with 1000 replicates to map the histogram using a shovel size of 100.\n\n\nShow the code\nvirtual_samples_100 &lt;- tibble(trial_ID = 1:1000) |&gt; \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 100))) |&gt; \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |&gt; \n  mutate(prop_red = numb_red / 100)\n\n\nvirtual_samples_100 |&gt;\n  ggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.04, \n                 boundary = 0.4, \n                 color = \"white\") +\n  labs(x = expression(paste(\"Proportion, \", hat(rho), \", of 100 beads that were red\")), \n       title = \"100\") \n\n\n\n\n\n\n\n\n\nFor easy comparison, we present the three resulting histograms in a single row with matching x and y axes:\n\n\nShow the code\n# Use bind_rows to combine the data from our three saved virtual sampling\n# objects. Use mutate() in each to clarify the n as the necessary number\n# of samples taken. This makes our data easier to interpret and prevents\n# duplicate elements.\n\nvirtual_prop &lt;- bind_rows(virtual_samples_25 |&gt; \n                            mutate(n = 25), \n                          virtual_samples_50 |&gt; \n                            mutate(n = 50), \n                          virtual_samples_100 |&gt; \n                            mutate(n = 100))\n\n# Plot our new object with the x-axis showing prop_red. Add elements binwidth,\n# boundary, and color for stylistic clarity. Use labs() to add an x-axis label\n# and title. Facet_wrap() splits the graph into multiple plots by the variable\n# (~n).\n\ncomparing_sampling_distributions &lt;- ggplot(virtual_prop, aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.04, boundary = 0.4, color = \"white\") +\n  labs(x = expression(paste(\"Proportion, \", hat(rho), \", of the beads that were red\")), \n       title = \"Comparing distributions of proportions red for three different shovel sizes.\") +\n  facet_wrap(~ n) \n\n# Inspect our new faceted graph. \n\ncomparing_sampling_distributions\n\n\n\n\n\nComparing the distributions of proportion red for different sample sizes (25, 50, 100). The important takeaway is that our center becomes more concentrated as our sample size increases, indicating a smaller standard deviation between our guesses.\n\n\n\n\nObserve that as the sample size increases, the histogram becomes taller and narrower. This is because the variation of the proportion red for each sample decreases. Remember: A large variation means there are a wide range of values that can be achieved, while smaller variations are concentrated around a specific value.\nThe Central Limit Theorem states, more or less, that when sample means are based on larger and larger sample sizes, the sampling distribution of these sample means becomes both narrower and more bell-shaped. In other words, the sampling distribution increasingly follows a normal distribution and the variation of this sampling distribution gets smaller, meaning smaller standard errors.\nWhy does variation decrease as sample size increases? If we use a large sample size like 100 or 500, our sample is much more representative of the population simply because more of the population is included. As a result, the proportion red in our sample (\\(\\hat{\\rho}\\)) will be closer to the population proportion (\\(\\rho\\)). On the other hand, smaller samples have much more variation because of our old friend chance. We are much more likely to have extreme samples that are not representative of our population.\nLet’s attempt to visualize the concept of variation a different way. For each sample size, let’s plot the proportion red for all 1,000 samples. With 3 different shovel sizes, we will have 3,000 total points, with each point representing an instance of sampling from the urn with a specific shovel size.\n\n\nShow the code\nvirtual_prop |&gt;\n  ggplot(aes(x = n, y = prop_red, color = as.factor(n))) +\n  geom_jitter(alpha = .15) + \n  labs(title = \"Results of 1,000 samples for 3 different shovel sizes.\",\n       subtitle = \"As shovel size increases, variation decreases.\",\n       y = \"Proportion red in sample\",\n       color = \"Shovel size\") +\n  \n  # We do not need an x axis, because the color of the points denotes the shovel size. \n   \n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())\n\n\n\n\n\n\n\n\n\nThis graph illustrates the exact same concept as the histogram. With the smallest shovel size there is significant variance from sample to sample, as samples take on a wide variety of sample proportions! However, as we increase the sample size, the points become more concentrated, or less variance.\nThere is also a third way to understand variation! We can be numerically explicit about the amount of variation in our three sets of 1,000 values of prop_red by using the standard deviation. A standard deviation is a summary statistic that measures the amount of variation within a numerical variable. For all three sample sizes, let’s compute the standard deviation of the 1,000 proportions red.\n\n\n\n\n\n\n\n\n\nComparing standard deviations of proportions red for three different shovels\n\n\nNumber of slots in shovel\nStandard deviation of proportions red\n\n\n\n\n25\n0.099\n\n\n50\n0.067\n\n\n100\n0.045\n\n\n\n\n\n\nComparing the number of slots in the shovel with the standard deviation of proportions red. Here, we see that standard deviation decreases with higher sample sizes. Larger sample sizes yield more precise estimates.\n\n\n \nAs the sample size increases, the sample to sample variation decreases, and our guesses at the true proportion of the urn’s beads that are red get more precise. The larger the shovel, the more precise the result.\n\n\n\n\n\nVariance appears everywhere in data science.\n\n\n\n\nLet’s take a step back from all the variance. The reality is that our code needs to be better optimized, as it is bad practice to make a separate tibble for each sample size. To make comparisons easier, let’s attempt to put all 3 shovel sizes in the same tibble using mapping.\n\n\nShow the code\ntibble(trial_ID = 1:1000) |&gt;\n  mutate(shovel_ID = map(trial_ID, ~c(25, 50, 100))) |&gt;\n  unnest(shovel_ID) |&gt;\n  mutate(samples = map(shovel_ID, ~slice_sample(urn, n = .))) |&gt;\n  mutate(num_red = map_int(samples, ~sum(.$color == \"red\"))) |&gt;\n  mutate(prop_red = num_red/shovel_ID)\n\n\n# A tibble: 3,000 × 5\n   trial_ID shovel_ID samples            num_red prop_red\n      &lt;int&gt;     &lt;dbl&gt; &lt;list&gt;               &lt;int&gt;    &lt;dbl&gt;\n 1        1        25 &lt;tibble [25 × 2]&gt;        7     0.28\n 2        1        50 &lt;tibble [50 × 2]&gt;       26     0.52\n 3        1       100 &lt;tibble [100 × 2]&gt;      37     0.37\n 4        2        25 &lt;tibble [25 × 2]&gt;        8     0.32\n 5        2        50 &lt;tibble [50 × 2]&gt;       15     0.3 \n 6        2       100 &lt;tibble [100 × 2]&gt;      46     0.46\n 7        3        25 &lt;tibble [25 × 2]&gt;        6     0.24\n 8        3        50 &lt;tibble [50 × 2]&gt;       21     0.42\n 9        3       100 &lt;tibble [100 × 2]&gt;      40     0.4 \n10        4        25 &lt;tibble [25 × 2]&gt;        9     0.36\n# ℹ 2,990 more rows\n\n\nTo those of us who do not completely understand mapping, do not fret! The tidyr package provides the expand_grid() function as a neat alternative. We can use expand_grid() and a new variable, shovel_size, to create a tibble which will organize our results. Instead of using 1,000 trials, let’s use 3 to get a feel for the function.\n\n\nShow the code\nexpand_grid(trial_ID = c(1:3), shovel_size = c(25, 50, 100))\n\n\n# A tibble: 9 × 2\n  trial_ID shovel_size\n     &lt;int&gt;       &lt;dbl&gt;\n1        1          25\n2        1          50\n3        1         100\n4        2          25\n5        2          50\n6        2         100\n7        3          25\n8        3          50\n9        3         100\n\n\nThe above sets the stage for simulating three samples for each of three different shovel sizes. Similar code as above can be used.\n\n\nShow the code\nexpand_grid(trial_ID = c(1:3), shovel_size = c(25, 50, 100)) |&gt; \n  mutate(shovel = map(shovel_size, ~ slice_sample(urn, n = .))) |&gt; \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |&gt; \n  mutate(prop_red = numb_red/shovel_size) \n\n\n# A tibble: 9 × 5\n  trial_ID shovel_size shovel             numb_red prop_red\n     &lt;int&gt;       &lt;dbl&gt; &lt;list&gt;                &lt;int&gt;    &lt;dbl&gt;\n1        1          25 &lt;tibble [25 × 2]&gt;         7     0.28\n2        1          50 &lt;tibble [50 × 2]&gt;        19     0.38\n3        1         100 &lt;tibble [100 × 2]&gt;       39     0.39\n4        2          25 &lt;tibble [25 × 2]&gt;         8     0.32\n5        2          50 &lt;tibble [50 × 2]&gt;        19     0.38\n6        2         100 &lt;tibble [100 × 2]&gt;       34     0.34\n7        3          25 &lt;tibble [25 × 2]&gt;         7     0.28\n8        3          50 &lt;tibble [50 × 2]&gt;        19     0.38\n9        3         100 &lt;tibble [100 × 2]&gt;       36     0.36\n\n\nAgain, we changed the second line to use shovel_size rather than trial_ID as the mapping variable since we can no longer hard code the shovel size into the call to slice_sample(). Expand to 1,000 simulations for each value of shovel_size and finish with a calculation of standard deviation.\n\n\nShow the code\nexpand_grid(trial_ID = c(1:1000), shovel_size = c(25, 50, 100)) |&gt; \n  mutate(shovel = map(shovel_size, ~ slice_sample(urn, n = .))) |&gt; \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |&gt; \n  mutate(prop_red = numb_red/shovel_size) |&gt;\n  summarize(st_dev_p_hat = sd(prop_red),\n            .by = shovel_size)\n\n\n# A tibble: 3 × 2\n  shovel_size st_dev_p_hat\n        &lt;dbl&gt;        &lt;dbl&gt;\n1          25       0.0967\n2          50       0.0682\n3         100       0.0453\n\n\nThis is, approximately, the same result as we saw above, but with 1 re-factored tibble instead of 3 separate ones. We can also use functions like expand_grid() in the future to make our code more concise.\nNow that we have this framework, there’s no need to limit ourselves to the sizes 25, 50, and 100. Why not try all integers from 1 to 100? We can use the same code, except we’ll now set shovel_size = 1:100.\n\n\nShow the code\nshovels_100 &lt;- expand_grid(trial_ID = c(1:1000), shovel_size = c(1:100)) |&gt; \n  mutate(shovel = map(shovel_size, ~ slice_sample(urn, n = .))) |&gt; \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |&gt; \n  mutate(prop_red = numb_red / shovel_size) |&gt; \n  summarize(st_dev_p_hat = sd(prop_red),\n            .by = shovel_size)\n\nglimpse(shovels_100)\n\n\nRows: 100\nColumns: 2\n$ shovel_size  &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n$ st_dev_p_hat &lt;dbl&gt; 0.48436516, 0.34093399, 0.27629253, 0.24069833, 0.2140195…\n\n\nNow, we have the standard deviation of prop_red for all shovel sizes from 1 to 100. Let’s plot that value to see how it changes as the shovel gets larger:\n\n\n\n\n\nComparing standard deviations of proportions red for 100 different shovels. The standard deviation decreases at the same rate as the square root of shovel size. The red line shows the standard error.\n\n\n\n\nThe red line here represents an important statistical concept: standard error (SE). As the shovel size increases, and thus our sample size increases, we find that the standard error decreases. If this is confusing right now, fear not! We will delve into the explanation of standard error in our next section.\n\n\n\n\n\nTo any poets and philosophers confused about this: don’t worry! It won’t be on a problem set.\n\n\n\n\nThis is the power of running many analyses at once using map functions and list columns: before, we could tell that the standard deviation was decreasing as the shovel size increased, but when only looking at shovel sizes of 25, 50, and 100, it wasn’t clear how quickly it was decreasing.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "sampling.html#sec-standard-errors",
    "href": "sampling.html#sec-standard-errors",
    "title": "3  Sampling",
    "section": "3.3 Standard error",
    "text": "3.3 Standard error\n\n\n\n\n\nStandard errors are just the way old people talk about confidence intervals.\n\n\n\n\nStandard errors (SE) quantify the effect of sampling variation on our estimates. In other words, they quantify how much we can expect the calculated proportions of a shovel’s beads that are red to vary from one sample to another sample to another sample, and so on. As a general rule, as sample size increases, the standard error decreases.\nThe standard error is the standard deviation of a sample statistic (aka point estimate), such as the proportion. For example, the standard error of the mean refers to the standard deviation of the distribution of sample means taken from a population.\nThe relationship between the standard error and the standard deviation is that, for a given sample size, the standard error equals the standard deviation divided by the square root of the sample size. Accordingly, the standard error is inversely proportional to the square root of the sample size. The larger the sample size, the smaller the standard error.\nIf this sounds confusing, don’t worry! It is. Before we can explain this in more depth, it is important to understand some terminology.\n\n3.3.1 Terminology and notation\n\n\n\n\n\nLet Yoda’s wisdom dull the pain of this terminology section.\n\n\n\n\nAll of the concepts underlying this terminology, notation, and definitions tie directly to the concepts underlying our tactile and virtual sampling activities. It will simply take time and practice to master them.\nFirst, a population is the set of relevant units. The population’s size is upper-case \\(N\\). In our sampling activities, the population is the collection of \\(N\\) = 1,000 identically sized red and white beads in the urn. This is about the simplest possible population. Other examples are all the adult men in the US, all the classrooms in a school, all the wheelbarrows in Massachusetts, all the values of your blood pressure, read at five minute intervals, for your entire life. Often, the population is extends over time, as with your blood pressure readings and is, therefore, more amorphous. Consider all the people who have run for governor of a US state since 1900, or all the people who will run for governor through 2050. Those are also populations.\nSecond, a population parameter is a numerical summary quantity about the population that is unknown, but you wish you knew. For example, when this quantity is the mean, the population parameter of interest is the population mean. This is mathematically denoted with the Greek letter \\(\\mu\\) pronounced “mu.” In our earlier sampling from the urn activity, however, since we were interested in the proportion of the urn’s beads that were red, the population parameter is the population proportion, denoted with \\(\\rho\\).\nThird, a census is an exhaustive enumeration or counting of all \\(N\\) units in the population in order to compute the population parameter’s value exactly. In our sampling activity, this would correspond to counting the number of red beads out of the \\(N\\) total in the urn and then computing the red population proportion, \\(\\rho\\), exactly. When the number \\(N\\) of individuals or observations in our population is large as was the case with our urn, a census can be quite expensive in terms of time, energy, and money. A census is impossible for any populations which includes the future, like our blood pressure next year or candidates for governor in 2050. There is a truth but we could not, even in theory, calculate it.\nFourth, sampling is the act of collecting a sample from the population when we can not, or do not want to, perform a census. The sample size is lower case \\(n\\), as opposed to upper case \\(N\\) for the population’s size. Typically the sample size \\(n\\) is much smaller than the population size \\(N\\). In our sampling activities, we used shovels with varying slots to extract samples of size \\(n\\) = 1 through \\(n\\) = 100.\nFifth, a point estimate, also known as a sample statistic, is a measure computed from a sample that estimates an unknown population parameter. In our sampling activities, recall that the unknown population parameter was the proportion of red beads and that this is mathematically denoted by \\(\\rho\\). Our point estimate is the sample proportion: the proportion of the shovel’s beads that are red. In other words, it is our guess at the proportion of the urn’s beads that are red. The point estimate of the parameter \\(\\rho\\) is \\(\\hat{\\rho}\\). The “hat” on top of the \\(\\rho\\) indicates that it is an estimate of the unknown population proportion \\(\\rho\\).\n\nSixth, a sample is said to be representative if it roughly looks like the population. In other words, are the sample’s characteristics a good representation of the population’s characteristics? In our sampling activity, are the samples of \\(n\\) beads extracted using our shovels representative of the urn’s \\(N\\) = 1000 beads?\nSeventh, a sample is generalizable if any results based on the sample can generalize to the population. In our sampling activity, can we generalize the sample proportion from our shovels to the entire urn? Using our mathematical notation, this is akin to asking if \\(\\hat{\\rho}\\) is a “good guess” of \\(\\rho\\)?\nEighth, biased sampling occurs if certain individuals or observations in a population have a higher chance of being included in a sample than others. We say a sampling procedure is unbiased if every observation in a population had an equal chance of being sampled. Had the red beads been much smaller than the white beads, and therefore more prone to falling out of the shovel, our sample would have been biased. In our sampling activities, since we mixed all \\(N = 1000\\) beads prior to each group’s sampling and since each of the equally sized beads had an equal chance of being sampled, our samples were unbiased.\nNinth, a sampling procedure is random if we sample randomly from the population in an unbiased fashion. In our sampling activities, this would correspond to sufficiently mixing the urn before each use of the shovel.\n\n\n\n\n\nFear not if you look like Spongebob after reading this section. We will re-cap right now!\n\n\n\n\nIn general:\n\nIf the sampling of a sample of size \\(n\\) is done at random, then\nthe sample is unbiased and representative of the population of size \\(N\\), thus\nany result based on the sample can generalize to the population, thus\nthe point estimate is a “good guess” of the unknown population parameter, thus\ninstead of performing a census, we can draw inferences about the population using sampling.\n\nSpecific to our sampling activity:\n\nIf we extract a sample of \\(n=50\\) beads at random, in other words, we mix all of the equally sized beads before using the shovel, then\nthe contents of the shovel are an unbiased representation of the contents of the urn’s 1000 beads, thus\nany result based on the shovel’s beads can generalize to the urn, thus\nthe sample proportion \\(\\hat{\\rho}\\) of the \\(n=50\\) beads in the shovel that are red is a “good guess” of the population proportion \\(\\rho\\) of the \\(N=1000\\) beads that are red, thus\ninstead of manually going over all 1,000 beads in the urn, we can make inferences about the urn by using the results from the shovel.\n\n\n\n3.3.2 Statistical definitions\nNow, for some important statistical definitions related to sampling. As a refresher of our 1,000 repeated/replicated virtual samples of size \\(n\\) = 25, \\(n\\) = 50, and \\(n\\) = 100 in Section 3.2, let’s display our figure showing the difference in proportions red according to different shovel sizes.\n\n\n\n\n\nPreviously seen three distributions of the sample proportion \\(\\hat{\nho}\\).\n\n\n\n\nThese types of distributions have a special name: sampling distributions; their visualization displays the effect of sampling variation on the distribution of a point estimate; in this case, the sample proportion \\(\\hat{\\rho}\\). Using these sampling distributions, for a given sample size \\(n\\), we can make statements about what values we typically expect.\nFor example, observe the centers of all three sampling distributions: they are all roughly centered around \\(0.4 = 40\\%\\). Furthermore, observe that while we are somewhat likely to observe sample proportions of red beads of \\(0.2 = 20\\%\\) when using the shovel with 25 slots, we will almost never observe a proportion of 20% when using the shovel with 100 slots. Observe also the effect of sample size on the sampling variation. As the sample size \\(n\\) increases from 25 to 50 to 100, the variation of the sampling distribution decreases and thus the values cluster more and more tightly around the same center of around 40%. We quantified this variation using the standard deviation of our sample proportions, seeing that the standard deviation decreases with the square root of the sample size:\n\n\n\n\n\nPreviously seen comparing standard deviations of proportions red for 100 different shovels\n\n\n\n\nSo as the sample size increases, the standard deviation of the proportion of red beads decreases. This type of standard deviation has another special name: standard error\n\n\n3.3.3 What is a “standard error”?\nThe “standard error” (SE) is a term that measures the accuracy with which a sample distribution represents a population through the use of standard deviation. Specifically, SE is used to refer to the standard deviation of a sample statistic (aka point estimate), such as the mean or median. For example, the “standard error of the mean” refers to the standard deviation of the distribution of sample means taken from a population.\n\nIn statistics, a sample mean deviates from the actual mean of a population; this deviation is the standard error of the mean.\n\nMany students struggle to differentiate the standard error from the standard deviation. The relationship between the standard error and the standard deviation is such that, for a given sample size, the standard error equals the standard deviation divided by the square root of the sample size. Accordingly, the standard error is inversely proportional to the sample size; the larger the sample size, the smaller the standard error because the statistic will approach the actual value.\n\nThe more data points involved in the calculations of the mean, the smaller the standard error tends to be. When the standard error is small, the data is said to be more representative of the true mean. In cases where the standard error is large, the data may have some notable irregularities. Thus, larger sample size = smaller standard error = more representative of the truth.\nTo help reinforce these concepts, let’s re-display our previous figure but using our new sampling terminology, notation, and definitions:\n\n\n\n\n\nThree sampling distributions of the sample proportion \\(\\hat{\nho}\\). Note the increased concentration on the bins around .4 as our sample size increases.\n\n\n\n\nFurthermore, let’s display the graph of standard errors for \\(n = 1\\) to \\(n = 100\\) using our new terminology, notation, and definitions relating to sampling.\n\n\n\n\n\nStandard errors of the sample proportion based on sample sizes of 1 to 100\n\n\n\n\nRemember the key message of this last table: that as the sample size \\(n\\) goes up, the “typical” error of your point estimate will go down, as quantified by the standard error.\n\n\n3.3.4 The moral of the story\nIf we could only know two pieces of information from our data, what would they be? First, you need a measure of the center of the distribution. This would include the mean or median, which shows the center of our data points. Second, we need a measure of the variability of the distribution. To understand our center, we must understand how different (or how spread) our data points are from one another. Thus, we need a measure like sd() or MAD. These are summary statistics which are necessary to understanding a distribution. Do those two figures encompass all you need to know about a distribution? No! But, if you are only allowed two numbers to keep, those are the most valuable.\n\nThe mean or median is a good estimate for the center of the posterior and the standard error or mad is a good estimate for the variability of the posterior, with +/- 2 standard errors covering 95% of the outcomes.\n\nThe standard error measures the accuracy of a sample distribution as compared to the population by using the standard deviation. Specifically, the standard deviation of our data points divided by the square root of the sample size. As such, we find that larger sample sizes = lower standard errors = more accurate and representative guesses.\nTo really drive home our point: standard error is just a fancy term for your uncertainty about something you don’t know. Standard error == our (uncertain) beliefs.\n\n\n\n\n\nIf you are wondering how much you need to know, follow this helpful guide of the information we have learned this chapter!\n\n\n\n\nThis hierarchy represents the knowledge we need to understand standard error (SE). At the bottom, we have math. It’s the foundation for our understanding, but it doesn’t need to be what we take away from this lesson. As we go up, we simplify the topic. The top of the pyramid are the basic levels of understanding that will help you to remember in the future.\nIf I know your estimate plus or minus two standard errors, I know your 95% confidence interval. This is valuable information. Standard error is really just a measure for how uncertain we are about something we do not know, the thing we are estimating. When we recall SE, we should remember that, all in all, it’s a complicated concept that can be distilled into: the way old people talk about confidence intervals.\n\nRecall that \\(\\hat{\\rho}\\) is the estimated value of p which comes from taking a sample. There can be billions and billions of \\(\\hat{\\rho}\\)’s. We look at a large group of \\(\\hat{\\rho}\\)’s, create a distribution of results to represent the possible values of p based on our findings, and then we compute a standard error to account for our own uncertainty about our predictions. Our 95% confidence interval for our prediction == our estimate plus or minus two standard errors.\nIn regards to the fifth layer of the hierarchy, we may wonder:\n\n“I thought that MADs were the same thing as standard deviations. Now you say they are the same things as standard errors. Which is it?”\n\nMADs and standard deviations are, more or less, the same thing. They are both measures of the variability of a distribution. In most cases, they have very similar values. A standard error is also a standard deviation. Specifically, it is the standard deviation of the distribution of the estimates, and that distribution of estimates is, more or less, your posterior. Therefore, we can use MAD, like standard error, to describe that distribution and the variability of that distribution.\nYou must understand what the standard error of \\(\\hat{\\rho}\\) means. You do not need to understand why.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "sampling.html#cardinal-virtues",
    "href": "sampling.html#cardinal-virtues",
    "title": "3  Sampling",
    "section": "3.4 Cardinal Virtues",
    "text": "3.4 Cardinal Virtues\nRecall that we began this chapter by asking about \\(\\rho\\), the proportion of red beads in the urn. Use the Cardinal Virtues to guide your thinking.\n\n3.4.1 Wisdom\n\n\n\n\n\n\n\n\n\nWisdom requires the creation of a Preceptor Table, an examination of our data, and a determination, using the concept of “validity,” as to whether or not we can (reasonably!) assume that the two come from the same population.\n\n3.4.1.1 Preceptor Table\nA Preceptor Table is a table with rows and columns, such that, if no data is missing, we can easily answer our questions.\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\nColor\n\n\n\n\n1\nwhite\n\n\n2\nwhite\n\n\n...\n...\n\n\n200\nred\n\n\n201\nwhite\n\n\n...\n...\n\n\n2078\nred\n\n\n2079\nwhite\n\n\n...\n...\n\n\n\n\n\n\n\n\n \nNote that the beads do not have ID numbers printed on them. The numbering is arbitrary. Having an ID just reminds us that there are actual units under consideration, even if we can not tell them apart, other than by color. We also include the ID to help visualize the fact that we don’t know the total number of beads in the urn, because our question never tells us! There could be 1,000 beads like our physical urn from earlier, or there could be a million beads. The ellipse at the bottom of the Preceptor Table denotes our uncertainty regarding urn size.\nThere is only one outcome column, “Color,” because this is not a causal model, for which we need to have (at least) two potential outcomes. Predictive models require only one outcome.\nIf we know the color of every bead, then calculating the proportion of beads which are red, \\(\\rho\\), is simple algebra.\n\n\n3.4.1.2 EDA\nThe data we have, unfortunately, only provides the color for 50 beads.\n\n\n\n\n\n\n\n\n\nData from Shovel\n\n\nID\nColor\n\n\n\n\n2\nwhite\n\n\n...\n...\n\n\n200\nred\n\n\n...\n...\n\n\n2079\nwhite\n\n\n3042\nwhite\n\n\n\n\n\n\n\n\n \nThe data table has exactly 50 rows. Again, there are, in truth, no ID numbers. But keeping track of which beads were in the sample and which beads were not can be helpful.\n\n\n3.4.1.3 Validity\nThe last step of Wisdom is to decide whether or not we can consider the units from the Preceptor Table and the units from the data to have been drawn from the same population. In this case, as with many sampling scenarios, it is trivial that we may make this assumption. If all the rows from the data are also rows in the Preceptor Table, we may assume that they are drawn from the same distribution.\nValidity involves the columns of our data set. Is the meaning of our columns consistent across the different data sources? In our urn scenario, does bead color in our sampled data and bead color in our Preceptor Table mean the same thing? The answer is yes, and validity can be assumed very easily.\n\n\n\n3.4.2 Justice\n\n\n\n\n\n\n\n\n\nJustice examines the assumptions of stability, representativeness, and unconfoundedness with regard to the Population Table.\n\n3.4.2.1 Population Table\nWe use The Population Table to acknowledge the wider source from which we could have collected our data.\nIt includes rows from three sources: the data for units we want to have (the Preceptor Table), the data for units which we have (our actual data), and the data for units we do not care about (the rest of the population, not included in the data or the Preceptor Table).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource\nLocation\nTime\nID\nColor\n\n\n\n\n…\nKnown, specific urn\nTime of sample - 2 years\n1\n?\n\n\n…\nKnown, specific urn\nTime of sample - 2 years\n200\n?\n\n\n…\nKnown, specific urn\nTime of sample - 2 years\n976\n?\n\n\n…\n…\n…\n…\n…\n\n\nData\nKnown, specific urn\nTime of sample\n2\nwhite\n\n\nData\nKnown, specific urn\nTime of sample\n200\nred\n\n\nData\nKnown, specific urn\nTime of sample\n1080\nwhite\n\n\n…\n…\n…\n…\n…\n\n\n…\nKnown, specific urn\nTime of sample + 3 weeks\n1\n?\n\n\n…\nKnown, specific urn\nTime of sample + 3 weeks\n200\n?\n\n\n…\nKnown, specific urn\nTime of sample + 3 weeks\n2025\n?\n\n\n…\n…\n…\n…\n…\n\n\nPreceptor Table\nKnown, specific urn\nNow\n1\n?\n\n\nPreceptor Table\nKnown, specific urn\nNow\n200\nred\n\n\nPreceptor Table\nKnown, specific urn\nNow\n2078\n?\n\n\n…\n…\n…\n…\n…\n\n\n…\nKnown, specific urn\nNow + 10 days\n1\n?\n\n\n…\nKnown, specific urn\nNow + 10 days\n200\n?\n\n\n…\nKnown, specific urn\nNow + 10 days\n2300\n?\n\n\n\n\n\n\n\n\n \nEach specific row represents one subject, which are individual beads in our urn scenario. Because there could be thousands or even millions of beads, we provide 3 examples for each category, and use ellipses to denote that there are many more subjects that we have yet to record.\nEach Population Table will usually have several types of columns: id, time, covariates, and outcome(s):\nWhen we construct a Preceptor Table to answer our question, we must select some covariates that we want all of our subjects to have. Our urn scenario has no covariates, however, so we will explore this issue in later chapters.\n\nBecause we draw our sample from the exact same urn our question asks us about, the data we collect comes directly from the Preceptor Table, all subjects in our population have the same location (“Known, specific urn”). The Preceptor Table and Population categories are essentially identical. This is the perfect scenario for us, but this rarely occurs in real life.\n\nPopulation Tables always have a column for Time. When answering a question we must specify the moment in time to which it applies, because stuff happens and things change.\nWe must acknowledge that the sample from the urn could have been taken at any time, so the contents of the urn in the past (our data) could be different from the contents of the urn when we want to answer our question now (the Preceptor Table). As such, there is a wider population we could have collected our data from: any time before collecting the sample, or anytime after collecting it.\nFinally, Population Tables have an outcome. Sometimes there will be multiple outcome columns, as in the case of casual models in which we need the values for two or more potential outcomes so that we can calculate a causal effect.\n\n\n3.4.2.2 Assumptions\n\nNow that we have created our Population Table, we can analyze the key assumptions of stability, representativeness and unconfoundedness.\nStability involves time. Is the model — meaning both the mathematical formula and the value of the parameters — stable over time? Realistically, an urn will be the same today, tomorrow or next year. However, what if someone dumps some red beads into the urn after we take our sample? Then we cannot assume stability, because the proportion of red beads in the urn, \\(\\rho\\), the instant before the dump is different than the proportion red in the urn after. We will assume no one is tampering with our urn, and assume stability across time periods.\nRepresentativeness involves the data rows, specifically the rows for which we have data versus the rows for which we might have had data. Are the rows that we do have data for representative of the rows for which we do not have data? For the sample proportion to be similar to the actual population proportion, we ideally want the data we have to be a random, unbiased selection from our population. In the context of our problem, the sampling mechanism of using a shovel of size 50 to sample beads from an urn in which the beads are thoroughly mixed should be enough to consider our sample representative of the population.\nThe sampling mechanism is the technical term for the process by which some beads were sampled and some were not. We hope that all members of the population have the same chance of being sampled, or else our data might be unrepresentative of the larger population. Another term for this would be having a “biased” sample. Almost all samples have some bias, but we must make a judgement call to see if the data we have is close enough to the data we want (i.e., the Preceptor Table) that we can consider both as coming from the same population. Our sample of 50 beads is taken from a mixed urn, so hopefully there is a near equal chance of selecting each bead, and our samples are representative of the population.\nUnconfoundedness involves the potential correlation between treatment assignment and the outcome. It is only a concern for causal models. Since this is a predictive model, we do not have to worry about unconfoundedness. There is no “treatment” which might be confounded with anything.\n\n\n\n3.4.3 Courage\n\n\n\n\n\n\n\n\n\nJustice verifies the Population Table. Courage creates a mathematical model which connects the outcome variable to the covariates, if any.\nThe data generating mechanism, or DGM, is a mathematical formula which mimics the process by which the data comes to us. The DGM for sampling scenarios with only two possible values is called Bernoulli and is often denoted as:\n\\[ red_i  \\sim Bernoulli(\\rho) \\] Each bead \\(i\\) which we sample can be either red or white. It is convenient to define the model in terms of whether or not the bead was red. If bead \\(i\\) is red, the value drawn is 1, which is the standard way of representing TRUE. In other words, \\(red_i = 1\\) means that bead \\(i\\) was red. Similarly, a white bead is indicated with 0, meaning that it is FALSE that the bead was red. \\(\\rho\\) is the only parameter in a Bernoulli model. It is the probability that a 1, instead of a 0, is drawn. That is, \\(\\rho\\) is the probability of a red bead.\nWe are Bayesian statisticians who make Bayesian models. This means that we make specific assumptions and consider data to be fixed and parameters to be variable. One of the most important distinctions is that in Bayesian data science, we don’t know the values of our parameters.\nSome non-Bayesian frameworks are concerned with the probability distribution of our observed data, but do not care much about the probability distribution for \\(\\rho\\) and assume it to be fixed. If \\(\\rho\\) is fixed, the equation above becomes one simple binomial distribution. Think of this as a standard 2 dimensional plot.\nWe Bayesians consider our observed data to be fixed. We don’t consider alternate realities where our observed data is different due to sampling variation. Instead, we are concerned with the probability distribution of our parameter. In our urn scenario, \\(\\rho\\) is variable, so we have to create a separate binomial distribution for each possible value of \\(\\rho\\). Think of this as a 3 dimensional joint distribution, as we created in Section 2.6.\nIt is essential to understand the joint distribution and the posterior, two concepts Bayesians use to solve problems. We will provide quick a quick review here, including statistical notation that may be helpful to some.\nThe joint distribution, \\(p(y | \\theta)\\), models the outcome \\(y\\) given one or more unknown parameter(s), \\(\\theta\\). The equation illustrates exact same concept we addressed while discussing the distinctions of Bayesian science: because our parameters are variable, we have to create separate distributions for each potential value. Combining all these distributions together creates a joint distribution that is 3 dimensional when plotted.\nThe posterior, \\(p(\\theta | y)\\), is the probability distribution of our parameter(s) \\(\\theta\\), created using data \\(y\\) that updates our beliefs. We have referenced the posterior many times before, and this definition does not change its meaning.\nIn our urn scenario, obtaining the posterior involves first creating many binomial distributions for each possible population proportion. This is the joint distribution, and it is a 3 dimensional model. We then select the distribution that corresponds with our data: 17 red beads are sampled. We can represent the posterior with the following:\n\\[\\text{Prob}(\\text{models} | \\text{data} = 17)\\]\nThis is equivalent to taking a 2 dimensional slice of the 3 dimensional model. We are left with a probability distribution for our parameter, \\(\\rho\\).\nIn this chapter, we will neither calculate that posterior “by hand,” as we did in Chapter 2. Nor will we use an R package to do so, as in Chapter 4. Instead, we will create a rough posterior using simple calculations with our data.\n\n\n3.4.4 Temperance\n\n\n\n\n\n\n\n\n\nCourage allowed us to assume a simple Bernoulli model and to calculate an estimate and its associated standard error.\nThe two most important parts of any distribution are, first, its center and, second, its spread. We need reasonable estimates for the center and the spread of the posterior for \\(\\rho\\).\nFirst, the percentage of red beads in the sample, $17/50 = 0.34%, is a good estimate for the center of the posterior. Throughout this section, we are leaving formalism behind. We have not proved that the sample provides a good estimate of the mean of the posterior but common sense suggests that it does.\nSecond, calculate the standard error:\n\\[  SE = \\frac{\\sigma\\text{ of data}}{\\sqrt{\\text{sample size}}} = \\frac{.4785}{\\sqrt{50}} \\approx .067\\]\nThe \\(\\sigma\\) of the data might seem to be a strange concept. What data do we have, besides 17 red beads and 33 white beads? The answer is that this is the data, and t is most sensibily represented as a vector of length 50, with 17 1’s and 33 0’s. The standard deviation of sd(c(rep(0, 17), rep(1, 33))) is 0.4785181.\nYou can then use the standard error to create a 95% confidence interval:\n\\[  CI = \\bar{x} \\hspace{.1cm} \\pm 2SE = .34 \\hspace{.1cm} \\pm .134\\]\nWith 95% confidence, the proportion of red beads in the urn is between 21% and 47%.\nKnowing that the standard error is the standard deviation of the posterior (and that the posterior is approximately normal) allows us to create a graphic.\n\n\nShow the code\ntibble(obs = rnorm(1000000, mean = 0.34, sd = 0.067)) |&gt; \n  ggplot(aes(x = obs)) +\n    geom_density(aes(y = after_stat(count/sum(count))))  +\n    labs(title = expression(paste(\"Posterior Distribution for  \", rho)),\n         subtitle = \"There is a 95% chance for a value between 21% and 47%.\",\n         x = expression(paste(\"Proportion, \", rho, \", of Red Beads in Urn\")),\n         y = \"Probability\") +\n    scale_x_continuous(labels = scales::percent_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\n\n\n\n\n\n\n\n\n\nThis is not precisely accurate. The full machinery of a proper Bayesian analysis would generate a (very) slightly different answer. To get that correct answer, we could either do everything by hand, as in Chapter 2, or use an R package, as in Chapter 4. However, the purpose of this chapter is to develop your intuition. Generating a 95% confidence interval using two times the standard error along with a normality assumption is good enough.\nMore important than the subtle issue\n\n\n3.4.4.1 Hypothesis tests\nStatisticians also use hypothesis tests to quickly try to answer questions. Our view on hypothesis tests is that:\nAmateurs test. Professionals summarize.\nTraditionally, most scientific papers are not so much interested in estimating \\(\\rho\\). They are interested in testing specific hypotheses. What do we mean by that?\nLet’s look at a possible hypothesis in our urn paradigm: there are equal number of red and white beads in the urn. The null hypothesis, denoted by \\(H_0\\), is the theory we are testing, while the alternative hypothesis, denoted by \\(H_a\\), represents the opposite of our theory. Therefore, our hypothesis is designed as such:\n\\(H_0\\): There are an equal number of red and white beads in the urn.\n\\(H_a\\): There are not an equal number of red and white beads in the urn.\nCan we reject that hypothesis? Convention: if the 95% confidence interval excludes the null hypothesis, then we reject it. Here, that would mean if our estimate (plus or minus 2 standard errors) excluded the possibility of the red and white beads being equal (\\(\\rho = .5\\)) we can reject the null hypothesis. In the previous section we determined that the 95% confidence interval is between 21% and 47%. Because 50% is outside of this interval, we could reject the null hypothesis, and conclude that it is unlikely that the proportion of beads in the urn is 50%.\nIf we were testing the theory that \\(\\rho = .45\\) instead, our null hypothesis would fall within the confidence interval. This does not mean that we accept the null hypothesis. Instead, we simply don’t reject it. In our scenario we only know that there is some possibility that \\(\\rho = .45\\). We’re just back where we started! This is why we never test — unless your boss demands a test. Use your judgment, make your models, summarize your knowledge of the world, and use that summary to make decisions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "sampling.html#summary",
    "href": "sampling.html#summary",
    "title": "3  Sampling",
    "section": "3.5 Summary",
    "text": "3.5 Summary\nIn this chapter, we performed both tactile and virtual sampling exercises to make inferences about an unknown parameter \\(\\rho\\): the proportion of red beads in the urn. We used the sample proportion \\(\\hat{\\rho}\\) to estimate the true proportion \\(\\rho\\).\nThere is a truth! There is a true value for \\(\\rho\\) which we do not know. We want to create a posterior probability distribution which summarizes our knowledge. We care about the posterior probability distribution of \\(\\rho\\). The center of that distribution is around the mean or median of the proportion in your sample. The standard deviation of that posterior is the standard deviation of the sample divided by the square root of our sample size. Note that this is the same thing as the standard deviation of the repeated samples.\nWe journey from reality, to our predictions, to the standard error of our predictions, to the posterior probability distribution for \\(\\rho\\). This is our sequence:\n\\(\\rho\\) (i.e., the truth) \\(\\Rightarrow\\) \\(\\hat{\\rho}\\) (i.e., my estimate) \\(\\Rightarrow\\) the standard error of \\(\\hat{\\rho}\\) (i.e., black box of math mumbo jumbo and computer simulation magic) \\(\\Rightarrow\\) our posterior probability distribution for \\(\\rho\\) (i.e., our beliefs about the truth).\nThis journey shows how our beliefs about the truth develop through our work. We begin with \\(\\rho\\); \\(\\rho\\) is the truth, the true but unknown value we are estimating. \\(\\hat{\\rho}\\) is our estimate for \\(\\rho\\). There can be millions and millions of \\(\\hat{\\rho}\\)’s. Next, we must estimate the standard error of our estimates (our \\(\\hat{\\rho}\\)’s) to account for the uncertainty of our predictions. Finally, we create a posterior probability distribution for \\(\\rho\\). This distribution is used to answer any questions about \\(\\rho\\).\nOther highlights:\n\nStandard error is just a fancy term for your uncertainty about something you don’t know. Standard error \\(\\approx\\) our (uncertain) beliefs.\nLarger sample sizes \\(\\implies\\) lower standard errors \\(\\implies\\) more accurate estimates.\n\n\n\nIf we could only know two pieces of information from our data, we would need a measure of the center of the distribution (like mean or median) and a measure of the variability of the distribution (like standard deviation or median absolute deviation).\nThe standard error refers to the standard deviation of a sample statistic (also known as a “point estimate”), such as the mean or median. Therefore, the “standard error of the mean” refers to the standard deviation of the distribution of sample means taken from a population.\n\nAs we continue our journey, recall the case of Primrose Everdeen and what she represents: no matter how realistic our model is, our predictions are never certain.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "4  Models",
    "section": "",
    "text": "4.1 Wisdom\nWisdom requires the creation of a Preceptor Table, an examination of our data, and a determination, using the concept of “validity,” as to whether or not we can (reasonably!) assume that the two come from the same population. We begin with a question:\nFor the purpose of this question, assume that it is April, 7 months before the election.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "models.html#wisdom",
    "href": "models.html#wisdom",
    "title": "4  Models",
    "section": "",
    "text": "What proportion of all votes will be cast for Joe Biden in the 2024 election?\n\n\n\n4.1.1 Preceptor Table\nA Preceptor Table is a table with rows and columns, such that, if no data is missing, we can easily answer our questions.\nUnits: The units are individual voters. Note that this is not all American adults or even all registered voters. To answer our question, we only need information about actual voters. Any other rows would be superfluous.\nOutcome: The outcome is the candidate for whom the vote was cast. But, looking at the question, this formulation is too broad. We don’t actually need to know the name of the candidate. We just need to know if the vote was cast for Biden or not. The name of the candidate, if it was not Biden, is irrelevant to our question.\nTreatment: There is no treatment.\nCausal or predictive model: This is clearly a predictive model since there is only one outcome for each person: their vote. It would be a causal model if we were considering a treatment which might change the vote of person \\(i\\) from one candidate to another. A causal model would require two\nCovariates: There are no covariates. If the question referred to who will win the election, then we would at least have to add a column for state of residence in order to tabulate electoral votes.\nMoment in Time: The moment in time is just after Election Day 2024.\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nVoter ID\nVoted for Biden\n\n\n\n\n1\n0\n\n\n2\n0\n\n\n...\n...\n\n\n200\n1\n\n\n201\n0\n\n\n...\n...\n\n\n2078\n1\n\n\n2079\n0\n\n\n...\n...\n\n\n\n\n\n\n\n \nOnce we have all the information in the Preceptor Table, it is easy to calculate the proportion of all votes which were cast for Biden.\nMoreover, we could use the Preceptor Table to answer other related questions. For example, assume we wanted to know the odds that the next two people we meet, just after the election, both voted for Biden.\nWith the data in the Preceptor Table, we could create a computer simulation which, for example, draws 1,000 pairs of voters at random from the table, checks to see if they both voted for Biden, and then report the percentage of the 1,000 which meet this criteria. Of course, we will need to construct such a simulation carefully. People don’t walk around at random! They walk with friends, and friends share the same politics more often than strangers do.\nThe important point is that the Preceptor Table has the fewest rows and columns such that, if no data is missing, we can answer our question.\n\n4.1.2 EDA\nThe data we have come from a YouGov poll (pdf) of 1,559 US adult citizens, conducted March 10 - 12, 2024. There are many questions in the poll. For the purpose of answering our question, the most relevant one is:\n\nIf an election for president were going to be held now and the Democratic nominee was oe Biden and the Republican nominee was Donald Trump, would you vote for…\n\nThe allowed choice are “Joe Biden,” “Donald Trump,” “Other,” “Not Sure,” and “I would not vote.” 42% of those polled indicated Joe Biden. Although rounding makes it impossible to know for sure, we will assume that 655 of the 1,559 “US adult citizens” would vote for Biden. Our data looks like:\n\n\n\n\n\n\n\n\nPolling Data\n\n\nPoll ID\nWould Vote for Biden\n\n\n\n\n1\n0\n\n\n2\n0\n\n\n...\n...\n\n\n200\n1\n\n\n201\n0\n\n\n...\n...\n\n\n1,559\n1\n\n\n\n\n\n\n\nThere are key differences between our data table and our Preceptor Table, despite their superficial similarities.\n\nOur Polling Data includes exactly 1,559 rows. We don’t know how many rows are in our Preceptor Table because we don’t know, today, how many people will vote in November 2024.\nThe ID column is labeled “Voter ID” in the Preceptor Table and “Poll ID” in the Polling Data. In many ways, the ID doesn’t matter. The person labeled “2” in the first table, for example, has no necessary connection to the person labeled “2” in the second table. The numbers don’t mean anything. But, conceptually, having different labels helps to highlight the issue of sampling and concerns about representativeness, concerns which we will address under Justice.\nThe variable labels differ. “Voted for Biden” is not the same thing as “Would Vote for Biden.” Indeed, the two columns represent very different things.\n\n4.1.3 Validity\nValidity is the consistency, or lack thereof, in the columns of the data set and the corresponding columns in the Preceptor Table. In order to consider the two data sets to be drawn from the same population, the columns from one must have a valid correspondence with the columns in the other. Validity, if true (or at least reasonable), allows us to construct the Population Table, which is the first step in Justice.\nOur key problem is that there is not a one-to-one correspondence between our data (which is about an reported intention to vote for Biden) and our Preceptor Table (which records an actual vote). What we tell other people we will do is often not what we actually do. Consider a voter who is embarrassed about her support for Biden. Perhaps she perceives the interviewer as a Republican and doesn’t care to admit to him, in March, her support for Biden. But, in November, in the privacy of the polling booth, she votes for Biden. In her case, the outcome from the data (what she told the pollster) did not have a valid correspondence with her behavior in the polling booth.\nWhen we “stack” our data on top of our Preceptor Table, we are assuming that intention is a valid measure of actual voting. There may be problem, as with our hypothetical example, but they are not numerous enough to matter.\nWe conclude the Wisdom section by summarizing how we hope to use the data we have to answer the question we started with:\n\nUsing data from a YouGov poll of 1,559 US adult citizens, conducted March 10 - 12, 2024, we seek to understand what proportion of voters will support Biden in the 2024 election.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "models.html#justice",
    "href": "models.html#justice",
    "title": "4  Models",
    "section": "\n4.2 Justice",
    "text": "4.2 Justice\n\n\n\n\n\n\n\n\nJustice is the second Cardinal Virtue in data science. Justice starts with the Population Table – the data we want to have, the data which we actually have, and all the other data from that same population. Each row of the Population Table is defined by a unique unit/time combination. We explore three key issues. First, does the relationship among the variables demonstrate stability, meaning is the model stable across different time periods? Second, are the rows associated with the data and, separately, the rows associated with the Preceptor Table representative of all the units from the population? Third, for causal models only, we consider unconfoundedness.\n\n4.2.1 Population Table\nWe use the Population Table to acknowledge the wider source from which we could have collected our data. It includes rows from three sources: the data for units we want to have (the Preceptor Table), the data for units which we have (our actual data), and the data for units we do not care about (the rest of the population, not included in the data or in the Preceptor Table). Consider:\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource\nTime\nID\nBiden\n\n\n\n…\nFebruary 2024\n1\n?\n\n\n…\nFebruary 2024\n200\n?\n\n\n…\nFebruary 2024\n976\n?\n\n\n…\n…\n…\n…\n\n\nData\nMarch 2024\n1\n0\n\n\nData\nMarch 2024\n200\n1\n\n\nData\nMarch 2024\n…\n…\n\n\nData\nMarch 2024\n1559\n1\n\n\n…\n…\n…\n…\n\n\n…\nOctober 2024\n1\n?\n\n\n…\nOctober 2024\n200\n?\n\n\n…\nOctober 2024\n2025\n?\n\n\n…\n…\n…\n…\n\n\nPreceptor Table\nNovember 2024\n1\n1\n\n\nPreceptor Table\nNovember 2024\n200\n0\n\n\nPreceptor Table\nNovember 2024\n2078\n1\n\n\n…\n…\n…\n…\n\n\n…\nDecember 2024\n1\n?\n\n\n…\nDecember 2024\n200\n?\n\n\n…\nDecember 2024\n2300\n?\n\n\n\n\n\n\n\n \nEvery row in a Population Table corresponds to a unit/time combination. This is different from the Preceptor Table and the data, in both of which each row is for a unit. The entire Preceptor Table and data each have a specific time associated with them, even if that moment corresponds to a few days or longer. Nothing in this imperfect world is ever instantaneous.\nA Population Table will usually have several types of columns: id, time, covariates, and outcome(s). There are no covariates in this simple example, but we will use them in later chapters.\nNow that we have created our Population Table, we can analyze the key assumptions of stability, representativeness and unconfoundedness.\n\n4.2.2 Stability\nStability means that the relationship between the columns in the Population Table is the same for three categories of rows: the data, the Preceptor Table, and the larger population from which both are drawn.\nOur Population Table is so simple that stability is true almost automatically true. There is only one column! Stability might become important as we think about the actual process by which we might meet the two voters in our original question, but, in terms of the Population Table itself, there is no problem.\n\n4.2.3 Representativeness\nRepresentativeness, or the lack thereof, concerns two relationship, among the rows in the Population Table. The first is between the Preceptor Table and the other rows. The second is between our data and the other rows. Ideally, we would like both the Preceptor Table and our data to be random samples from the population. Sadly, this is almost never the case.\nThe sampling mechanism is the technical term for the process by which some people were sampled and some were not. We hope that all members of the population have the same chance of being sampled, or else our data might be unrepresentative of the larger population. Another term for this would be having a “biased” sample. Almost all samples have some bias, but we must make a judgement call to see if the data we have is close enough to the data we want (i.e., the Preceptor Table) that we can consider both as coming from the same population.\nOne concern might involve the process by which YouGov sampled potential voters in March. If that process were flawed, if the people it sampled were systematically different than the people it did not, then the assumption of representativeness would fail. Imagine that YouGov only polled residents in Washington DC. That would be bad, even if everyone polled answered truthfully. We would end up with data which was much more pro-Biden than the country as a whole.\nRepresentativeness might also be a problem with regard to the Preceptor Table, even if our data was perfectly representativeness of the US population. Recall that the set of voters is much smaller than the full set of all US adults. In fact, voters as a class are systematically different from all adults. They are richer, better educated and more knowledgeable about politics. To the extent that the Preceptor Table — which is only voters — is not representative of all adults, we might have a problem.\nThe assumption of representativeness is never completely true. We can only hope/assume that it is true enough that the inferences we draw from our data can be roughly applied to our question. Let’s assume that is this case.\n\n4.2.4 Unconfoundedness\nUnconfoundedness means that the treatment assignment is independent of the potential outcomes, when we condition on pre-treatment covariates. This assumption is only relevant for causal models. We describe a model as “confounded” if this is not true. The easiest way to ensure unconfoundedness is to assign treatment randomly.\nSince this is a predictive model, we do not have to worry about unconfoundedness. There is no “treatment” which might be confounded with anything.\n\nUsing data from a YouGov poll of 1,559 US adult citizens, conducted March 10 - 12, 2024, we seek to understand what proportion of voters will support Biden in the 2024 election. Biden’s popularity might change significantly over the course of the election campaign.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "models.html#courage",
    "href": "models.html#courage",
    "title": "4  Models",
    "section": "\n4.3 Courage",
    "text": "4.3 Courage\n\n\n\n\n\n\n\n\nJustice verifies the Population Table. Courage creates a mathematical model which connects the outcome variable to the covariates, if any. Then, using code, we create a fitted model, including posterior probability distributions for all the unknown parameters.\nGiven that the polling problem is conceptually similar to the beads-in-an-urn problem, it is useful to perform the same back-of-the-envelope calculation we did in Chapter 3. To do that, we first need to create the tibble:\n\npoll_data &lt;- tibble(biden = c(rep(1, 655), \n                              rep(0, 904)))\n\nslice_sample(poll_data, n = 10)\n\n# A tibble: 10 × 1\n   biden\n   &lt;dbl&gt;\n 1     1\n 2     1\n 3     0\n 4     0\n 5     0\n 6     1\n 7     1\n 8     0\n 9     1\n10     0\n\n\nFirst, calculate the mean. This is our estimate of the center of the posterior distribution for \\(\\rho\\), the unknown proportion of the voters who support Biden.\n\nmean(poll_data$biden)\n\n[1] 0.4201411\n\n\n42% of the sample support Biden. Second, calculate the standard error of this estimate, by taking the standard deviation of the data and dividing it by the square root of the number of observations.\n\nsd(poll_data$biden) / sqrt(length(poll_data$biden))\n\n[1] 0.01250475\n\n\nThe standard error is about 1.25%. This suggests that a 95% confidence interval for the true value of \\(\\rho\\) would be:\n\nc(mean(poll_data$biden) - 2 * sd(poll_data$biden) / sqrt(length(poll_data$biden)),\n  mean(poll_data$biden) + 2 * sd(poll_data$biden) / sqrt(length(poll_data$biden)))\n\n[1] 0.3951316 0.4451506\n\n\nThe margin of error, which is another term for two times the standard error, is about 2.5%. This is less than the value of 3.5% given in the pdf. The reason for the difference involves all the technical adjustments which YouGov needs to make to their estimate. We ignore those problems and so our value is probably too small.\n\nThe data generating mechanism, or DGM, is a mathematical formula which mimics the process by which the data comes to us. The DGM for sampling scenarios with only two possible values is called Bernoulli, usually denoted as:\n\n\\[ red_i  \\sim Bernoulli(\\rho) \\] Each bead \\(i\\) which we sample can be either red or white. It is convenient to define the model in terms of whether or not the bead was red. If bead \\(i\\) is red, the value drawn is 1, which is the standard way of representing TRUE. In other words, \\(red_i = 1\\) means that bead \\(i\\) was red. Similarly, a white bead is indicated with 0, meaning that it is FALSE that the bead was red. \\(\\rho\\) is the only parameter in a Bernoulli model. It is the probability that a 1, instead of a 0, is drawn. That is, \\(\\rho\\) is the probability of a red bead, which is the same thing as the proportion of red beads in the urn, assuming again that the sampling mechanism is not biased.\nWe typical estimate the unknown parameter \\(\\rho\\) from the Bernoulli model by using a logit regression:\n\\[\n\\rho = \\frac{e^{\\beta_0}}{1 + e^{\\beta_0}}\n\\]\nThere are generally two parts for a statistical model: family and link function. The family is the probability distribution which generates the randomness in our data. The link function is the mathematical formula which links our data to the unknown parameters in the probability distribution. We will be reviewing these concepts over and over again in the rest of the Primer, so don’t worry if things are a little unclear right now.\n\n\n4.3.1 Models\nLoad the brms package:\n\nlibrary(brms)\n\nThe brms package provides a user-friendly interface to work with the statistical language Stan, the leading tool for Bayesian model building.\n\n4.3.1.1 Gaussian\nThe key function in the brms package is brm().\n\nfit_gauss &lt;- brm(formula = biden ~ 1,\n                data = poll_data)\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 7.6e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.76 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.1 seconds (Warm-up)\nChain 1:                0.096 seconds (Sampling)\nChain 1:                0.196 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2.7e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.103 seconds (Warm-up)\nChain 2:                0.12 seconds (Sampling)\nChain 2:                0.223 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 3.3e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.33 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.103 seconds (Warm-up)\nChain 3:                0.097 seconds (Sampling)\nChain 3:                0.2 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 3.4e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.34 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.103 seconds (Warm-up)\nChain 4:                0.12 seconds (Sampling)\nChain 4:                0.223 seconds (Total)\nChain 4: \n\n\nNotes:\n\nbrm() produces a great deal of output, as you can see above. This is useful if there is some sort of problem. But, most of the time, we don’t care. So we suppress the output in the rest of the Primer, using refresh = 0 and silent = 2.\nWe almost always assign the result of a brm() call to an object, as here. By convention, the name of that object begins with “fit” because it is a fitted model object. This model was created quickly but larger models take longer. So, we assign the result to an object so we don’t need to recreate it.\nThe first argument to brm() is formula. This is provided using the R formula syntax in which the dependent variable, red is separated from the independent variables by a tilde: ~. This is the standard approach in almost all R model estimating functions. In this case, the only independent variable is a constant, which is represented with a 1.\nThe second argument to brm() is data, which is a tibble containing the data used to estimate the model parameters. The variables specified in the formula must match the variables in the tibble which is passed in, which is .\nBecause the fitting process is random, if we run this code again we will get a (very slightly) different answer. In most applications, the differences are too small to matter. But, they are annoying! So, we use the seed argument so that your result will match ours.\n\n\nfit_gauss &lt;- brm(formula = biden ~ 1,\n                data = poll_data,\n                refresh = 0,\n                silent = 2,\n                seed = 9)\n\n\n\nPrint out the model object.\n\nfit_gauss\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: biden ~ 1 \n   Data: poll_data (Number of observations: 1559) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.42      0.01     0.40     0.44 1.00     3949     2944\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.49      0.01     0.48     0.51 1.00     3816     2236\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThere are many details. We will take a first pass here, leaving out elements which we will return to in later chapters. Highlights:\n\nThe “Family” is “gaussian,” which is the default value for the family argument in brms(). In the next example, we will provide the family argument explicitly. Gaussian is another term for the normal distribution. Using “gaussian” is almost certainly a mistake on our part, but “discovering” this mistake and then fixing it is a useful exercise.\nThe “Links” are “identity” for both “mu” and “sigma.” Recall the the gaussian/normal distribution is specified as \\(N(\\mu, \\sigma^2)\\) in which \\(\\mu\\), or “mu,” is the parameter for the mean or center of the distribution and \\(\\sigma\\), or “sigma,” is the parameter for the standard deviation or spread of the distribution.\n\n\nThe “Formula” is biden ~ 1, which is what we passed in to the formula argument when we called brm().\nThe “Data” is poll_data, which is what we passed in to the data argument when we called brm(). Note the reminder that there are 1,559 observations. Never hurts to check that the reported number of observations matches what you expect.\nFitting Bayesian models is computationally complex. The information about “Draws,” and the values for “Rhat,” “Bulk_ESS,” and “Tail_ESS” refer to those complexities. The bottom message gives a basic explanation. Don’t worry about any of that now.\nThe “Intercept” is the key part of the model. Because the family is (incorrectly!) Gaussian and the link function an identity, the actual model we are estimating looks like:\n\n\\[ biden_i =  \\mu + \\epsilon_i \\]\nwith \\(\\epsilon_i \\sim N(0, \\sigma^2)\\). \\(y_i\\) is the height of male \\(i\\). \\(\\mu\\) is true proportion of Biden voters. \\(\\epsilon_i\\) is the “error term,” the difference between the vote of person \\(i\\) and the true proportion of Biden voters. The obvious problem with this is that \\(\\mu\\), also called the “Intercept,” is about 0.42. The value of biden is either 0 or 1. So, the value of \\(\\epsilon\\) is either -0.42 or + 0.58. No other value is possible. But that makes no sense if \\(\\epsilon_i \\sim N(0, \\sigma^2)\\)! This will cause problems soon.\n\nIgnoring that Gaussian/Bernoulli issue for now, we have an “Intercept” (or \\(\\mu\\)) value of 0.42. Note the complexity involved in even this simple model. We have multiple ways of describing the same concept: the true proportion of Biden voters. First, we have \\(\\rho\\), a mathematical symbol associated with Bernoulli models and to which we will return. Second, we have 1, meaning the symbol in the formula: biden ~ 1. This is an “intercept only” model, one with no covariates. Third, we have \\(\\mu\\), the standard symbol for the intercept in a intercept-only linear or Gaussian model. Fourth, we have the word “Intercept,” as in the printout from the fitted model object. In other words, \\(\\rho\\), \\(1\\), \\(\\mu\\), and Intercept all refer to the same thing!\nThe “Est.Error” is, more or less, the same thing as the standard error of our 0.42 estimate for the intercept. The full value is 0.0123, which is very close to the value we calculated by hand above.\nThe lower and upper bounds of the 95% confidence interval are indicated by “l-95% CI” and “u-95% CI.” The number also match, subject to rounding, our calculations. (We can’t use the by-hand calculations with more complex models, otherwise we would have no need for brms!)\nThe “sigma” variable in the “Family Specific Parameters” section refers to out estimate of \\(\\sigma\\), given the assumed truth of a gaussian error term. Since that assumption is clearly false, we won’t waste time interpreting the results here.\n\nOnce we have a fitted model, we can use that model, along with the R packages tidybayes and bayesplots, to answer various questions.\n\nlibrary(tidybayes)\nlibrary(bayesplot)\n\n\nWe can generate a posterior distribution for \\(\\rho\\) with:\n\nfit_gauss |&gt; \n  add_epred_draws(newdata = tibble(.rows = 1))\n\n# A tibble: 4,000 × 5\n# Groups:   .row [1]\n    .row .chain .iteration .draw .epred\n   &lt;int&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;\n 1     1     NA         NA     1  0.439\n 2     1     NA         NA     2  0.398\n 3     1     NA         NA     3  0.404\n 4     1     NA         NA     4  0.417\n 5     1     NA         NA     5  0.427\n 6     1     NA         NA     6  0.431\n 7     1     NA         NA     7  0.426\n 8     1     NA         NA     8  0.408\n 9     1     NA         NA     9  0.402\n10     1     NA         NA    10  0.438\n# ℹ 3,990 more rows\n\n\nThe first argument to the key function, add_epred_draws(), is newdata. We often want to examine the behavior of the model with new data sets, data which was not used in model estimation. The object passed as newdata is generally a tibble, with a row corresponding to each set of values for the independet variables. In this case, we have an intercept-only model, which means that there are no independent variables, other than the intercept, which does not need to be explicitly passed in. So, newdata is a tibble with no variables and one row.\n\ntibble(.rows = 1)\n\n# A tibble: 1 × 0\n\n\nThe resulting tibble includes 4,000 rows, each of which is a draw from the posterior distribution of \\(\\rho\\). (Recall that \\(\\rho\\) and \\(\\mu\\) refer to the same thing.) You can ignore that columns named .row, .chain, and .iteration. The .draw variable just keeps count of the draws. The key variable is .epred, which us a draw from the posterior distribution of \\(\\rho\\). The “e” in .epred stands for expected value. Grahically, we have:\n\nfit_gauss |&gt; \n  add_epred_draws(newdata = tibble(.rows = 1)) |&gt; \n  ggplot(aes(x = .epred)) + \n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 50) + \n    labs(title = expression(paste(\"Posterior Probability Distribution of \", rho)),\n         subtitle = \"Distribution is centered at 42%\",\n         x = expression(paste(\"Proportion, \", rho, \", of Biden Voters\")),\n         y = \"Probability\") + \n  \n    scale_x_continuous(labels = scales::percent_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\n\n\n\n\n\n\n\nEven more useful than posteriors of the parameters in the model are predictions we can make with the model. The key function for exploring those predictions is add_predicted_draws(). This function behaves similarly to add_epred_draws(). In both cases, we need to provide the value for the newdata argument, generally a tibble with a column for all the variables used in the fitted model. Again, because this is an intercept-only model, there is no need for columns in the newdata tibble. No variables are used in fit_gauss.\n\nfit_gauss |&gt; \n  add_predicted_draws(newdata = tibble(.rows = 1))\n\n# A tibble: 4,000 × 5\n# Groups:   .row [1]\n    .row .chain .iteration .draw .prediction\n   &lt;int&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt;       &lt;dbl&gt;\n 1     1     NA         NA     1     -0.185 \n 2     1     NA         NA     2      0.452 \n 3     1     NA         NA     3      0.177 \n 4     1     NA         NA     4      1.06  \n 5     1     NA         NA     5      0.495 \n 6     1     NA         NA     6      0.912 \n 7     1     NA         NA     7      1.63  \n 8     1     NA         NA     8      0.379 \n 9     1     NA         NA     9      0.337 \n10     1     NA         NA    10     -0.0841\n# ℹ 3,990 more rows\n\n\nThe columns which result are the same as with add_epred_draws() except that .epred is replaced with .prediction. (The reason that all the variable names have a leading . is that this convention makes it less likely, but not impossible, that they will conflict with any variable names in the newdata tibble.)\n\n4.3.2 Tests\nA glance at the result shows the problem: .prediction takes on a variety of values. But that is impossible! By definition, the only possible values are 0/1. We must have made a mistake in specifying the model. Another way to see the model is to perform a “posterior predictive check,” a comparison of model predictions with your actual data. The bayesplot package includes the pp_check() function for that purpose.\n\npp_check(fit_gauss)\n\nUsing 10 posterior draws for ppc type 'dens_overlay' by default.\n\n\n\n\n\n\n\n\n\\(y\\) is the original data. \\(y_rep\\) is a replication of the data, using the fit_gauss model. In other words, we “pretend” that fit_gauss is the true model. We then create 10 versions of the output data, as created by add_predictive_draws(), each the same size are our original data, which is 1,559 observations in this case. We then graph both \\(y\\) and the 10 versions of \\(y_rep\\).\nAs the graphic makes clear, we have a serious problem. The data, meaning the variable biden, which is referred to as \\(y\\) in the plot, is always either 0 or 1, hence the two spikes in the plot. \\(y_rep\\), on the other hand, is a continuous variable with a range of -0.8 to about 1.8. That is a contradiction! If our model were true, then the replicated values — the posterior predictions — would look a lot like our original data.\nSadly, most usage of pp_check() does not produce such a clear cut conclusion. We need to decide if \\(y\\) and \\(y_rep\\) are “close enough” that our model can be used as if it were true.\n\n4.3.2.1 Bernoulli\nWe have discovered that our initial assumption about the correct mathematical formula for the data generating mechanism was mistaken. Instead of a gaussion (or Normal) probability model, we should use a Bernoulli model. We must modify our call to brm() as follows:\n\nfit_bern &lt;- brm(formula = biden ~ 1,\n                data = poll_data,\n                family = bernoulli(),\n                refresh = 0,\n                silent = 2,\n                seed = 12)\n\nThis call to brm() is the same as the previous one, except that we have specified the value of the family argument as bernoulli(). (Note that we can give just the name of the family, which is itself a function, or include the parentheses as in bernoulli(). The results are the same.) Recall that the default value of family is gaussian() or, equivalently, guassian. In a Bernoulli data generating mechanism, we have:\n\\[ biden_i  \\sim Bernoulli(\\rho) \\]\nEach voter \\(i\\) which we sample either supports Biden or they do not. If person \\(i\\) supports Biden, the value drawn is 1, which is the standard way of representing TRUE. In other words, \\(biden_i = 1\\) means that person \\(i\\) supports Biden. Similarly, a person not supporting Biden is indicated with 0, meaning that it is FALSE that the person \\(i\\) supports Biden. \\(\\rho\\) is the only parameter in a Bernoulli model. It is the probability that a 1, instead of a 0, is drawn. That is, \\(\\rho\\) is the probability that a person supports Biden or, equivalently, the proportion of all voters who support Biden.\nConsider the fitted object:\n\nfit_bern\n\n Family: bernoulli \n  Links: mu = logit \nFormula: biden ~ 1 \n   Data: poll_data (Number of observations: 1559) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.32      0.05    -0.42    -0.22 1.00     1451     1785\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe output is, of course, very similar to that for fit_gauss. The “Family” is “bernoulli” rather than “gaussian.” The “Links” are “mu = logit” instead of “mu = identity.” There is no link for “sigma” because a Bernoulli model has only one parameter. The “Formula,” “Data,” and “Draws” are the same as before. In order to interpret the Intercept value of -0.32, we need to revisit the concept of a link function.\nVia my friend Claude, we have:\n\nIn statistical modeling, a link function is a mathematical function that relates the linear predictor (the linear combination of the predictors and their coefficients) to the expected value of the response variable. The link function is a key component of generalized linear models (GLMs) and is used to account for the relationship between the predictors and the response variable when the response variable follows a non-normal distribution or has a restricted range.\n\n\nThe purpose of the link function is to transform the expected value of the response variable to a scale that is unbounded and continuous, allowing it to be modeled as a linear combination of the predictors. The choice of the link function depends on the distribution of the response variable and the assumptions made about the relationship between the predictors and the response.\n\nDon’t worry if this is difficult to understand, we will revisit in future chapters.\nThe default link function for a Bernoulli model is logit, as in:\n\\[\n\\rho = \\frac{e^{\\beta_0}}{1 + e^{\\beta_0}}\n\\] By definition, the parameter \\(\\rho\\) is only allowed to take values between 0 and 1. We want to constrain the model so that only these values are even possible. Using the Gaussian model was a bad choice because it allows for values anywhere from positive to negative infinity. The magic of the link function, as you can see, is that it can only ever produce a number close to zero, if \\(\\beta_0\\) is very small, or close to one, if \\(\\beta_0\\) is very large. In other words, the model structure (which is a “logit” function) “bakes in” the restrictions on \\(\\rho\\) from the very start. We call this a “link” function because it links the unknown parameters we are estimating (\\(\\beta_0\\) in this case) to the paramter(s) (just one parameter, \\(\\rho\\), in this case, ) which are part of the data generating mechanism. Once we have \\(\\rho\\) (or, more precisely, the posterior distribution of \\(\\rho\\)) we no longer care about \\(\\beta_0\\).\nWe can now interpret the value of the Intercept (which is the same thing as \\(\\mu\\) in this case, but not the same thing as \\(\\rho\\)) as the value of \\(\\beta_0\\). In fact, the most common mathematical symbol for the intercept of a statistical model is \\(\\beta_0\\).\n\n\\[\n\\begin{eqnarray}\n\\rho &=& \\frac{e^{\\beta_0}}{1 + e^{\\beta_0}}\\\n\\rho &=& \\frac{e^{-0.32}}{1 + e^{-0.32}}\\\n\\rho &\\approx& 0.42\n\\end{eqnarray}\n\\] \n\n\n\n\n\n\n\n\n0.42 is the same number we calculated by hand and the same result when we used family = gaussian(). We can calculate the values for the upper and lower 95% confidence intervals by plugging -0.22 and -0.42 into the same formula.\nWe can create the entire posterior distribution for \\(\\rho\\) using similar code as before:\n\nfit_bern |&gt; \n  add_epred_draws(newdata = tibble(.rows = 1)) |&gt; \n  ggplot(aes(x = .epred)) + \n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 50) + \n    labs(title = expression(paste(\"Posterior Probability Distribution of \", rho)),\n         subtitle = \"Distribution is centered at 42%\",\n         x = expression(paste(\"Proportion, \", rho, \", of Biden Voters\")),\n         y = \"Probability\") + \n    scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\n\n\n\n\n\n\n\nIf both fit_gauss and fit_bern produce the same posterior distribution for \\(\\rho\\), then who cares which one we use? The answer is that fit_bern produces reasonable posterior predictions:\n\nfit_bern |&gt; \n  add_predicted_draws(newdata = tibble(.rows = 1))\n\n# A tibble: 4,000 × 5\n# Groups:   .row [1]\n    .row .chain .iteration .draw .prediction\n   &lt;int&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt;       &lt;int&gt;\n 1     1     NA         NA     1           1\n 2     1     NA         NA     2           1\n 3     1     NA         NA     3           1\n 4     1     NA         NA     4           0\n 5     1     NA         NA     5           1\n 6     1     NA         NA     6           1\n 7     1     NA         NA     7           0\n 8     1     NA         NA     8           0\n 9     1     NA         NA     9           1\n10     1     NA         NA    10           0\n# ℹ 3,990 more rows\n\n\nAll the values of .prediction are either zero or one, as we expect. Consider:\n\npp_check(fit_bern, type = \"bars\")\n\nUsing 10 posterior draws for ppc type 'bars' by default.\n\n\n\n\n\n\n\n\nUsing type = \"bars\" is a good choice for pp_check() when family is bernoulli(). Note how the 10 replications of \\(y_rep\\) are indistinguishable from the true output values \\(y\\). A good model produces replications, sometimes termed “fake data,” which is indistinguishable for the actual data.\n\n4.3.3 Data Generating Mechanism\nWe now have our data generating mechansim:\n\\[ biden_i  \\sim Bernoulli(\\rho = 0.42) \\]\n\n\nUsing data from a YouGov poll of 1,559 US adult citizens, conducted March 10 - 12, 2024, we seek to understand what proportion of voters will support Biden in the 2024 election. Biden’s popularity might change significantly over the course of the election campaign. In the poll, Biden’s support was much less than 50%.\n\nWe have not yet used our DGM to answer our question. But, the DGM itself, prior to us asking any specific question, includes all sorts of interesting information. It is sensible for us to add one aspect of that information to our summary of the work done so far.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "models.html#temperance",
    "href": "models.html#temperance",
    "title": "4  Models",
    "section": "\n4.4 Temperance",
    "text": "4.4 Temperance\n\n\n\n\n\n\n\n\nCourage produced the data generating mechanism. Temperance guides us in the use of the DGM — or the “model” — we have created to answer the questions with which we began. We create posteriors for the quantities of interest. We should be modest in the claims we make. The posteriors we create are never the “truth.” The assumptions we made to create the model are never perfect. Yet decisions made with flawed posteriors are almost always better than decisions made without them.\n\n4.4.1 Questions and Answers\nRecall the question with which we began:\n\nWhat proportion of all votes will be cast for Joe Biden in the 2024 election?\n\nLet’s use fit_bern to answer this question with the help of add_epred_draws().\n\nfit_bern |&gt; \n  add_epred_draws(newdata = tibble(.rows = 1)) |&gt; \n  summarise(\n    lower_95 = quantile(.epred, 0.025),\n    median = quantile(.epred, 0.5),\n    upper_95 = quantile(.epred, 0.975)\n  )\n\n# A tibble: 1 × 4\n   .row lower_95 median upper_95\n  &lt;int&gt;    &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n1     1    0.397  0.420    0.445\n\n\nWe can also refer to the figure we created above of the posterior probability distribution for \\(\\rho\\). Of course, if we want (mostly) precise numbers, we need to do more than just stare at a graphic. We should calculate precise quantiles from the posterior distribution.\n\nUsing data from a YouGov poll of 1,559 US adult citizens, conducted March 10 - 12, 2024, we seek to understand what proportion of voters will support Biden in the 2024 election. Biden’s popularity might change significantly over the course of the election campaign. In the poll, Biden’s support was much less than 50%. We estimate that Biden’s percentage of the vote in Election Day will be about 42%, plus or minus 2.5%.\n\n\n4.4.2 Humility\nWe can never know the truth.\nRecall all the ways in which are DGM is a poor description of reality. We assume representativeness. But that is false! We know that our sample is not perfectly representative of the people who will be voting in the election. No poll is ever perfect. The sort of people who answer polling questions are systematically different from those who do not. Professionals will do their best to “control” for those differences, a topic beyond the scope of this Primer, but those controls will never be perfect.\nWe also assume stability in constructing our DGM. This is certainly false. The 0.42 estimate is highly unlikely to stay stable throughout the election campaign.\nLife is not a textbook. We first calculate the textbook answer, with all of its absurd assumptions. We then use that answer, along with our other knowledge, to produce the best possible estimate for our quantity of interest along with our uncertainty about that estimate.\nOur initial answer to the question has two major problems. First, it does not use information about other US presidential elections. Second, it is over-confident because it assumes that the model’s assumptions are correct.\nA better answer would be:\n\nUsing data from a YouGov poll of 1,559 US adult citizens, conducted March 10 - 12, 2024, we seek to understand what proportion of voters will support Biden in the 2024 election. Biden’s popularity might change significantly over the course of the election campaign. In the poll, Biden’s support was much less than 50%. We estimate that Biden’s percentage of the vote in Election Day will be about 44%, plus or minus 5%.\n\nWait!?! This is not what the model reported!\nWhere did that 44% come from? The model estimated 42%. But, the problem, as always, is that our models are always flawed. They always assume things that are false. They always leave out lots of information. For example, no Democrat has won less than 43% of the votes for president since Walter Mondale in 1980. Is Biden really a worse candidate then every other Democrat in almost 50 years? Probably not.\nDoes that history “prove” anything? No! History suggests. It does not construct proofs. Your job as a data scientist is not simply to read the numbers from your R Console. Your job is to combine those numbers with other sources of information and then offer your best estimate for the quantity of interest.\nWhere did that “plus or minus 5%” come from? The 95% confidence (or uncertainty or whatever) intervals provided by brms run from about 39.7% to 44.5%. That would suggest a plus/minus range of somewhere between 2.3% and 2.5%.\nYou always need to be prepared to give two ranges of uncertainty related to your quantity of interest. First, is the range provided by the model, the one which assumes that every aspect of the model is perfectly true. For this range to be correct, every assumption must be true.\nBut, in addition to the computer output, you bring your experience to bear. You know that polls are volatile. You know that polling is hard. In general, the uncertainty interval provided by the model is a floor for the true uncertainty.\nThe world is always more uncertain than our models would have us believe.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "two-parameters.html",
    "href": "two-parameters.html",
    "title": "5  Two Parameters",
    "section": "",
    "text": "5.1 Wisdom\nWisdom begins with the Preceptor Table. What data would we, ideally, require to answer our questions? We then explore the data that we actually have. We apply the concept of validity to ensure that the data we want and the data we have are similar enough to allow the latter to inform us about the former.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Two Parameters</span>"
    ]
  },
  {
    "objectID": "two-parameters.html#wisdom",
    "href": "two-parameters.html#wisdom",
    "title": "5  Two Parameters",
    "section": "",
    "text": "5.1.1 Preceptor Table\nWhich rows and columns of data do you need such that, if you had them all, the calculation of the quantity of interest would be trivial? The steps we usually take to construct the Preceptor Table include:\nUnits: All the men in the world, one row per man.\nOutcome: This is the variable which we are trying to explain/understand/predict. This is not the same thing as the answer to the question we have been asked. The question might, as above, be about the height of the 3rd tallest man we meet out of the next 100. But the concepts of 3rd or 100 do not appear in the Preceptor Table. Instead, height is our outcome variable. But, if we can build a model which explains/understands/predicts height, we can use that model to answer our questions.\nTreatment: There are not treatment variables.\nCausal or predictive model: We have only one outcome, so the model is predictive.\nCovariates: There are no (explicit) covariates in this model, although we will need to make use of variables like age and sex to construct our sample data.\nMoment in Time: This is often implicit in the question itself. One of our key roles as data scientists is to clarify the questions which we are asked. In this case, it seems clear that the questions refer to now, the present moment.\nPredictive models care nothing about causation. Causal models are often also concerned with prediction, if only as a means of measuring the quality of the model. Here, we are looking at prediction.\nSo, what does our Preceptor Table look like? Assuming we are predicting height for every adult male on Earth at this moment in time, we would have height data for every male at least 18 years of age. This means that we would have about 4 billion rows, one for each male, along with a column for each individual’s height.\nHere are some rows from our Preceptor Table:\n\n\n\n\n\n\n\nID\nHeight (cm)\n\n\n\nPerson 1\n150\n\n\nPerson 2\n172\n\n\n...\n...\n\n\nPerson 45,000\n160\n\n\nPerson 45,001\n142\n\n\n...\n...\n\n\n\n\n\n\n\nThis table would extend all the way until person 4 billion-and-something. If we had this table, all of our questions could be answered with simple math and/or simulations. No inference is necessary if we have a Preceptor Table. But what does our actual data look like?\n\n5.1.2 EDA\nConsider the nhanes data set from the National Health and Nutrition Examination Survey conducted from 2009 to 2011 by the Centers for Disease Control and Prevention.\n\nlibrary(tidyverse)\nlibrary(primer.data)\nlibrary(skimr)\nglimpse(nhanes)\n\nRows: 10,000\nColumns: 15\n$ survey         &lt;int&gt; 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2…\n$ sex            &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"Male…\n$ age            &lt;int&gt; 34, 34, 34, 4, 49, 9, 8, 45, 45, 45, 66, 58, 54, 10, 58…\n$ race           &lt;chr&gt; \"White\", \"White\", \"White\", \"Other\", \"White\", \"White\", \"…\n$ education      &lt;fct&gt; High School, High School, High School, NA, Some College…\n$ hh_income      &lt;fct&gt; 25000-34999, 25000-34999, 25000-34999, 20000-24999, 350…\n$ weight         &lt;dbl&gt; 87.4, 87.4, 87.4, 17.0, 86.7, 29.8, 35.2, 75.7, 75.7, 7…\n$ height         &lt;dbl&gt; 164.7, 164.7, 164.7, 105.4, 168.4, 133.1, 130.6, 166.7,…\n$ bmi            &lt;dbl&gt; 32.22, 32.22, 32.22, 15.30, 30.57, 16.82, 20.64, 27.24,…\n$ pulse          &lt;int&gt; 70, 70, 70, NA, 86, 82, 72, 62, 62, 62, 60, 62, 76, 80,…\n$ diabetes       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ general_health &lt;int&gt; 3, 3, 3, NA, 3, NA, NA, 4, 4, 4, 4, 4, 2, NA, NA, 3, NA…\n$ depressed      &lt;fct&gt; Several, Several, Several, NA, Several, NA, NA, None, N…\n$ pregnancies    &lt;int&gt; NA, NA, NA, NA, 2, NA, NA, 1, 1, 1, NA, NA, NA, NA, NA,…\n$ sleep          &lt;int&gt; 4, 4, 4, NA, 8, NA, NA, 8, 8, 8, 7, 5, 4, NA, 5, 7, NA,…\n\n\nnhanes includes 15 variables, including physical attributes like weight and height. Let’s restrict our attention to three variables: age, sex and height.\n\nnhanes |&gt; \n  select(age, sex, height)\n\n# A tibble: 10,000 × 3\n     age sex    height\n   &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1    34 Male     165.\n 2    34 Male     165.\n 3    34 Male     165.\n 4     4 Male     105.\n 5    49 Female   168.\n 6     9 Male     133.\n 7     8 Male     131.\n 8    45 Female   167.\n 9    45 Female   167.\n10    45 Female   167.\n# ℹ 9,990 more rows\n\n\nExamine a random sample:\n\nnhanes |&gt; \n  select(age, sex, height) |&gt; \n  slice_sample(n = 5)\n\n# A tibble: 5 × 3\n    age sex    height\n  &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;\n1    54 Female   161.\n2    10 Male     148 \n3     5 Female   104.\n4    31 Female   165.\n5     0 Female    NA \n\n\nWe think of both age and height as numbers. And they are numbers! But R distinguishes between “integers” and “doubles,” only the second of which allow for decimal values. In the nhanes data, age is an integer and height is a double.\n\nnhanes |&gt; \n  select(age, sex, height) |&gt; \n  glimpse()\n\nRows: 10,000\nColumns: 3\n$ age    &lt;int&gt; 34, 34, 34, 4, 49, 9, 8, 45, 45, 45, 66, 58, 54, 10, 58, 50, 9,…\n$ sex    &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"Male\", \"Fema…\n$ height &lt;dbl&gt; 164.7, 164.7, 164.7, 105.4, 168.4, 133.1, 130.6, 166.7, 166.7, …\n\n\nBe on the lookout for anything suspicious. Are there any NA’s in your data? What types of data are the columns, i.e. why is age characterized as integer instead of double? Are there more females than males?\nYou can never look at your data too closely.\nIn addition to glimpse(), we can run skim(), from the skimr package, to calculate summary statistics.\n\nnhanes |&gt; \n  select(age, sex, height) |&gt; \n  skim()\n\n\nData summary\n\n\nName\nselect(nhanes, age, sex, …\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\nsex\n0\n1\n4\n6\n0\n2\n0\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nage\n0\n1.00\n36.74\n22.40\n0.0\n17.0\n36\n54.0\n80.0\n▇▇▇▆▅\n\n\nheight\n353\n0.96\n161.88\n20.19\n83.6\n156.8\n166\n174.5\n200.4\n▁▁▁▇▂\n\n\n\n\n\nInteresting! There are 353 missing values of height in our subset of data. Just using glimpse() does not show us that. Let’s filter out the NA’s using drop_na(). This will delete the rows in which the value of any variable is missing. Because we want to examine height in men (not boys, nor females), let’s limit our data to only include adult males.\n\nch5 &lt;- nhanes |&gt; \n  filter(sex == \"Male\", age &gt;= 18) |&gt; \n  select(height) |&gt; \n  drop_na()\n\nLet’s plot this data using geom_histogram().\n\n\nShow the codech5 |&gt;\n  ggplot(aes(x = height)) + \n    geom_histogram(bins = 50) +\n    labs(title = \"Male Adult Height in the US in 2010\",\n         x = \"Height (cm)\",\n         y = \"Count\",\n         caption = \"Source: National Health and Nutrition Examination Survey\"\n         ) +\n    theme_classic()\n\n\n\n\n\n\n\nWill the data we have — which is only for a sample of adult American men more than a decade ago — allow us to answer our questions, however roughly? Only if the assumption of validity makes sense.\n\n5.1.3 Validity\nValidity involves our columns. More specifically, whether our columns mean the same thing. Does “height” in our Preceptor Table mean the same thing as “height” in NHANES? Almost certainly. Of course, we need to be careful about mistakes like measurement units, like centimeters in one and inches in the other. And there can be issues like: Are measurements taken with shoes on or shoes off? But, for the most part, the “height” variable in NHANES in 2010 is a valid proxy for the “height” of individuals today. We can stack the two data sets together and consider them to have come from the same population.\nSince validity holds, we can combine the Preceptor Table and our data into a Population Table.\n\n5.1.4 Population\n\nOne of the most important components of Wisdom is the concept of the “population.”\nThe population is not the set of people for which we have data — the participants in the CDC’s Health and Nutrition Examination Survey conducted from 2009 to 2011. This is the data set. Nor is it the set of all the individuals about whom we would like to have data. Those are the rows in the Preceptor Table. The population is the larger — potentially much larger — set of individuals which include both the data we have and the data we want. Generally, the population will be much larger than either the data we have or the data we want. In fact, there is almost always a time dimension to consider. We generally want to make inferences about right now or about the future. By definition, the data we have is always from the past.\nIn this case, we want to estimate average height for males today, not for people in 2009 – 2011. We also want to estimate height for males outside the United States, a group that is excluded from our data set. Is it reasonable to generate conclusions for the world from this group? Maybe? We have limited data to work with and we have to determine how far we are willing to generalize to other groups.\nIt is a judgment call, a matter of Wisdom, as to whether or not we may assume that the data we have and the data we want to have (i.e., the Preceptor Table) are drawn from the same population.\nIn the social sciences, there is never a perfect relationship between the data you have and the question you are trying to answer. Data for American males in the past is not the same thing as data for American males today. Nor is it the same as the data for men in France or Mexico. Yet, this data is relevant. Right? It is certainly better than nothing.\nUsing not-perfect data is generally better than using no data at all.\nIs not-perfect data always better? No! If your problem is estimating the median height of 5th grade girls in Tokyo, we doubt that our data is at all relevant. Wisdom recognizes the danger of using non-relevant data to build a model and then mistakenly using that model in a way which will only make the situation worse. If the data won’t help, don’t use the data, don’t build a model. Better to just use your common sense and experience. Or find better data.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Two Parameters</span>"
    ]
  },
  {
    "objectID": "two-parameters.html#justice",
    "href": "two-parameters.html#justice",
    "title": "5  Two Parameters",
    "section": "\n5.2 Justice",
    "text": "5.2 Justice\n\n\n\n\n\n\n\n\nHaving looked at our data and decided that it is “close enough” to our questions that creating a model will help us come up with better answers, we move on to Justice.\nJustice emphasizes a few key concepts:\n\nThe Population Table, a structure which includes a row for every unit in the population. We generally break the rows in the Population Table into three categories: the data for units we want to have (the Preceptor Table), the data for units which we actually have (our actual data), and the data for units we do not care about (the rest of the population, not included in the data or the Preceptor Table).\nStability means that the relationship between the columns in the Population Table is the same for three categories of rows: the data, the Preceptor Table, and the larger population from which both are drawn.\nRepresentativeness, or the lack thereof, concerns two relationship, among the rows in the Population Table. The first is between the Preceptor Table and the other rows. The second is between our data and the other rows. Ideally, we would like both the Preceptor Table and our data to be random samples from the population. Sadly, this is almost never the case.\nUnconfoundedness means that the treatment assignment is independent of the potential outcomes, when we condition on pre-treatment covariates. This assumption is only relevant for causal models. We write that a model is “confounded” if this is not true. The easiest way to ensure unconfoundedness is to randonly assign treatment.\nThe mathematical structure of the Data Generating Mechanism. Models require math, so we need to create a mathematical formula which connects our outcome to our covariates.\n\n\n5.2.1 The Population Table\nThe Population Table shows rows from three sources: the Preceptor Table, the actual data, and the population (outside of the data).\nOur Preceptor Table rows contain the information that we would want to know in order to answer our questions. These rows contain entries for our covariates (sex and year) but they do not contain any outcome results (height). We are trying to answer questions about the male population in 2024, so our sex entries for these rows will read “Male” and our year entries of these rows will read “2024”.\nOur actual data rows contain the information that we do know. These rows contain entries for both our covariates and the outcomes. In this case, the actual data comes from a study conducted on males in 2009-2011, so our sex entries for these rows will read “Male” and our year entries of these rows will either read “2009”, “2010”, or “2011”.\nOur other rows contain no data. These are subjects which fall under our desired population, but for which we have no data. As such, all outcomes and covariates are missing. (A subtle point is that, even for other data, we “know” the ID and the Year for each subject. Of course, we don’t really know these things, but, conceptually, we are defining the meaning of those rows on the basis of those variables.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource\nSex\nYear\nHeight\n\n\n\nPreceptor Table\nMale\n2024\n?\n\n\nPreceptor Table\nMale\n2024\n?\n\n\n…\n…\n…\n…\n\n\nActual Data\nMale\n2009\n180\n\n\nActual Data\nMale\n2011\n160\n\n\nActual Data\nMale\n2010\n168\n\n\n…\n…\n…\n…\n\n\nOther\n?\n?\n?\n\n\nOther\n?\n?\n?\n\n\nOther\n?\n?\n?\n\n\n\n\n\n\n\n\n5.2.2 Representativeness\nRepresentativeness involves the rows of the Population Table. More specifically, are the rows that we do have data for representative of the rows for which we do not have data? Ideally, the data we have is a random, unbiased selection from our population, and so the answer to our question is yes.\nFor our nhanes data, is this the case? It is time to investigate.\nAccording to the CDC, individuals are invited to participate in NHANES based on a randomized process. First, the United States is divided into a number of geographical groups (to ensure counties from all areas). From each of these groups, counties are randomly selected to participate. After a county has been randomly selected, members of the households in that county are notified of the upcoming survey, and must volunteer their time to participate. It is clear that this process goes through several layers of randomization (promising!). That being said, many counties are excluded by the end of the process. It is also possible for certain groups or communities to be less representative of the greater population, though we cannot know that for certain.\nThere is also the fact that participation is voluntary. Perhaps certain individuals (immobile, elderly, anxious) are less likely to participate. Perhaps individuals that are hospitalized do not get the opportunity to participate. This impacts our data!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Two Parameters</span>"
    ]
  },
  {
    "objectID": "two-parameters.html#courage",
    "href": "two-parameters.html#courage",
    "title": "5  Two Parameters",
    "section": "\n5.3 Courage",
    "text": "5.3 Courage\n\n\n\n\n\n\n\n\nIn data science, we deal with words, math, and code, but the most important of these is code. We need Courage to create the model, to take the leap of faith that we can make our ideas real.\nJustice gave us the Population Table. Courage selects the data generating mechanism. We first specify the mathematical formula which connects the outcome variable we are interested in with the other data that we have. We explore different models. We need to decide which variables to include and to estimate the values of unknown parameters. We check our models for consistency with the data we have. We avoid hypothesis tests. We select one model.\nWe use a simple linear model:\n\\[ y_i =  \\mu + \\epsilon_i \\]\nwith \\(\\epsilon_i \\sim N(0, \\sigma^2)\\). \\(y_i\\) is the height of male \\(i\\). \\(\\mu\\) is the average height of all males in the population. \\(\\epsilon_i\\) is the “error term,” the difference between the height of male \\(i\\) and the average height of all males.\n\\(\\epsilon_i\\) is normally distributed with a mean of 0 and a standard deviation of \\(\\sigma\\). The mean being 0 relates to our concept of accuracy; we are assuming that our data is representative enough to be accurate, and so we can expect our average error to be 0. The standard deviation, on the other hand, relates to our concept of precision; the smaller \\(\\sigma\\) is, the more precise our data is, and the larger \\(\\sigma\\) is, the less precise our data is.\nThis is the simplest model we can construct. Note:\n\nThe model has two unknown parameters: \\(\\mu\\) and \\(\\sigma\\). Before we can do anything else we need to estimate the values of these parameters. Can we ever know their exact value? No! Perfection lies only in God’s own R code. But, by using a Bayesian approach similar to what we used in Chapters Chapter 2 and Chapter 4, we will be able to create a posterior probability distribution for each parameter.\n\n\nThe model is wrong, as are all models.\nThe parameter we most care about is \\(\\mu\\). That is the parameter with a substantively meaningful interpretation. Not only is the meaning of \\(\\sigma\\) difficult to describe, we also don’t particular care about its value. Parameters like \\(\\sigma\\) in this context are nuisance or auxiliary parameters. We still estimate their posterior distributions, but we don’t really care what those posteriors look like.\n\\(\\mu\\) is not the average height of the men in the sample. We can calculate that directly. It is 175.870667. No estimation required! Instead, \\(\\mu\\) is the average height of men in the population. Recall from the discussions in Chapter 4 that the population is the universe of people/units/whatever about which we seek to draw conclusions. On some level, this seems simple. On a deeper level, it is very subtle. For example, if we are walking around Copenhagen, then the population we really care about, in order to answer our three questions, is the set of adult men which we might meet today. This is not the same as the population of adult men in the US in 2010. But is it close enough? Is it better than nothing? We want to assume that both men from nhanes (the data we have) and men we meet in Copenhagen today (the data we want to have) are drawn from the same population. Each case is a different and the details matter.\n\\(\\sigma\\) is an estimate for the standard deviation of the errors, i.e., variability in height after accounting for the mean.\n\n\n5.3.1 Models\nLet’s estimate a simple version of the model. First, we need to load the brms and tidybayes packages.\n\nlibrary(brms)\nlibrary(tidybayes)\n\nBecause we are estimating a linear model, we begin with:\n\nfit_1 &lt;- brm(formula = height ~ 1,\n             data = ch5,\n             family = gaussian(),\n             silent = 2,\n             refresh = 0,\n             seed = 12)\n\nNote:\n\nThere is a direct connection between the mathematical form of the model created under Justice and the code we use to fit the model under Courage. height ~ 1 is the code equivalent of \\(y_i =  \\mu\\).\nSetting family = gaussian() implies that \\(\\epsilon_i \\sim N(0, \\sigma^2)\\), just as we assumed. That is not a coincidence! If \\(\\epsilon_i\\) had a different distribution, we would need to use a different statistical family.\nThere are several ways to examine the fitted model. The simplest is to print it. Recall that just typing x at the prompt is the same as writing print(x).\n\n\nfit_1\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 \n   Data: ch5 (Number of observations: 3658) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   175.87      0.12   175.63   176.11 1.00     3788     2818\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     7.48      0.09     7.31     7.65 1.00     3893     2633\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNote:\n\nWe will be looking at this print out many times in the rest of the Primer. You will get used to it. The header information about Family, Links and so on just confirms what we already know. Still, you want to check this to make sure you did not make a mistake in the call to brm().\nThe key parameter is the “Intercept,” which is the same thing as \\(\\mu\\) in the mathematical description of the model and the same thing as 1 when we set formula = height ~ 1. In other words, we are speaking three languages here: English (“Intercept”), math ($mu$), and code (height ~ 1). But all three languages are referring to the same underlying concept.\nThe “Estimate” of 175.87 makes sense. After all, the simple mean of the height in the data is:\n\n\nmean(ch5$height)\n\n[1] 175.8707\n\n\n\nThe estimated error (“Est.Error”) also makes sense. This is the standard error and can be calculated by hand as the standard deviation of the data divided by the squart root of the number of observations:\n\n\nsd(ch5$height) / sqrt(3658)\n\n[1] 0.1237131\n\n\n\nThe end points of the confidence intervals are simply the estimate \\(\\pm\\) two times the standard error.\nWe don’t really care about the values associated with sigma, which has no physical meaning. Parameters like this are often referred to as “nuisance” parameters because, although they are necessary to constructing the fitted model, we don’t really care about them outside that context. The Intercept, on the other hand, is the mean of our posterior distribution for the unknown parameter \\(\\mu\\), the average height of an adult male.\n\nConsider the posterior distribution of \\(\\mu\\).\n\nfit_1 |&gt; \n  add_epred_draws(newdata = tibble(.rows = 1)) |&gt; \n  ggplot(aes(x = .epred)) + \n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 50) + \n    labs(title = expression(paste(\"Posterior Probability Distribution of \", mu)),\n         subtitle = \"Distribution is centered at 175.9 cm\",\n         x = expression(paste(\"Mean, \", mu, \", height of adult men\")),\n         y = \"Probability\") + \n  \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\n\n\n\n\n\n\n\nThis is almost exactly the same code as we used in ?sec-modeling. It is the same code which we will use in the future.\n\n5.3.2 Data Generating Mechanism\nWe modeled height, a continuous variable measured in centimeters, as a linear function of a constant term. The average adult male height in the US was around 176 cm. Mathematically:\n\\[ height_i =  176 + \\epsilon_i \\]\nwith \\(\\epsilon_i \\sim N(0, 0.75^2)\\).\n\n5.3.3 Tests\nBefore excepting fit_1 as our data generating mechanism, we should perform a posterior predictive check.\n\nShow the codebrms::pp_check(fit_1)\n\nUsing 10 posterior draws for ppc type 'dens_overlay' by default.\n\n\n\n\n\n\n\n\nNote how similar our actual data, \\(y\\), is to the 10 versions of the replicated data, \\(y_rep\\). They are close enough that we are happy to use fit_1. However, the match is not perfect! The actual data is slightly more “peaked” and also features some weird bumps in the tails, especially around 200 cm. It would be possible, but not easy, to modify our model to match the actual data more closely. For now, we will just accept fit_1 as our data generating mechanism.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Two Parameters</span>"
    ]
  },
  {
    "objectID": "two-parameters.html#temperance",
    "href": "two-parameters.html#temperance",
    "title": "5  Two Parameters",
    "section": "\n5.4 Temperance",
    "text": "5.4 Temperance\n\n\n\n\n\n\n\n\n\n5.4.1 Questions and Answers\n\nWhat is the probability that the next adult male we meet will be taller than 180 centimeters?\n\nThere are two fundamentally different kinds of unknowns which we care about: expected values and predicted values. With the former, we are not interested in any specific individual. The individual value is irrelevant. With predicted values, we care, not about the average, but about this specific person. With the former, we use add_epred_draws(). With the latter, the relevant function is add_predicted_draws(). Both functions return draws from a posterior probability distribution, but the unknown number which underlies the posterior are very different.\nRecall the mathematics:\n\\[ y_i =  \\mu + \\epsilon_i \\]\nWith expected values or averages, we can ignore the \\(\\epsilon_i\\) term in this formula. The expected value of \\(\\epsilon_i\\) is zero since, by assumption, \\(\\epsilon_i \\sim N(0, \\sigma^2)\\). However, we can’t ignore \\(\\epsilon_i\\) when predicting the height for a single individual.\n\nfit_1 |&gt; \n  add_predicted_draws(newdata = tibble(.rows = 1))\n\n# A tibble: 4,000 × 5\n# Groups:   .row [1]\n    .row .chain .iteration .draw .prediction\n   &lt;int&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt;       &lt;dbl&gt;\n 1     1     NA         NA     1        170.\n 2     1     NA         NA     2        180.\n 3     1     NA         NA     3        185.\n 4     1     NA         NA     4        177.\n 5     1     NA         NA     5        190.\n 6     1     NA         NA     6        168.\n 7     1     NA         NA     7        179.\n 8     1     NA         NA     8        177.\n 9     1     NA         NA     9        176.\n10     1     NA         NA    10        182.\n# ℹ 3,990 more rows\n\n\nAs before, it is straightforward to turn draws from the posterior probability distribution into a graphic:\n\nfit_1 |&gt; \n  add_predicted_draws(newdata = tibble(.rows = 1)) |&gt; \n  ggplot(aes(x = .prediction)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for Height of Random Male\",\n         subtitle = \"Uncertainty for a single individual is much greater than for the expected value\",\n         x = \"Height (cm)\",\n         y = \"Probability\",\n         caption = \"Data source: NHANES\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\n\n\n\nNote:\n\nThe posterior for an individual is much wider than the posterior for the expected value.\nEyeballing, seems like there is a 1 out of 3 chance that the next man we meet, or any randomly chosen man, is taller than 180 cm.\nWe can calculate the exact probability by manipulating the tibble of draws directly.\n\n\nfit_1 |&gt; \n  add_predicted_draws(newdata = tibble(.rows = 1)) |&gt; \n  mutate(tall = if_else(.prediction &gt; 180, TRUE, FALSE)) |&gt; \n  summarize(odds = mean(tall))\n\n# A tibble: 1 × 2\n   .row  odds\n  &lt;int&gt; &lt;dbl&gt;\n1     1 0.283\n\n\nIf 30% or so of the draws from the posterior probability distribution are greater than 180 cm, then there is about a 30% chance that the next individual will be taller than 180 cm.\nAgain, the key conceptual difficulty is the population. The problem we actually have involves walking around London, or wherever, today. The data we have involve America in 2010. Those are not the same things! But they are not totally different. Knowing whether the data we have is “close enough” to the problem we want to solve is at the heart of Wisdom. Yet that was the decision we made at the start of the process, the decision to create a model in the first place. Now that we have created a model, we look to the virtue of Temperance for guidance in using that model. The data we have is never a perfect match for the world we face. We need to temper our confidence and act with humility. Our forecasts will never be as good as a naive use of the model might suggest. Reality will surprise us. We need to take the model’s claims with a family-sized portion of salt.\n\nWhat is the probability that, among the next 4 men we meet, the tallest is at least 10 cm taller than the shortest?\n\nBayesian models are beautiful because, via the magic of simulation, we can answer (almost!) any question. Because the question is about four random individuals, we need add_predicted_draws() to give us four sets of draws from four identical posterior probability distributions. All we need to do is to change 1 to 4 in our previous code and then widen the data. If you need to predict X individuals, then you need a tibble with X rows, regardless of whether or not those rows are otherwise identical.\n\nfit_1 |&gt; \n  add_predicted_draws(newdata = tibble(.rows = 4)) |&gt;\n  select(.row, .draw, .prediction) |&gt; \n  pivot_wider(values_from = .prediction, names_from = .row)\n\n# A tibble: 4,000 × 5\n   .draw   `1`   `2`   `3`   `4`\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1  176.  177.  176.  176.\n 2     2  177.  186.  180.  187.\n 3     3  186.  181.  165.  187.\n 4     4  180.  177.  168.  175.\n 5     5  185.  186.  175.  180.\n 6     6  160.  177.  162.  189.\n 7     7  189.  176.  177.  187.\n 8     8  175.  178.  161.  153.\n 9     9  177.  168.  177.  178.\n10    10  170.  169.  167.  172.\n# ℹ 3,990 more rows\n\n\nAgain, we have the subtle issue. In the same way that light is both a wave and a particle, these columns are both posteriors and draws from those posteriors, when considered row by row. In each row, we are running an experiment in which we meet 4 men. We measure their heights, we determine if the tallest is more than 10 centimeters taller than the shortest. We then do the same thing is row 2, row 3 and so on. The functions row_wise() and c_across() make this (mostly) simple.\n\ndraws &lt;- fit_1 |&gt; \n  add_predicted_draws(newdata = tibble(.rows = 4)) |&gt;\n  select(.row, .draw, .prediction) |&gt; \n  pivot_wider(values_from = .prediction, names_from = .row) |&gt; \n  rowwise() %&gt;%\n  mutate(diff = max(c_across(`1`:`4`)) - min(c_across(`1`:`4`)))\n\ndraws\n\n# A tibble: 4,000 × 6\n# Rowwise: \n   .draw   `1`   `2`   `3`   `4`  diff\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1  183.  174.  171.  178. 12.4 \n 2     2  179.  186.  174.  173. 12.3 \n 3     3  177.  184.  177.  179.  6.59\n 4     4  180.  173.  170.  179. 10.8 \n 5     5  173.  177.  176.  166. 11.6 \n 6     6  170.  173.  182.  181. 11.9 \n 7     7  176.  182.  183.  179.  6.37\n 8     8  173.  176.  159.  164. 16.4 \n 9     9  179.  180.  174.  189. 15.4 \n10    10  185.  172.  164.  194. 29.8 \n# ℹ 3,990 more rows\n\n\nUnfortunately, this code can take a while to run, so we save the results in a permanent tibble named draws. This will be a common approach, just as we have saved the fitted model object as fit_1, or whatever, rather than recreating it each time.\nThe next step is to calculate the number of interest. We can not, directly, draw the height of the tallest or shortest out of 4 random men. However, having drawn 4 random men, we can calculate those numbers, and the difference between them.\nThese steps serve as a template for much of the analysis we do later. It is often very hard to create a model directly of the thing we want to know. There is no easy way to create a model which estimates this height difference directly. It is easy, however, to create a model which allows for random draws.\nGive us enough random draws, and a tibble in which to store them, and we can estimate the world.\nOnce we have random draws from the posterior distribution we care about, graphing the posterior probability distribution is the same-old, same-old.\n\nShow the codedraws %&gt;%\n  mutate(diff = max(c_across(`1`:`4`)) - min(c_across(`1`:`4`))) |&gt; \n  ggplot(aes(x = diff)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for Max Height Difference Among Four Men\",\n         subtitle = \"The expected value for this difference would be much more narrow\",\n         x = \"Height Difference in Centimeters\",\n         y = \"Probability\",\n         caption = \"Data source: NHANES\") + \n    scale_x_continuous(breaks = seq(0, 50, 10),\n                       labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) \n\n\n\n\n\n\n\nThere is about an 78% chance that, when meeting 4 random men, the tallest will be at least 10 cm taller than the shortest. Exact calculation:\n\nsum(draws$diff &gt; 10) / length(draws$diff)\n\n[1] 0.77725\n\n\n\n\nWhat is our posterior probability distribution of the height of the 3rd tallest man out of the next 100 we meet?\n\nThe same approach will work for almost any question.\n\nfit_1 |&gt; \n  add_predicted_draws(newdata = tibble(.rows = 100)) |&gt;\n  select(.row, .draw, .prediction) |&gt; \n  pivot_wider(values_from = .prediction, names_from = .row)\n\n# A tibble: 4,000 × 101\n   .draw   `1`   `2`   `3`   `4`   `5`   `6`   `7`   `8`   `9`  `10`  `11`  `12`\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1  178.  176.  176.  179.  176.  179.  184.  175.  185.  179.  185.  163.\n 2     2  172.  171.  176.  175.  163.  175.  177.  173.  182.  175.  163.  176.\n 3     3  177.  184.  175.  182.  176.  180.  175.  174.  162.  165.  175.  162.\n 4     4  183.  178.  190.  165.  173.  185.  177.  179.  174.  179.  187.  178.\n 5     5  168.  173.  187.  171.  177.  179.  190.  186.  187.  155.  185.  190.\n 6     6  184.  169.  162.  186.  160.  169.  172.  171.  163.  171.  175.  178.\n 7     7  170.  178.  166.  172.  170.  185.  167.  180.  189.  173.  186.  187.\n 8     8  171.  171.  174.  172.  171.  175.  186.  164.  183.  174.  175.  176.\n 9     9  173.  182.  172.  182.  181.  168.  175.  197.  168.  168.  187.  173.\n10    10  185.  173.  184.  181.  166.  178.  173.  167.  184.  171.  176.  178.\n# ℹ 3,990 more rows\n# ℹ 88 more variables: `13` &lt;dbl&gt;, `14` &lt;dbl&gt;, `15` &lt;dbl&gt;, `16` &lt;dbl&gt;,\n#   `17` &lt;dbl&gt;, `18` &lt;dbl&gt;, `19` &lt;dbl&gt;, `20` &lt;dbl&gt;, `21` &lt;dbl&gt;, `22` &lt;dbl&gt;,\n#   `23` &lt;dbl&gt;, `24` &lt;dbl&gt;, `25` &lt;dbl&gt;, `26` &lt;dbl&gt;, `27` &lt;dbl&gt;, `28` &lt;dbl&gt;,\n#   `29` &lt;dbl&gt;, `30` &lt;dbl&gt;, `31` &lt;dbl&gt;, `32` &lt;dbl&gt;, `33` &lt;dbl&gt;, `34` &lt;dbl&gt;,\n#   `35` &lt;dbl&gt;, `36` &lt;dbl&gt;, `37` &lt;dbl&gt;, `38` &lt;dbl&gt;, `39` &lt;dbl&gt;, `40` &lt;dbl&gt;,\n#   `41` &lt;dbl&gt;, `42` &lt;dbl&gt;, `43` &lt;dbl&gt;, `44` &lt;dbl&gt;, `45` &lt;dbl&gt;, `46` &lt;dbl&gt;, …\n\n\nAgain, we have 100 (identical) posteriors, each representing the height of a random adult male. Each column is a posterior. (In later models, the columns will represent different posteriors, but we keep things simple for now.) Each row can then be viewed as an experiment. Instead of just meeting one random man, however, we meet 100. Draws from a posterior are the same thing as one random unit from the underlying population.\nThe magic of rowwise()and c_across() allows us to easily determine the height of the tallest man out of 100.\n\ndraws &lt;- fit_1 |&gt; \n  add_predicted_draws(newdata = tibble(.rows = 100)) |&gt;\n  select(.row, .draw, .prediction) |&gt; \n  pivot_wider(values_from = .prediction, names_from = .row) |&gt; \n  rowwise() %&gt;%\n  mutate(third_tallest = sort(c_across(`1`:`100`))[98])\n\n\nShow the codedraws |&gt; \n  ggplot(aes(x = third_tallest, y = after_stat(count / sum(count)))) +\n    geom_histogram(bins = 100) +\n    labs(title = \"Posterior for Height of 3rd Tallest Man from Next 100\",\n         subtitle = \"Should we have more or less certainty about behavior in the tails?\",\n         x = \"Height (cm)\",\n         y = \"Probability\",\n         caption = \"Data source: NHANES\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) \n\n\n\n\n\n\n\n\n5.4.2 Humility\nWhen answering questions as we have been, it can be easy to falsely believe that we are delivering the truth. This is not the case. In fact, there are three primary levels of knowledge which we need to understand in order to account for our uncertainty.\nThe three primary levels of knowledge possible knowledge in our scenario include: the Truth (the Preceptor Table), the DGM Posterior, and Our Posterior.\nIf we know the Truth (with a capital “T”), then we know the Preceptor Table. With that knowledge, we can directly answer our question precisely. We can calculate each individual’s height, and any summary measure we might be interested in, like the average height for different ages or countries.\nThis level of knowledge is possible only under an omniscient power, one who can see every outcome in every individual under every treatment. The Truth would show, for any given individual, their actions under control, their actions under treatment, and each little factor that impacted those decisions.\nThe Truth represents the highest level of knowledge one can have — with it, our questions merely require algebra or, at worst, simulation. There is no need to estimate a treatment effect, or the different treatment effects for different groups of people. We would not need to predict at all — we would know.\nThe DGM posterior is the next level of knowledge, which lacks the omniscient quality of The Truth. This posterior is the posterior we would calculate if we had perfect knowledge of the data generating mechanism, meaning we have the correct model structure and exact parameter values. This is often falsely conflated with “our posterior,” which is subject to error in model structure and parameter value estimations.\nWhat we do with the DGM posterior is the same as our posterior — we estimate parameters based on data and predict the future with the latest and most relevant information possible. The difference is that, when we calculate posteriors for an unknown value in the DGM posterior, we expect those posteriors to be perfect.\nUnfortunately, our posterior possesses even less certainty! In the real world, we don’t have perfect knowledge of the DGM: the model structure and the exact parameter values. What does this mean?\nWhen we go to our boss, we tell them that this is our best guess. It is an informed estimate based on the most relevant data possible. From that data, we have created a posterior for the average height of males.\nDoes this mean we are certain that the average height lies is the most probable outcome in our posterior? Of course not! As we would tell our boss, it would not be shocking to find out that the actual average height was less or more than our estimate.\nThis is because a lot of the assumptions we make during the process of building a model, the processes in Wisdom, are subject to error. Perhaps our data did not match the future as well as we had hoped. Ultimately, we try to account for our uncertainty in our estimates. Even with this safeguard, we aren’t surprised if we are a bit off.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Two Parameters</span>"
    ]
  },
  {
    "objectID": "three-parameters-causal.html",
    "href": "three-parameters-causal.html",
    "title": "6  Three Parameters: Causal",
    "section": "",
    "text": "6.1 Wisdom\nWisdom requires the creation of a Preceptor Table, an examination of our data, and a determination, using the concept of “validity,” as to whether or not we can (reasonably!) assume that the two come from the same population.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Three Parameters: Causal</span>"
    ]
  },
  {
    "objectID": "three-parameters-causal.html#wisdom",
    "href": "three-parameters-causal.html#wisdom",
    "title": "6  Three Parameters: Causal",
    "section": "",
    "text": "6.1.1 Preceptor Table\nCausal or predictive model: In this case, the model is clearly causal, so our Preceptor Table will have two columns for the potential outcomes. If all you need to know to answer the question is the outcome under one value of the treatment, then the model is predictive, even if there is a treatment.\nOutcome: A person’s attitude toward immigration is the outcome.\nFor the questions that we have at hand we will be talking about all the adults in July 1, 2012 at the location of Chicago, Illinois which will allow us to answer the questions more accurately.\nUnits: Our units for this scenario would be individuals because the questions are about the attributes of unique people at the station. The question does not specify which individuals we are interested in, so assume it is adults in Chicago.\nTreatment: In any causal model, there is at least one covariate which is defined as the “treatment,” something which we can manipulate so that some units receive one version and other units get a different version. A “treatment” is just a covariate which we could manipulate, at least in theory. In this case, the treatment is exposure to Spanish-speakers. Units can either be exposed, i.e., they receive the “treatment,” or they can not be exposed, i.e., they receive the “control.”\nMoment in Time: We are interested in the causal effect today, in the year 2024.\nAgain, some of these details were not indicated in the original question. We almost always have to enage in a dialogue with whomever asked that question.\nOur Preceptor Table:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\nPotential Outcomes\nCovariate\n\n\nControl Ending Attitude\nTreated Ending Attitude\nTreatment\n\n\n\n\n1\n5*\n8\nYes\n\n\n2\n7\n4*\nNo\n\n\n…\n…\n…\n…\n\n\n10\n3*\n5\nYes\n\n\n11\n10*\n7\nYes\n\n\n…\n…\n…\n…\n\n\nN\n6\n13*\nNo\n\n\n\n\n\n\n\n\nStarred values were not observed in the experiment. That is, Person 10 received the treatment and, afterwards, had a value of 5 for her attitude toward immigration. We don’t know — we can’t know — what her attitude would have been if she had received control instead. But, with the tools of inference, we can estimate what that value would have been if she had, in fact, received control.\nA Preceptor Table is the smallest possible table with rows and columns such that, if there is no missing data, all our questions are easy to answer. To answer questions about the average treatment effect, of exposing people to Spanish-speakers, on their attitudes toward immigration, we need a row for every adult in Chicago in 2024.\n\n6.1.2 Exploratory Data Analysis\nRecall the discussion from Chapter 1. Enos (2014) randomly placed Spanish-speaking confederates on nine train platforms around Boston, Massachusetts. The data that we want to analyze consists of the attitude toward immigration after the experiment is complete (att_end) and the exposure to Spanish-speakers (treatment) of each individual on these train platforms.\n\ntrains |&gt;\n  select(att_end, treatment)\n\n# A tibble: 115 × 2\n   att_end treatment\n     &lt;dbl&gt; &lt;fct&gt;    \n 1      11 Treated  \n 2      10 Treated  \n 3       5 Treated  \n 4      11 Treated  \n 5       5 Control  \n 6      13 Treated  \n 7      13 Control  \n 8      11 Treated  \n 9      12 Control  \n10      10 Treated  \n# ℹ 105 more rows\n\n\nThe treatment can either be “Treated” or “Control” which are the two factors that may influence att_end. Participants were asked three questions about immigration issues, each of which allowed for an answer indicating strength of agreement on a scale form 1 to 5, with higher values for att_end indicating more agreement with conservative viewpoints.\n\ntrains |&gt;\n  select(att_end, treatment) |&gt;\n  summary()\n\n    att_end         treatment \n Min.   : 3.000   Treated:51  \n 1st Qu.: 7.000   Control:64  \n Median : 9.000               \n Mean   : 9.139               \n 3rd Qu.:11.000               \n Max.   :15.000               \n\n\nThe data include information about each respondent’s sex, political affiliations, age, income and so on. treatment indicates whether a subject was in the control or treatment group. The key outcome is their attitude toward immigration after the experiment: att_end.\nsummary() shows us what the different values of att_end and treatment are because it is a factor. The range for att_end seems reasonable.\n\nShow the codetrains |&gt; \n  ggplot(aes(x = att_end, fill = treatment)) +\n    geom_bar(aes(y = after_stat(count/sum(count))),\n                   position = \"dodge\") +\n    labs(title = \"Ending Attitude Toward Immigration\",\n         subtitle = \"Treated Individuals Are More Conservative\",\n         x = \"Attitude\",\n         y = \"Probability\",\n         fill = NULL) +\n    scale_y_continuous(labels = scales::percent_format()) + \n    theme_classic()\n\n\n\n\n\n\n\nWhen compared with individuals in the control group, the treated individuals seem more conservative.\n\n6.1.3 Validity\nValidity concerns the relationship between the columns in the Preceptor Table and the data. Are they similar enough that we can “stack” one on top of the other in order to create a Population Table?\natt_end, the outcome measure in our data is fairly clear measure of one’s attitude toward immigration. But is that exactly what we care about in the Chicago of 2024? Probably not. Politics in America has changed a great deal, not least with the Trump presidency. The phrases used in the 2012 survey do not mean the same thing today as they meant then. The aspects of immigration policy which we are most interested in have changed. But in a conflict between the data and the Preceptor Table, the latter must adjust because we can’t (easily) change the data.\ntreatment, the treatment variable in our data, involves Spanish-speakers on commuter train platforms. Is that what we are proposing to do in Chicago in 2024? Probably not! And, even if we actually attempted to use that treatment today, we would never be able to replicate it exactly, not just because we almost certainly could not get the same Spanish-speakers to come to Chicago. And, even if we did, they would be older, and different in other ways. Outside of scientific experiments, it is almost never the case that the treatment variable in the data will perfectly match the treatment variable in the Preceptor Table.\nAs always, we face a choice. We can either give up on the analysis — and this is often the correct decision! — or we can adjust the meaning of the columns in the Preceptor Table so that they are “close enough” to the data that validity holds. You won’t be surprised to see that we choose the option behind Door #2!\n\nUsing data from a 2012 survey of Boston-area commuters, we seek to measure the causal effect of exposure to Spanish-speakers on attitudes toward immigration among adults in Chicago and similar cities in 2024.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Three Parameters: Causal</span>"
    ]
  },
  {
    "objectID": "three-parameters-causal.html#justice",
    "href": "three-parameters-causal.html#justice",
    "title": "6  Three Parameters: Causal",
    "section": "\n6.2 Justice",
    "text": "6.2 Justice\n\n\n\n\nJustice\n\n\n\nThe four concerns of Justice remain the same: the Population Table, stability, representativeness, and unconfoundedness.\n\n6.2.1 Population Table\nAfter assuming validity, we can now create our Population Table. Recall that every row from both the Preceptor Table and the data is included in the Population Table (at least at the start), along with all the rows from the underlying population from which we assume that both the Preceptor Table and the data were drawn. The Population Table includes rows from three sources: the Preceptor Table, the actual data, and all other members of the population.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation Table\n\n\nSource\nPotential Outcomes\nCovariates\n\n\nControlled\nTreated\nTreatment\nYear\nCity\n\n\n\n\n…\n…\n…\n…\n…\n…\n\n\nData\n2\n7*\nNo\n2012\nBoston, MA\n\n\nData\n10*\n6\nYes\n2012\nBoston, MA\n\n\n…\n…\n…\n…\n…\n…\n\n\n…\n…\n…\n…\n…\n…\n\n\nPreceptor Table\n…\n…\n…\n2024\nChicago, IL\n\n\nPreceptor Table\n…\n…\n…\n2024\nChicago, IL\n\n\n…\n…\n…\n…\n…\n…\n\n\n\n\n\n\n\nNote: the values that have a star next to them symbolize the possible values that may exist if they were “control” instead of “treated” or vice versa.\nOur year within the Population table is an example of the moment in time.\nOur actual data rows contain the information that we do know. These rows contain entries for both our covariates and the outcomes. In this case, the actual data comes from a study conducted on train commuters around Boston, MA in 2012, so our city entries for these rows will read “Boston, MA” and our year entries of these rows will read “2012”.\nDoes the Population Table have to include all the rows in the data? No! For example, if our question is just about attitudes toward immigration among men, we might delete all the rows with female respondents from the data before constructing the Population Table.\nEach row in the Population Table represents a unique ID/time combination. We rarely specify the Population Table exactly. For example, does the Population Table extend to 2010? Or 2050? Does it include Los Angeles? How about London? None of those questions have to be answered for us to make progress. As long as there is an underlying population from which we may assume that the rows in both the data and the Preceptor Table have been drawn, we can proceed.\n\n6.2.2 Stability\nIf the assumption of stability holds, then, first, the relationships between the columns in 2012 is the same as the relationships across the entire population and, second, that the same applies to the relationships between the columns in 2024.\nRecall the underlying logic of every data science analysis. We start wih data that was collected at a specific moment in time. We estimate a relationship between the variables, in this case between treatment and att_end. We then assume that this relationship holds beyond the time period of the data itself. First, we assume that the relationship holds across a larger time period than just the moment corresponding to our data gathering. Second, we then assume that this relationship also holds for the moment in time corresponding to our Preceptor Table.\nThe problem is that there are lots of reasons for thinking that stability does not hold in this case because US politics has changed so much since 2012, especially in regard to immigration. Immigration is much more salient now then it was then, so it is likely that the effect of the treatment might be very different today. However, if we don’t assume stability then we can’t use data from 2012 to inform our inferences about 2024. So, we assume it.\n\n6.2.3 Representativeness\nRepresentativeness, or the lack thereof, concerns two relationship, among the rows in the Population Table. The first is between the Preceptor Table and the other rows from that moment in time. The second is between our data and the other rows from that moment.\nStability looks across time periods. Representativeness looks within time periods.\nOur Preceptor Table is not a random draw from the underlying population today. We only care about Chicago (and not any other city). If the Preceptor Table is a random draw, then, by definition, it should be representative of the underlying population, at least within a given time period. Just because something is true across all cities in 2024 does not guarantee that it is true in Chicago. In fact, it is only true if Chicago is representative of the other 2024 rows from the population.\nA similar set of issues apply to our data. Ideally, our data would be a random sample from the underlying population in 2012. But it isn’t! We only have data from Boston, and not any other city. We need to assume that the rows we have from Boston are representive of the other rows in the population from 2012.\nThat assumption seems pretty extreme! For example, the survey respondents were all suburban commuters on train platforms. There are numerous ways in which such a sample is unrepresentative of both Boston adults as a whole, much less all the adults in other cities.\n\n6.2.4 Unconfoundedness\nUnconfoundedness means that the treatment assignment is independent of the potential outcomes, when we condition on pre-treatment covariates. A model is confounded if this is not true.\nSince treatment was assigned at random, we can be fairly confident that treatment assignment is idependent of everything. This is the beauty of randomization and the key reason why randomized control trials are the gold standard in statistics for drawing conclusions about causal effects.\nPutting these concerns together, we should add at least one sentence about these concerns to the analysis summary:\n\nUsing data from a 2012 survey of Boston-area commuters, we seek to measure the causal effect of exposure to Spanish-speakers on attitudes toward immigration among adults in Chicago and similar cities in 2024. There is some concern that the relationship has changed since our data was collected.\n\nIs that the only reasonable concern? No! There are always numerous problems with any data analysis because our assumptions are never true. Honesty compels us to highlight at least one such problem anytime we present our results. Which one to highlight and how to do so are matters of professional judgment.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Three Parameters: Causal</span>"
    ]
  },
  {
    "objectID": "three-parameters-causal.html#courage",
    "href": "three-parameters-causal.html#courage",
    "title": "6  Three Parameters: Causal",
    "section": "\n6.3 Courage",
    "text": "6.3 Courage\n\n\n\n\nCourage\n\n\n\nJustice gave us the Population Table. Courage selects the data generating mechanism. We first specify the mathematical formula which connects the outcome variable we are interested in with the other data that we have. We explore different models. We need to decide which variables to include and to estimate the values of unknown parameters. We check our models for consistency with the data we have. We avoid hypothesis tests. We select one model.\nBecause the outcome variable is continuous, we use a standard regression model.\n\\[ y_i =  \\beta_0 + \\beta_1 x_i + \\epsilon_i \\]\nwith \\(\\epsilon_i \\sim N(0, \\sigma^2)\\). \\(y_i\\) is the attitude toward immigration for person \\(i\\) after being exposed to the treatment or control. \\(x_i\\) corresponds to the treatment status for person \\(i\\). As we saw in the EDA above, treatment is a factor variable with two levels: “Treatment” and “Control.” Since English words don’t work in formulas, brm() will automatically transform treatment into a 0/1 variable named treatmentControl which will take on the value 1 if person \\(i\\) received control and 0 if they received treatment.\n\n6.3.1 Models\nWe create our fitted model in the usual way.\n\nfit_gauss &lt;- brm(formula = att_end ~ treatment,\n             data = trains,\n             family = gaussian(),\n             silent = 2,\n             refresh = 0,\n             seed = 9)\n\nBecause the default value of family is guassian(), we could have left out this part of the function call and gotten the same result. We add the seed argument to ensure that the fitted object is identical whenever we (or you) run this code. Recall that there is randomness in the modeling fitting process. This rarely affects the substantive results. But it is often convenient to be able to replicate a given model exactly. Using seed accomplishes this goal. The number 9 is arbitrary, other than being my lucky number. As usual, we use silent = 2 and refresh = 0 to turn off all the annoying messages.\n\nfit_gauss\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: att_end ~ treatment \n   Data: trains (Number of observations: 115) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept           10.00      0.39     9.23    10.77 1.00     3576     2992\ntreatmentControl    -1.54      0.51    -2.55    -0.50 1.00     3593     2332\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.80      0.19     2.45     3.19 1.00     3008     2550\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe math for this model is exactly the same as the math for the predictive model in the first part of this chapter, although we change the notation a bit for clarity.\n\\[ attitude_i = \\beta_0 + \\beta_1 control_i + \\epsilon_i\\]\nwhere \\[control_i \\in \\{0,1\\}\\] \\[\\epsilon_i \\sim N(0, \\sigma^2)\\]\nNothing has changed, except for the meaning of the data items and the interpretations of the parameters. Let’s take a look at the parameters of the situation which plays a role alongside the mathematics to create the fitted model.\nOn the left-hand side we still have the outcome, \\(attitude_i\\), however in this case, this is a person’s attitude toward immigration after the experiment is complete. \\(attitude_i\\) takes on integer values between 3 and 15 inclusive. On the right-hand side, the part contained in the model will consist of the terms \\(\\beta_0\\) and \\(\\beta_1\\). For the most part, we don’t care about the values of parameters in our models. We care about the answers to our questions and the uncertainty associated therewith.\nBut in this simple model, the parameters also have a substantive interpretation. \\(\\beta_0\\) is also the average attitude toward immigration for treated individuals. \\(\\beta_1\\) is the difference between the attitude toward immigration for treated individuals — those exposed to Spanish-speakers — and the average attitude toward immigration for control individuals — those not exposed to Spanish-speakers. These are both our parameters. \\(control_i\\) is our explanatory variable and takes the value 0 or 1. In other words, this is a binary variable.The last part, \\(\\epsilon\\) (“epsilon”), represents the part that is not explained by our model and is called the error term. It is simply the difference between the outcome and our model predictions. In our particular case, this includes all factors that have an influence on someone’s attitude toward immigration but are not accounted for by treatment status. We assume that this error follows a normal distribution with an expected value of 0.\n\nNote that the formula applies to everyone in the population, not just the 115 people for whom we have data. This is what the stability assumption buys us. The index \\(i\\) does not just go from 1 through 115. It goes from 1 through \\(N\\), where \\(N\\) is the number of individuals in the population. Conceptually, everyone has an att_end under both treatment and under control, even we can only ever observe one of these.\nThe small \\(i\\)’s are an index for the data set. It is equivalent to the “ID” column in our Preceptor Table and simply states that the outcome for person \\(i\\) is explained by the predictor variables (\\(treatment_i\\) and \\(control_i\\)) for person \\(i\\), along with an error term.\n\n6.3.2 Testing\n\npp_check(fit_gauss)\n\n\n\n\n\n\n\nOur graph in the darker blue represents the actual data. As we can see with the lighter blue graph, our fitted model is able to generate a distribution of the data that is similar when compared to the actual data. However, the “fake-data” produces some values for att_end which are impossible. We know from the survey details that the lowest possible value for att_end is 3 and the highest is 15. But, with our “fake-data” simulations, we see several values that are less than 3 and great than 15. Consider:\n\npp_check(fit_gauss, type = \"hist\", bins = 12)\n\n\n\n\n\n\n\nAgain, we see the problem. The true data is always within the range of 3 to 15. Our simulated (or “fake”) data sometimes strays outside this range, even if not very far and not very often. Is it a serious flaw? That is tough to decide.\nLet’s explore the issue by first changing treat_end from a continuous to an integer variable, which is probably more correct, and then re-estimate the model. Besides changing att_end to an integer, we also change the family to poisson() since this is the recommended model for dealing with “count” data\n\ntrains2 &lt;- trains |&gt; \n  mutate(att_end = as.integer(att_end))\n\nfit_poisson &lt;- brm(formula = att_end ~ treatment,\n                   data = trains2,\n                   family = poisson(),\n                   silent = 2,\n                   refresh = 0,\n                   seed = 9)\n\nBecause the poisson mode has a very different structure, the parameter estimates do not have a simple interpretation.\n\nfit_poisson\n\n Family: poisson \n  Links: mu = log \nFormula: att_end ~ treatment \n   Data: trains2 (Number of observations: 115) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            2.30      0.04     2.21     2.39 1.00     4291     3127\ntreatmentControl    -0.17      0.06    -0.30    -0.04 1.00     3722     2343\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe posterior predictive check looks better:\n\nShow the codepp_check(fit_poisson, type = \"hist\", bins = 19)\n\nUsing 10 posterior draws for ppc type 'hist' by default.\n\n\n\n\n\n\n\n\nThe check is better but not perfect. We still have predicted values that are below three or above 15, even though that is impossible. A common approach at this point would be to use the negative binomial family. We save that exercise for another day.\nThe purpose of the virtue of Courage is to create a data generating mechanism which is consistent with our understanding of how the world works and not-contradicted by he data which we have. Although there are some problems with fit_gauss, it is good enough for our purposes. So, the DGM looks like:\n\\[ attitude_i = 10 - 1.5  * control_i + \\epsilon_i\\]\nwhere \\[control_i \\in \\{0,1\\}\\] and \\[\\epsilon_i \\sim N(0, 2.8^2)\\].\n\n6.3.3 Data Generating Mechanism\nRecall the Preceptor Table and the units from the beginning. We are concerned with three main ideas from the Preceptor Table: our units (individuals at the train station), our outcome (individual’s attitude toward immigration) covariates (the treatment that each individual received).\nWith the help of the data generating mechanism that we have created from our fitted model we can fill in the missing values to the Preceptor Tables. We use the data generating mechanism as the last step of Courage as it allows us to analyze and view our fitted model’s data.\nAlthough we can review the posteriors of the parameters by looking at fit_gauss, it can bse useful to present the numbers in a formatted table.\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n\n95% CI1\n\n\n\n\n(Intercept)\n10\n9.2, 11\n\n\ntreatment\n\n\n\n\n    Treated\n—\n—\n\n\n    Control\n-1.5\n-2.5, -0.50\n\n\n\n\n1 CI = Credible Interval\n\n\n\n\n\n\n(Intercept) corresponds to 10.0 which is \\(\\beta_0\\). As always, R has, behind the scenes, estimated the entire posterior probability distribution for \\(\\beta_0\\). We are 95% confident that the true, but unknowable, average attitude toward immigration among the Treated in the population to be between 9.2 and 11. The posterior of \\(b_1\\), which is the coefficient of \\(control_i\\), is centered around -1.5 with a 95% confidence interval between -2.5 and -0,5.\nUp until now, we have used the Bayesian interpretation of “confidence interval.” This is also the intuitive meaning which, outside of academia, is almost universal. There is a truth out there. We don’t know, and sometimes can’t know, the truth. A confidence interval, and its associated confidence level, tells us how likely the truth is to lie within a specific range. If your boss asks you for a confidence interval, she almost certainly is using this interpretation.\nBut, in contemporary academic research, the phrase “confidence interval” is usually given a “Frequentist” interpretation. (The biggest divide in statistics is between Bayesians and Frequentists. The Frequentist approach, also known as “Classical” statistics, has been dominant for 100 years. Its power is fading, which is why this textbook is Bayesian.) For a Frequentist, a 95% confidence interval means that, if we were to apply the procedure we used in an infinite number of future situations like this, we would expect the true value to fall within the calculated confidence intervals 95% of the time. In academia, a distinction is sometimes made between confidence intervals (which use the Frequentist interpretation) and credible intervals (which use the Bayesian interpretation). We won’t worry about that difference in this Primer.\nWe can now add a sentence which describes our model to our summary of the entire project.\n\nUsing data from a 2012 survey of Boston-area commuters, we seek to measure the causal effect of exposure to Spanish-speakers on attitudes toward immigration among adults in Chicago and similar cities in 2024. There is some concern that the relationship has changed since our data was collected. We modeled att_end, a summary measure of attitude toward immigration measured on a 3 to 15 integer scale, as a linear function of treatment.\n\nObviously, a single extra sentence can only tell the reader so much. The important thing to note is that we are showing you a recipe for doing data science and for communicating your results. That communication will always include at least one sentence about the form of your model, the outcome variable and the covariates and/or treatments.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Three Parameters: Causal</span>"
    ]
  },
  {
    "objectID": "three-parameters-causal.html#temperance",
    "href": "three-parameters-causal.html#temperance",
    "title": "6  Three Parameters: Causal",
    "section": "\n6.4 Temperance",
    "text": "6.4 Temperance\n\n\n\n\nTemperance\n\n\n\nCourage gave us the data generating mechanism. With Temperance we can create posteriors of the quantities of interest. We should be modest in the claims we make.\n\n6.4.1 Questions and Answers\nRecall the original question:\n\nWhat is the average treatment effect, of exposing people to Spanish-speakers, on their attitudes toward immigration?\n\nWe have already clarified that the question refers to residents of Chicago (and similar cities) in 2024.\nUse add_epred_draws() as usual. Creating the value for the newdata argument is tricky since we need to construct the treatment variable by hand. Even though treatment is a factor variable within trains, we can get away with using a simple character variable since either version would be transformed into a 0/1 variable by add_epred_draws().\n\nfit_gauss |&gt; \n  add_epred_draws(\n    newdata = tibble(treatment = c(\"Treated\", \"Control\")))\n\n# A tibble: 8,000 × 6\n# Groups:   treatment, .row [2]\n   treatment  .row .chain .iteration .draw .epred\n   &lt;chr&gt;     &lt;int&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;\n 1 Treated       1     NA         NA     1   9.92\n 2 Treated       1     NA         NA     2   9.71\n 3 Treated       1     NA         NA     3   9.50\n 4 Treated       1     NA         NA     4  10.7 \n 5 Treated       1     NA         NA     5  10.3 \n 6 Treated       1     NA         NA     6  10.0 \n 7 Treated       1     NA         NA     7  10.2 \n 8 Treated       1     NA         NA     8  10.2 \n 9 Treated       1     NA         NA     9  10.3 \n10 Treated       1     NA         NA    10  10.6 \n# ℹ 7,990 more rows\n\n\nWe have 8,000 rows. The first four thousand rows are draws from the first row of newdata, meaning treatment equals \"Treated\". The next four thousand rows are draws for treatment = \"Control\". Mapping this tibble is easy because of its tidy structure.\n\nfit_gauss |&gt; \n  add_epred_draws(\n    newdata = tibble(treatment = c(\"Treated\", \"Control\"))) |&gt; \n  ggplot(aes(x = .epred, fill = treatment)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for Immigration Attitude Post Experiment\",\n         subtitle = \"Exposure to Spanish-speakers shifts immigration attitudes rightward\",\n         x = \"Attitude Toward Immigration\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\n\n\n\n\n\n\n\nJust eyeballing the graph, we can see that the posterior att_end for Control is centered around 8.5. For Treated, the center is around 10. Since the causal effect is the difference between two outcomes, we can simply subtract and estimate that the causal effect is about \\(10 - 8.5 = 1.5\\).\nEven better, we can do the calculation on the individual rows of the posterior draws. But, first, we need to “widen” the tibble.\n\nfit_gauss |&gt; \n  add_epred_draws(\n    newdata = tibble(treatment = c(\"Treated\", \"Control\"))) |&gt; \n  pivot_wider(id_cols = .draw, names_from = treatment, values_from = .epred)\n\n# A tibble: 4,000 × 3\n   .draw Treated Control\n   &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1     1    9.92    8.10\n 2     2    9.71    8.71\n 3     3    9.50    8.76\n 4     4   10.7     8.41\n 5     5   10.3     8.32\n 6     6   10.0     7.75\n 7     7   10.2     7.89\n 8     8   10.2     7.99\n 9     9   10.3     8.33\n10    10   10.6     8.13\n# ℹ 3,990 more rows\n\n\nThe resulting table has only 4,000 rows because we have put the posteriors for Treated and Control next to each other. Recall that we can perform algebra on posteriors just as we do with numbers. That is, the easiest way to calculate the causal effect is to subtract the posterior for the outcome under Control from the posterior of the outcome under Treatment.\n\nfit_gauss |&gt; \n  add_epred_draws(\n    newdata = tibble(treatment = c(\"Treated\", \"Control\"))) |&gt; \n  pivot_wider(id_cols = .draw, names_from = treatment, values_from = .epred) |&gt; \n  mutate(causal_effect = Treated - Control) |&gt; \n  ggplot(aes(x = causal_effect)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for Average Treatment Effect\",\n         subtitle = \"Exposure to Spanish-speakers shifts immigration attitudes rightward\",\n         x = \"Difference in Attitude\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\n\n\n\n\n\n\n\nThe posterior of the average causal effect is centered around 1.5, with a 95% confidence from about 0.5 to 2.5. How big an effect is 1.5? The difference between Republican and Democratic attitudes toward immigration, before the experiment, was about 2.3.\n\ntrains |&gt; \n  summarize(mean(att_start), .by = party)\n\n# A tibble: 2 × 2\n  party      `mean(att_start)`\n  &lt;chr&gt;                  &lt;dbl&gt;\n1 Democrat                8.81\n2 Republican             11.1 \n\n\nSo the causal effect of hearing Spanish-speakers is about the same as 2/3s of the distance between Republicans and Democrats. This is not a small effect! To summarize our analysis:\n\nUsing data from a 2012 survey of Boston-area commuters, we seek to measure the causal effect of exposure to Spanish-speakers on attitudes toward immigration among adults in Chicago and similar cities in 2024. There is some concern that the relationship has changed since our data was collected. We modeled att_end, a summary measure of attitude toward immigration measured on a 3 to 15 integer scale, as a linear function of treatment. The average causal effect of treatment was about 1.5, with a 95% confidence interval of 0.5 to 2.5. Hearing Spanish-speakers made people more conservative with regard to immigration. That 1.5 effect is about 2/3s of the distance between the average attitudes toward immigration of Republicans versus Democrats.\n\n\n6.4.2 Humility\nWe can never know the truth.\nWe need to maintain humility when we are making our inferences and decisions. Stay cautious my friends.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Three Parameters: Causal</span>"
    ]
  },
  {
    "objectID": "three-parameters-causal.html#summary",
    "href": "three-parameters-causal.html#summary",
    "title": "6  Three Parameters: Causal",
    "section": "\n6.5 Summary",
    "text": "6.5 Summary\n\n\n\n\nEnos, Ryan D. 2014. “Causal Effect of Intergroup Contact on Exclusionary Attitudes.” Proceedings of the National Academy of Sciences 111 (10): 3699–3704. https://doi.org/10.1073/pnas.1317670111.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Three Parameters: Causal</span>"
    ]
  },
  {
    "objectID": "mechanics.html",
    "href": "mechanics.html",
    "title": "7  Mechanics",
    "section": "",
    "text": "7.1 Transforming variables\nIt is often convenient to transform a predictor variable so that our model makes more sense.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Mechanics</span>"
    ]
  },
  {
    "objectID": "mechanics.html#transforming-variables",
    "href": "mechanics.html#transforming-variables",
    "title": "7  Mechanics",
    "section": "",
    "text": "7.1.1 Centering\nConsider a model of income as a function of age.\n\\[ income_i = \\beta_0  + \\beta_1 age_i + \\epsilon_i\\]\n\nWe fit this using the trains data from primer.data. We will also be using a new package, broom.mixed, which allows us to tidy regression data for plotting.\n\nlibrary(tidyverse)\nlibrary(primer.data)\nlibrary(brms)\nlibrary(broom.mixed)\n\n\n\nfit_1 &lt;- brm(formula = income ~ age, \n             data = trains,\n             family = gaussian(),\n             silent = 2,\n             refresh = 0,\n             seed = 45)\n\nfixef(fit_1)\n\n             Estimate  Est.Error       Q2.5      Q97.5\nIntercept 103852.8995 25270.8777 54103.8562 152150.544\nage          894.2228   566.5796  -201.3167   2003.449\n\n\nThere is nothing wrong with this model. Yet the interpretation of \\(\\beta_0\\), the intercept in the regression, is awkward. It represents the average income for people of age zero. That is useless! There are no people of zero age in our data. And, even if there were, it would be weird to think about such people taking the commuter train into Boston and filling out our survey forms.\nIt is easy, however, to transform age into a variable which makes the intercept more meaningful. Consider a new variable, c_age, which is age minus the average age in the sample. Using this centered version of age does not change the predictions or residuals in the model, but it does make the intercept easier to interpret.\n\ntrains_2 &lt;- trains |&gt; \n  mutate(c_age = age - mean(age))\n\nfit_1_c &lt;- brm(formula = income ~ c_age, \n               data = trains_2,\n               family = gaussian(),\n               silent = 2,\n               refresh = 0,\n               seed = 16)\n\nfixef(fit_1_c)\n\n             Estimate Est.Error        Q2.5      Q97.5\nIntercept 141889.6888 6910.7931 128532.2391 155190.379\nc_age        909.3219  568.1943   -250.0184   1972.618\n\n\nThe intercept, 141,890, is the expected income for someone with c_age = 0, i.e., someone of an average age in the data, which is around 42.\n\n7.1.2 Scaling\nCentering — changing a variable via addition/subtraction — often makes the intercept easier to interpret. Scaling — changing a variable via multiplication/division — often makes it easier to interpret coefficients. The most common scaling method is to divide the variable by its standard deviation.\n\ntrains_3 &lt;- trains |&gt; \n  mutate(s_age = age / sd(age))\n\nfit_1_s &lt;- brm(formula = income ~ s_age, \n               data = trains_3,\n               family = gaussian(),\n               silent = 2,\n               refresh = 0,\n               seed = 16)\n\nfixef(fit_1_s)\n\n           Estimate Est.Error      Q2.5     Q97.5\nIntercept 103506.67 25525.634 54271.954 153952.84\ns_age      10949.15  7002.374 -2828.438  24754.34\n\n\ns_age is age scaled by its own standard deviation. A change in one unit of s_age is the same as a change in one standard deviation of the age, which is about 12. The interpretation of \\(\\beta_1\\) is now:\nWhen comparing two people, one about 1 standard deviation worth of years older than the other, we expect the older person to earn about 11,000 dollars more.\nBut, because we scaled without centering, the intercept is now back to the (nonsensical) meaning of the expected income for people of age 0.\n\n7.1.3 z-scores\nThe most common transformation applies both centering and scaling. The base R function scale() subtracts the mean and divides by the standard deviation. A variable so transformed is a “z-score,” meaning a variable with a mean of zero and a standard deviation of one. Using z-scores makes interpretation easier, especially when we seek to compare the importance of different predictors.\n\ntrains_4 &lt;- trains |&gt; \n  mutate(z_age = scale(age))\n\n\nfit_1_z &lt;- brm(formula = income ~ z_age, \n               data = trains_4,\n               family = gaussian(),\n               silent = 2,\n               refresh = 0,\n               seed = 16)\n\nfixef(fit_1_z)\n\n           Estimate Est.Error       Q2.5     Q97.5\nIntercept 141649.82  6968.046 127466.909 154700.35\nz_age      10998.74  7093.726  -2920.582  25277.06\n\n\nThe two parameters are easy to interpret after this transformation.\nThe expected income of someone of average age, which is about 42 in this study, is about 142,000 dollars.\nWhen comparing two individuals who differ in age by one standard deviation, which is about 12 years in this study, the older person is expected to earn about 11,000 dollars more than the younger.\nNote that, when using z-scores, we would often phrase this comparison in terms of “sigmas.” One person is “one sigma” older than another person means that they are one standard deviation older. This is simple enough, once you get used to it, but also confusing since we are already using the word “sigma” to mean \\(\\sigma\\), the standard deviation of \\(\\epsilon_i\\). Alas, language is something we deal with rather than something we control. You will hear the same word “sigma” applied to both concepts, even in the same sentence. Determine meaning by context.\n\n7.1.4 Taking logs\nIt is often helpful to take the log of predictor variables, especially in cases in which their distribution is skewed. You should generally only take the log of variables for which all the values are strictly positive. The log of a negative number is not defined. Consider the number of registered voters (rv13) at each of the polling stations in kenya.\n\nShow the codex &lt;- kenya |&gt; \n  filter(rv13 &gt; 0)\n\nrv_p &lt;- x |&gt; \n  ggplot(aes(rv13)) + \n    geom_histogram(bins = 100) +\n    labs(x = \"Registered Voters\",\n         y = NULL) \n\nlog_rv_p &lt;- x |&gt; \n  ggplot(aes(log(rv13))) + \n    geom_histogram(bins = 100) +\n    labs(x = \"Log of Registered Voters\",\n         y = NULL) +\n    expand_limits(y = c(0, 175))\n\nrv_p + log_rv_p +\n  plot_annotation(title = 'Registered Votes In Kenya Communities',\n                  subtitle = \"Taking logs helps us deal with outliers\")\n\n\n\n\n\n\n\nMost experienced data scientists would use the log of rv13 rather than the raw value. Comments:\n\nWe do not know the “true” model. Who is to say that a model using the raw value is right or wrong?\nCheck whether or not this choice meaningfully affects the answer to your question. Much of the time, it won’t. That is, our inferences are often fairly “robust” to small changes in the model. If you get the same answer with rv13 as from log_rv13, then no one cares which you use.\nFollow the conventions in your field. If everyone does X, then you should probably do X, unless you have a good reason not to. If you do have such a reason, explain it prominently.\nMost professionals, when presented with data distributed like rv13, would take the log. Professionals (irrationally?) hate outliers. Any transformation which makes a distribution look more normal is generally considered a good idea.\n\nMany of these suggestions apply to every aspect of the modeling process.\n\n\n7.1.5 Adding transformed terms\nInstead of simply transforming variables, we can add more terms which are transformed versions of a variable. Consider the relation of height to age in nhanes. Let’s start by dropping the missing values.\n\nno_na_nhanes &lt;- nhanes |&gt; \n  select(height, age) |&gt; \n  drop_na() \n\nFit and plot a simple linear model:\n\nnhanes_1 &lt;- brm(height ~ age,\n                data = no_na_nhanes,\n                family = gaussian(),\n                silent = 2,\n                refresh = 0,\n                seed = 16)\n\n\nno_na_nhanes |&gt; \n  ggplot(aes(x = age, y = height)) +\n    geom_point(alpha = 0.1) +\n    geom_line(aes(y = fitted(nhanes_1)[, \"Estimate\"]),\n             color = \"red\",\n             linewidth = 2) +\n    labs(title = \"Age and Height\",\n         subtitle = \"Children are shorter, but a linear fit is poor\",\n         x = \"Age\",\n         y = \"Height (cm)\",\n         caption = \"Data source: NHANES\")\n\n\n\n\n\n\n\nThat is not a very good model, obviously.\nAdding a quadratic term makes it better. (Note the need for I() in creating the squared term within the formula argument.)\n\nnhanes_2 &lt;- brm(height ~ age + I(age^2),\n                data = no_na_nhanes,\n                family = gaussian(),\n                silent = 2,\n                refresh = 0,\n                seed = 27)\n\nno_na_nhanes |&gt; \n  ggplot(aes(x = age, y = height)) +\n    geom_point(alpha = 0.1) +\n    geom_line(aes(y = fitted(nhanes_2)[, \"Estimate\"]), \n              color = \"red\") +\n    labs(title = \"Age and Height\",\n         subtitle = \"Quadratic fit is much better, but still poor\",\n         x = \"Age\",\n         y = \"Height (cm)\",\n         caption = \"Data source: NHANES\")\n\n\n\n\n\n\n\nStill, we have not made use of our background knowledge in creating these variables. We know that people don’t get any taller after age 18 or so. Let’s create variables which capture that break.\n\nnhanes_3 &lt;- brm(height ~ I(ifelse(age &gt; 18, 18, age)),\n                data = no_na_nhanes,\n                family = gaussian(),\n                silent = 2,\n                refresh = 0,\n                seed = 87)\n\n\nno_na_nhanes |&gt; \n  ggplot(aes(x = age, y = height)) +\n    geom_point(alpha = 0.1) +\n    geom_line(aes(y = fitted(nhanes_3)[, \"Estimate\"]), \n              color = \"red\") +\n    labs(title = \"Age and Height\",\n         subtitle = \"Domain knowledge makes for better models\",\n         x = \"Age\",\n         y = \"Height (cm)\",\n         caption = \"Data source: NHANES\")\n\n\n\n\n\n\n\nThe point is that we should not take the variables we receive as given. We are the captains of our souls. We transform variables as needed.\n\n7.1.6 Transforming the outcome variable\nTransforming predictor variables is uncontroversial. It does not matter much. Change most continuous predictor variables to \\(z\\)-scores and you won’t go far wrong. Or keep them in their original form, and take care with your interpretations. It’s all good.\nTransforming the outcome variable is a much more difficult question. Imagine that we seek to create a model which explains rv13 from the kenya tibble. Should we transform it?\n\nMaybe? There are no right answers. A model with rv13 as the outcome variable is different from a model with log(rv13) as the outcome. The two are not directly comparable.\nMuch of the same advice with regard to taking logs of predictor variables applies here as well.\n\nSee Gelman, Hill, and Vehtari (2020) for more useful discussion.\n\n7.1.7 Interpreting coefficients\n\nWhen we interpret coefficients, it is important to know the difference between across unit and within unit comparisons. When we compare across unit, meaning comparing Joe and George, we are not looking at a causal relationship. Within unit discussions, where we are comparing Joe under treatment versus Joe under control, are causal. This means that within unit interpretation is only possible in causal models, where we are studying one unit under two conditions. When we talk about two potential outcomes, we are discussing the same person or unit under two conditions.\nTo put this in terms of the Preceptor tables, a within unit comparison is looking at one row of data: the data for Joe under control and the data for Joe under treatment. We are comparing one unit, or (in this case) one person, to itself under two conditions. An across unit comparison is looking at multiple rows of data, with a focus on differences across columns. We are looking at differences without making any causal claims. We are predicting, but we are not implying causation.\nThe magnitude of the coefficients in linear models are relatively easy to understand. That is not true for logistic regressions. In that case, use the Divide-by-Four rule: Take a logistic regression coefficient (other than the constant term) and divide it by 4 to get an upper bound on the predictive difference corresponding to a unit difference in that variable. All this means is that, when evaluating if a predictor is helpful in a logistic regression only, divide the coefficient by four.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Mechanics</span>"
    ]
  },
  {
    "objectID": "mechanics.html#selecting-variables",
    "href": "mechanics.html#selecting-variables",
    "title": "7  Mechanics",
    "section": "\n7.2 Selecting variables",
    "text": "7.2 Selecting variables\nHow do we decide which variables to include in a model? There is no one right answer to this question.\n\n7.2.1 General guidelines for selecting variables\nWhen deciding which variables to keep or discard in our models, our advice is to keep a variable X if any of the following circumstances apply:\n\nThe variable has a large and well-estimated coefficient. This means, roughly, that the 95% confidence interval excludes zero. “Large” can only be defined in the context of the specific model. Speaking roughly, removing a variable with a large coefficient meaningfully changes the predictions of the model.\nUnderlying theory/observation suggests that X has a meaningfully connection to the outcome variable.\nIf the variable has a small standard error relative to the size of the coefficient, it is general practice to include it in our model to improve predictions. The rule of thumb is to keep variables for which the estimated coefficient is more than two standard errors away from zero. Some of these variables won’t “matter” much to the model. That is, their coefficients, although well-estimated, are small enough that removing the variable from the model will not affect the model’s predictions very much.\nIf the standard error is large relative to the coefficient, i.e., if the magnitude of the coefficient is more than two standard errors from zero, and we find no other reason to include it in our model, we should probably remove the variable from your model.\n\n\nThe exception to this rule is if the variable is relevant to answering a question which we have. For example, if we want to know if the ending attitude toward immigration differs between men and women, we need to include gender in the model, even if its coefficient is small and closer to zero than two standard errors.\nIt is standard in your field to include X in such regressions.\nYour boss/client/reviewer/supervisor wants to include X.\n\nLet’s use the trains data set to evaluate how helpful certain variables are to creating an effective model, based on the guidelines above.\n\n7.2.2 Variables in the trains data set\nTo look at our recommendations in practice, let’s focus on the trains data set. The variables in trains include gender, liberal, party, age, income, att_start, treatment, and att_end. Which variables would be best to include in a model?\n\n7.2.3 att_end ~ treatment + att_start\nFirst, let’s look at a model with a left hand variable, att_end, and two right side variables, treatment and att_start.\n\nfit_1_model &lt;- brm(att_end ~ treatment + att_start,\n                   data = trains,\n                   family = gaussian(),\n                   silent = 2,\n                   refresh = 0,\n                   seed = 16)\n\nfixef(fit_1_model)\n\n                   Estimate Est.Error       Q2.5      Q97.5\nIntercept         2.3387046 0.4284036  1.4809571  3.1766254\ntreatmentControl -0.9513246 0.2478740 -1.4390199 -0.4534002\natt_start         0.7975082 0.0399413  0.7205094  0.8773148\n\n\nHow do we decide which variables are useful? First, let’s interpret our coefficients.\n\nThe variable before the tilde, att_end, is our outcome.\nThe explanatory variables are treatment, which says whether a commuter relieved treatment or control conditions, and att_start, which measures attitude at the start of the study.\nThe 95% confidence interval for att_end is equal to the coefficient– 2.34– plus or minus two standard errors. This shows the estimate for the att_end where the commuters were under treatment and were not in the control group.\nThe variable treatmentControl represents the offset in att_end from the estimate for our Intercept. This offset is for the group of people that were in the Control group. To find the estimated att_end for those in this group, you must add the median for treatmentControl to the Intercept.\nThe variable att_start measures the expected difference in att_end for every one unit increase in att_start.\n\nThe causal effect of the variable treatmentControl is -0.95. This means that, compared with the predicted att_end for groups under treatment, those in the control group have a predicted attitude that is almost one entire point lower. As we can see, this is a large and well estimated coefficient. Recall that this means, roughly, that the 95% confidence interval excludes zero. To calculate the 95% confidence interval, we take the coefficient plus or minus two standard errors. As we can see, the 95% confidence interval does exclude zero, suggesting that treatment is a worthy variable.\nIn addition to being meaningful, which is enough to justify inclusion in our model, this variable satisfies a number of other qualifications: - The variable has a small standard error. - The variable is considered an indicator variable, which separates two groups of significance (treatment and control) that we would like to study.\nThe variable att_start, with a coefficient of 0.8, is also meaningful.\nConclusion: keep both variables! treatment and att_start are both significant, as well as satisfying other requirements in our guidelines. They are more than worthy of inclusion in our model.\n\n7.2.4 income ~ age + liberal\nNow, we will look at income as a function of age and liberal, a proxy for political party.\n\nfit_2_model &lt;- brm(income ~ age + liberal,\n                   data = trains,\n                   family = gaussian(),\n                   silent = 2,\n                   refresh = 0,\n                   seed = 16)\n\nfixef(fit_2_model)\n\n              Estimate Est.Error         Q2.5      Q97.5\nIntercept   110315.102 24676.374  63166.27383 158173.664\nage           1080.188   566.713    -22.75402   2177.090\nliberalTRUE -32434.045 13909.045 -59756.44978  -5266.503\n\n\nGreat! We have an estimate for income of those who fall into the category of liberalFALSE, as well as data on our right hand side variables of age and liberalTRUE.\nFirst, let’s interpret our coefficients.\n\nThe variable before the tilde, income, is our outcome.\nThe explanatory variables are liberal, a logical value of TRUE or FALSE, and age, a numeric variable.\nThe Intercept is estimating income where liberal == FALSE. Therefore, it is the estimated income for commuters that are not liberals and who have age = 0. The estimate for age is showing the increase in income with every additional year of age.\nThe estimate for liberalTRUE represents the offset in predicted income for commuters who are liberal. To find the estimate, we must add the coefficient to our (Intercept) value. We see that, on average, liberal commuters make less money.\n\nIt is important to note that we are not looking at a causal relationship for either of these explanatory variables. We are noting the differences between two groups, without considering causality. This is known as an across unit comparison. When we compare across unit we are not looking at a causal relationship.\nWhen comparing liberalTRUE with our (Intercept), recall that the Intercept is calculating income for the case where liberal == FALSE. As we can see, then, the coefficient for liberalTRUE, -32,434, shows that the liberals in our data set make less on average than non-liberals. The coefficient is large relative to the Intercept and, with rough mental math, we see that the 95% confidence interval excludes 0. Therefore, liberal is a helpful variable.\nThe variable age, however, does not appear to have a meaningful impact on our Intercept. The coefficient of age is low and the 95% confidence interval does not exclude 0.\nConclusion: definitely keep liberal! age is less clear. It is really a matter of preference at this point.\n\n\n7.2.5 Final thoughts\nNow that we have looked at three cases of variables and decided whether they should be included, let’s discuss the concept of selecting variables generally.\nThe variables we decided to keep and discard are not necessarily the variables you would keep or discard, or the variables that any other data scientist would keep or discard. It is much easier to keep a variable than it is to build a case for discarding a variable. Variables are helpful even when not significant, particularly when they are indicator variables which may separate the data into two groups that we want to study.\nThe process of selecting variables – though we have guidelines – is complicated. There are many reasons to include a variable in a model. The main reasons to exclude a variable are if the variable isn’t significant or if the variable has a large standard error.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Mechanics</span>"
    ]
  },
  {
    "objectID": "mechanics.html#comparing-models-in-theory",
    "href": "mechanics.html#comparing-models-in-theory",
    "title": "7  Mechanics",
    "section": "\n7.3 Comparing models in theory",
    "text": "7.3 Comparing models in theory\nDeciding which variables to include in a model is a subset of the larger question: How do we decide which model, out of the set of possible models, to choose?\nConsider two models which explain attitudes to immigration among Boston commuters.\n\nfit_liberal &lt;- brm(att_end ~ liberal,\n                   data = trains,\n                   family = gaussian(),\n                   silent = 2,\n                   refresh = 0,\n                   seed = 12)\n       \nfixef(fit_liberal)\n\n             Estimate Est.Error     Q2.5      Q97.5\nIntercept   10.024922  0.345711  9.32136 10.6974615\nliberalTRUE -2.001268  0.523003 -3.02537 -0.9764544\n\n\n\nfit_att_start &lt;- brm(att_end ~ att_start,\n                     data = trains,\n                     family = gaussian(),\n                     silent = 2,\n                     refresh = 0,\n                     seed = 37)\n\nfixef(fit_att_start)\n\n           Estimate  Est.Error      Q2.5     Q97.5\nIntercept 1.6546839 0.41144428 0.8643464 2.4591489\natt_start 0.8146102 0.04254966 0.7299836 0.8971717\n\n\nThey both seem like good models! The results make sense. People who are liberal have more liberal attitudes about immigration, so we would expect their att_end scores to be lower. We would also expect people to provide similar answers in two surveys administered a week or two apart. It makes sense that those with higher (more conservative) values for att_start would also have higher values for att_end.\nHow do we choose between these models?\n\n7.3.1 Better models make better predictions\nThe most obvious criteria for comparing models is the accuracy of the predictions. For example, consider the use of liberal to predict att_end.\n\nShow the codetrains |&gt; \n  mutate(pred_liberal = fitted(fit_liberal)[,\"Estimate\"]) |&gt; \n  ggplot(aes(x = pred_liberal, y = att_end)) +\n    geom_jitter(width = 0.05, height = 0.2, alpha = 0.5) +\n    \n    # Add a red circle where our predictions are most accurate (where the x and y\n    # values are the same, which is where our predictions = the true attitudes).\n    # pch = 1 makes the inside of the point translucent to show the number of\n    # correct predictions.\n  \n    annotate(\"point\", x = 8, y = 8, size = 20, pch = 1, color = \"red\") +\n    annotate(\"point\", x = 10, y = 10, size = 20, pch = 1, color = \"red\") +\n  \n    labs(title = \"Modeling Attitude Toward Immigration\",\n         subtitle = \"Liberals are less conservative\",\n         x = \"Predicted Attitude\",\n         y = \"True Attitude\")\n\n\n\n\n\n\n\n\nBecause there are only two possible values for liberal — TRUE and FALSE — there are only two predictions which this model will make: about 10 for liberal == FALSE and about 8 for liberal == TRUE. (The points in the above plot are jittered.) For some individuals, these are perfect predictions. For others, they are poor predictions. the red circles on our plot illustrate the areas where our predictions are equal to true values. As we can see, the model isn’t great at predicting attitude end. (Note the two individuals who are liberal == TRUE, and who the model thinks will have att_end == 8, but who have att_end == 15. The model got them both very, very wrong.)\nConsider our second model, using att_start to forecast att_end.\n\nShow the codetrains |&gt; \n  mutate(pred_liberal = fitted(fit_att_start)[,\"Estimate\"]) |&gt; \n  ggplot(aes(x = pred_liberal, y = att_end)) +\n    geom_jitter(width = 0.05, height = 0.2, alpha = 0.5) +\n  \n  # Insert red line where our predictions = the truth using geom_abline with an\n  # intercept, slope, and color.\n  \n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n    labs(title = \"Modeling Attitude Toward Immigration\",\n         subtitle = \"Survey responses are somewhat consistent\",\n         x = \"Predicted Attitude\",\n         y = \"True Attitude\")\n\n\n\n\n\n\n\nBecause att_end takes on 13 unique values, the model makes 13 unique predictions. Some of those predictions are perfect! But others are very wrong. The red line shows the points where our predictions match the truth. Note the individual with a predicted att_end of around 9 but with an actual value of 15. That is a big miss!\nRather than looking at individual cases, we need to look at the errors for all the predictions. Fortunately, a prediction error is the same thing as a residual, which is easy enough to calculate.\n\ntrains |&gt; \n  select(att_end, att_start, liberal) |&gt; \n  mutate(pred_lib = fitted(fit_liberal)[,\"Estimate\"]) |&gt; \n  mutate(resid_lib = pred_lib - att_end) |&gt; \n  mutate(pred_as = fitted(fit_liberal)[,\"Estimate\"]) |&gt; \n  mutate(resid_as = pred_as - att_end)\n\n# A tibble: 115 × 7\n   att_end att_start liberal pred_lib resid_lib pred_as resid_as\n     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1      11        11 FALSE      10.0    -0.975    10.0   -0.975 \n 2      10         9 FALSE      10.0     0.0249   10.0    0.0249\n 3       5         3 TRUE        8.02    3.02      8.02   3.02  \n 4      11        11 FALSE      10.0    -0.975    10.0   -0.975 \n 5       5         8 TRUE        8.02    3.02      8.02   3.02  \n 6      13        13 FALSE      10.0    -2.98     10.0   -2.98  \n 7      13        13 FALSE      10.0    -2.98     10.0   -2.98  \n 8      11        10 FALSE      10.0    -0.975    10.0   -0.975 \n 9      12        12 FALSE      10.0    -1.98     10.0   -1.98  \n10      10         9 FALSE      10.0     0.0249   10.0    0.0249\n# ℹ 105 more rows\n\n\nLet’s look at the square root of the average squared error.\n\ntrains |&gt; \n  select(att_end, att_start, liberal) |&gt; \n  mutate(lib_err = (fitted(fit_liberal)[,\"Estimate\"] - att_end)^2) |&gt; \n  mutate(as_err = (fitted(fit_att_start)[,\"Estimate\"] - att_end)^2) |&gt; \n  summarize(lib_sigma = sqrt(mean(lib_err)),\n            as_sigma = sqrt(mean(as_err))) \n\n# A tibble: 1 × 2\n  lib_sigma as_sigma\n      &lt;dbl&gt;    &lt;dbl&gt;\n1      2.68     1.35\n\n\nThere are many different measures of the error which we might calculate. The squared difference is most common for historical reasons: it was the mathematically most tractable in the pre-computer age. Having calculated a squared difference for each observation, we can sum them or take their average or take the square root of their average. All produce the same relative ranking, but the last is most popular because it (more or less) corresponds to the estimated \\(\\sigma\\) for a linear model. Note how these measures are the same as the ones produced by the Bayesian models created above.\n\n\nSadly, it is not wise to simply select the model which fits the data best because doing so can be misleading. After all, you are cheating! You are using that very data to select your parameters and then, after using the data once, turning around and “checking” to see how well your model fits the data. It better fit! You used it to pick your parameters! This is the danger of overfitting.\n\n7.3.2 Beware overfitting\nOne of the biggest dangers in data science is overfitting, using a model with too many parameters which fits the data we have too well and, therefore, works poorly on data we have yet to see. Consider a simple example with 10 data points.\n\n\n\n\n\n\n\n\nWhat happens when we fit a model with one predictor?\n\n\n\n\n\n\n\n\nThat is a reasonable model. It does not fit the data particularly well, but we certainly believe that higher values of x are associated with higher values of y. A linear fit is not unreasonable.\n\nBut we can also use some of the lessons from above and try a quadratic fit by adding \\(x^2\\) as a predictor.\n\n\n\n\n\n\n\n\nIs this a better model? Maybe?\nBut why stop at adding \\(x^2\\) to the regression? Why not add \\(x^3\\), \\(x^4\\) and all the way to \\(x^9\\)? When we do so, the fit is much better.\n\nShow the codenine_pred &lt;- lm(y ~ poly(x, 9),\n                       data = ovrftng)\n\nnewdata &lt;- tibble(x = seq(1, 10, by = 0.01),\n                  y = predict(nine_pred, \n                              newdata = tibble(x = x)))\n\novrftng |&gt; \n  ggplot(aes(x, y)) +\n    geom_point() +\n    geom_line(data = newdata, \n              aes(x, y)) +\n    labs(title = \"`y` as a 9-Degree Polynomial Function of `x`\") +\n    scale_x_continuous(breaks = seq(2, 10, 2)) +\n    scale_y_continuous(breaks = seq(2, 10, 2)) \n\n\n\n\n\n\n\nIf the only criteria we cared about was how well the model predicts using the data on which the parameters were estimated, then a model with more parameters will always be better. But that is not what truly matters. What matters is how well the model works on data which was not used to create the model.\n\n\n7.3.3 Better models make better predictions on new data\nThe most sensible way to test a model is to use the model to make predictions and compare those predictions to new data. After fitting the model using stan_glm, we would use posterior_predict to obtain simulations representing the predictive distribution for new cases. For instance, if we were to predict how someone’s attitude changes toward immigration among Boston commuters based on political affiliation, we would want to go out and test our theories on new Boston commuters.\nWhen thinking of generalization to new data, it is important to consider what is relevant new data in the context of the modeling problem. Some models are used to predict the future and, in those cases, we can wait and eventually observe the future and check how good our model is for making predictions. Often models are used to obtain insight to some phenomenon without immediate plan for predictions. This is the case with our Boston commuters example. In such cases, we are also interested whether learned insights from on part of the data generalizes to other parts of the data. For example, if we know how political attitudes informed future immigration stances in Boston commuters, we may want to know if those same conclusions could generalize to train commuters in different locations.\nEven if we had detected clear problems with our predictions, this would not necessarily mean that there is anything wrong with the model as fit to the original data set. However, we would need to understand it further before generalizing to other commuters.\nOften, we would like to evaluate and compare models without waiting for new data. One can simply evaluate predictions on the observed data. But since these data have already been used to fit the model parameters, these predictions are optimistic.\nIn cross validation, part of the data is used to fit the model and the rest of the data—the hold-out set—is used as a proxy for future data. When there is no natural prediction task for future data, we can think of cross validation as a way to assess generalization from one part of the data to another part.\nIn any form of cross validation, the model is re-fit leaving out one part of the data and then the prediction for the held-out part is evaluated. In the next section, we will look at a type of cross validation called leave-one-out (LOO) cross validation.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Mechanics</span>"
    ]
  },
  {
    "objectID": "mechanics.html#comparing-models-in-practice",
    "href": "mechanics.html#comparing-models-in-practice",
    "title": "7  Mechanics",
    "section": "\n7.4 Comparing models in practice",
    "text": "7.4 Comparing models in practice\nTo compare models without waiting for new data, we evaluate predictions on the observed data. However, due to the fact that the data has been used to fit the model parameters, our predictions are often optimistic when assessing generalization.\nIn cross validation, part of the data is used to fit the model, while the rest of the data is used as a proxy for future data. We can think of cross validation as a way to assess generalization from one part of the data to another part. How do we do this?\n\nWe can hold out individual observations, called leave-one-out (LOO) cross validation; or groups of observations, called leave-one-group-out cross validation; or use past data to predict future observations, called leave-future-out cross validation. When we perform cross validation, the model is re-fit leaving out one part of the data and then the prediction for the held-out part is evaluated.\nFor our purposes, we will be performing cross validation using leave-one-out (LOO) cross validation.\n\n7.4.1 Cross validation using loo()\n\n\nTo compare models using leave-one-out (LOO) cross validation, one piece of data is excluded from our model. The model is then re-fit and makes a prediction for the missing piece of data. The difference between the predicted value and the real value is calculated. This process is repeated for every row of data in the data set.\nIn essence: One piece of data is excluded from our model, the model is re-fit, the model attempts to predict the value of the missing piece, we compare the true value to the predicted value, and we assess the accuracy of our model’s prediction. This process occurs for each piece of data, allowing us to assess the model’s accuracy in making predictions.\nTo perform leave-one-out(LOO) cross validation, we will be using the function loo() from an R package. This is how we will determine which model is superior for our purposes.\nFirst, we will refamiliarize ourselves with our first model, fit_liberal.\n\nShow the codefixef(fit_liberal)\n\n             Estimate Est.Error     Q2.5      Q97.5\nIntercept   10.024922  0.345711  9.32136 10.6974615\nliberalTRUE -2.001268  0.523003 -3.02537 -0.9764544\n\n\nNow, we will perform loo() on our model and look at the results.\n\nShow the codeloo_liberal &lt;- loo(fit_liberal)\n\nloo_liberal\n\n\nComputed from 4000 by 115 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -279.6  7.5\np_loo         3.0  0.5\nlooic       559.2 15.0\n------\nMCSE of elpd_loo is 0.0.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.8, 1.0]).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details.\n\n\nWhat does any of this mean?\n\n\nelpd_loo is the estimated log score along with a standard error representing uncertainty due to using only 115 data points.\n\np_loo is the estimated “effective number of parameters” in the model.\n\nlooic is the LOO information criterion, −2 elpd_loo, which we compute for comparability to deviance.\n\nFor our purposes, we mostly need to focus on elpd_loo. Let’s explain, in more depth, what this information means.\nBasically, when we run loo(), we are telling R to take a piece of data out of our data set, re-estimate all parameters, and then predict the value for the missing piece of data. The value for elpd_loo() is based off of how close our estimate was to the truth. Therefore, elpd_loo() values inform us of the effectiveness of our model in predicting data it has not seen before.\n\nThe higher our value for elpd_loo, the better our model performs. This means that, when comparing models, we want to select the model with the higher value for elpd_loo.\n\nLet’s turn our attention to our second model. To begin, let’s observe the qualities of fit_att_start once again.\n\nShow the codefixef(fit_att_start)\n\n           Estimate  Est.Error      Q2.5     Q97.5\nIntercept 1.6546839 0.41144428 0.8643464 2.4591489\natt_start 0.8146102 0.04254966 0.7299836 0.8971717\n\n\nGreat! Now, let’s perform loo() on this model.\n\nShow the codeloo_att_start &lt;- loo(fit_att_start) \n\nloo_att_start\n\n\nComputed from 4000 by 115 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -201.6 12.4\np_loo         4.1  1.7\nlooic       403.2 24.9\n------\nMCSE of elpd_loo is 0.1.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.8, 1.1]).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details.\n\n\nThe elpd_loo value for this model is -201.7. This is higher than the elpd_loo for att_liberal, implying that this model is superior. However, we can’t see our estimates together. Is there a simpler way to calculate which model is better?\nActually, yes! Using the function loo_compare(), we can compare the models directly.\n\n7.4.2 Comparing models using loo_compare()\n\nTo compare the two models directly, we can use the function loo_compare with our two loo objects created above. This will calculate the difference in elpd_loo() between our models for us, making our job easier:\n\nShow the codeloo_compare(loo_att_start, loo_liberal)\n\n              elpd_diff se_diff\nfit_att_start   0.0       0.0  \nfit_liberal   -78.0      11.9  \n\n\nThe value for elpd_diff is equal to the difference in elpd_loo between our two models. These_diff shows that the difference in standard error.\nTo interpret the results directly, it is important to note that the first row will be the superior model. The values of elpd_diff and att_start will be 0, as these columns show the offset in the estimates compared to the better model. To reiterate: when the better model is compared to itself, those values will be 0. The following rows show the offset in elpd and se values between the less effective model, fit_liberal, and the more effective model, fit_att_start.\nThe better model is clear: fit_att_start. Therefore, the attitude at the start of the trains study is more significant to predicting final attitude when compared with the variable liberal, which is an analog for political affiliation.\nAs we have seen, loo_compare is a shortcut for comparing two models. When you are deciding between two models, loo_compare() is a great way to simplify your decision.\nWhat do we do when the value of loo_compare() is small? As a general practice, differences smaller than four are hard to distinguish from noise. In other words: when elpd_diff is less than 4, there is no advantage to one model over the other.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Mechanics</span>"
    ]
  },
  {
    "objectID": "mechanics.html#testing-is-nonsense",
    "href": "mechanics.html#testing-is-nonsense",
    "title": "7  Mechanics",
    "section": "\n7.5 Testing is nonsense",
    "text": "7.5 Testing is nonsense\nAs always, it is important to look at the practices of other professionals and the reasons we may choose not to follow those tactics. For instance, our continued problem with hypothesis testing. In hypothesis testing, we assert a null hypothesis \\(H_0\\) about our data and an alternative hypothesis \\(H_a\\).\nWhen performing hypothesis testing, we either reject the hypothesis or we do not reject it. The qualifications for rejecting are met if the 95% confidence interval excludes the null hypothesis. If the hypothesis is included in our 95% confidence interval, we do not reject it. In the case of “insignificant” results, with p &gt; 0.5, we also can’t “reject” the null hypothesis. However, this does not mean that we accept it.\nThe premise of hypothesis testing is to answer a specific question – one that may not even be particularly relevant to our understanding of the world – about our data. So, what are our problems with hypothesis testing? - Rejecting or not rejecting hypotheses doesn’t helps us to answer real questions. - The fact that a difference is not “significant” has no relevance to how we use the posterior to make decisions. - Statistical significance is not equal to practical importance. - There is no reason to test when you can summarize by providing the full posterior probability distribution.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Mechanics</span>"
    ]
  },
  {
    "objectID": "mechanics.html#summary",
    "href": "mechanics.html#summary",
    "title": "7  Mechanics",
    "section": "\n7.6 Summary",
    "text": "7.6 Summary\nIn this chapter, we covered a number of topics important to effectively creating models.\nKey commands:\n\nCreate a model using brm().\nAfter creating a model, we can use loo() to perform leave-one-out cross validation. This assesses how effectively our model makes predictions for data it has not seen yet.\nThe command loo_compare() allows us to compare two models, to see which one performs better in leave-one-out cross validation. The superior model makes better predictions.\n\nRemember:\n\nWe can transform variables – through centering, scaling, taking logs, etc. – to make them more sensible. Consider using a transformation if the intercept is awkward. For instance, if the intercept for age represents the estimate for people of age zero, we might consider transforming age to be easier to interpret.\nWhen selecting variables to include in our model, follow this rule: keep it if the variable has a large and well-estimated coefficient. This means that the 95% confidence interval excludes zero. Speaking roughly, removing a variable with a large coefficient meaningfully changes the predictions of the model.\nWhen we compare across unit, meaning comparing Joe and George, we are not looking at a causal relationship. Within unit discussions, where we are comparing Joe under treatment versus Joe under control, are causal. This means that within unit interpretation is only possible in causal models, where we are studying one unit under two conditions.\nWhen we talk about two potential outcomes, we are discussing the same person or unit under two conditions.\n\n\n\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Analytical Methods for Social Research. Cambridge University Press. https://doi.org/10.1017/9781139161879.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Mechanics</span>"
    ]
  },
  {
    "objectID": "four-parameters-categorical.html",
    "href": "four-parameters-categorical.html",
    "title": "\n8  Four Parameters: Categorical\n",
    "section": "",
    "text": "8.1 Wisdom\nWisdom requires the creation of a Preceptor Table, an examination of our data, and a determination, using the concept of validity, as to whether or not we can (reasonably!) assume that the two come from the same population.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Four Parameters: Categorical</span>"
    ]
  },
  {
    "objectID": "four-parameters-categorical.html#wisdom",
    "href": "four-parameters-categorical.html#wisdom",
    "title": "\n8  Four Parameters: Categorical\n",
    "section": "",
    "text": "8.1.1 Preceptor Table\nA Preceptor Table is smallest possible table with rows and columns such that, if there is no missing data, our question is easy to answer.\nCausal or predictive model: This model is predictive not causal. Sex can not be a treatment because there is no (plausible) way to manipulate it. Recall the motto: “No causation without manipulation.”\nUnits: The rows of the Preceptor Table refer to individual US voters. The question suggests that we are not interested in people who did not vote, although one might explore if men were more or less likely to vote in the first place. As always, the initial question rarely specifies the Preceptor Table precisely.\nOutcome: We want Presidential voting behavior in 1992. Such explorations are often restricted to just the two “major party” candidates, the nominees of the Democratic and the Republican parties, Bill Clinton and George HW Bush. But, in 1992, Ross Perot was a very successful “third party” candidate, winning almost 19% of the vote. Should he be included in the analysis? What about the 4th place finisher, Libertarian Andre Marrou? Again, the question does not specify. However, the outcome variable is certainly the candidate for whom an individual voted.\nCovariates: The only covariate we will consider is sex, with two values: “male” and “female”. Whatever your feelings about the contemporary limits of the sex binary, this is the only data collected in 1992s. In a full analysis, we would make use of more covariates, but the primary purpose of this chapter is to explore a categorical model with three possible outcomes.\nMoment in Time: The Preceptor Table refers to the election season in the fall of 1992. We make no claims about other US presidential elections, not least because few feature a third party candidate as popular as Perot. Our purpose here is historical. We want to better understand what happened in this election, not make claims about other time periods. The moment in time is, however, longer than just Election Day itself since mail-in votes were cast in the weeks proceeding the election.\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\nOutcome\nCovariate\n\n\nVote\nSex\n\n\n\n\n1\nDemocrat\nM\n\n\n2\nThird Party\nF\n\n\n…\n…\n…\n\n\n10\nRepublican\nF\n\n\n11\nDemocrat\nF\n\n\n…\n…\n…\n\n\n103,754,865\nRepublican\nM\n\n\n\n\n\n\n\nAs usual the ID column is irrelevant. We don’t need to know exactly who voted for whom to answer our question. We just need to know the sex of each voter.\nThere were 104,425,014 total votes cast in the 1992 Presidential election. But our table only has 103,754,865 rows because we are only modeling people who cast their votes for one of the three main candidates. Is that the “right” approach? It depends! As always, data science is a dialogue between the question with which we start and the data/tools we have available to answer it. In this case, we choose to simplify the question, to focus on only the three main candidates, mainly because we want to use a model with three outcomes. We could, instead, create a more complicated model which included votes cast for other candidates. Our new question is now:\n\nWhat was the relationship between sex and voting in the 1992 US Presidential election among supporters of the three leading candidates: Clinton, Bush and Perot?\n\n\n8.1.2 EDA\nOur data comes from American National Election Studies. The primer.data package includes a version of the main data set with a selection of variables. The full ANES data is much richer than this relatively simple tibble.\n\nnes\n\n# A tibble: 46,838 × 18\n    year state sex    income  age     education      race  ideology voted region\n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;  &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;          &lt;chr&gt; &lt;fct&gt;    &lt;chr&gt; &lt;fct&gt; \n 1  1952 NY    Female 68 - 95 25 - 34 Highschool     White Weak Re… Yes   North…\n 2  1952 NY    Female 68 - 95 25 - 34 Elementary     White Indepen… Yes   North…\n 3  1952 NY    Female 34 - 67 25 - 34 Highschool     White Indepen… Yes   North…\n 4  1952 NY    Male   34 - 67 55 - 64 Some Highscho… White Strong … Yes   North…\n 5  1952 OH    Female 0 - 16  65 - 74 Highschool +   White Strong … Yes   Midwe…\n 6  1952 OH    Female 68 - 95 45 - 54 Some Highscho… White Indepen… Yes   Midwe…\n 7  1952 ID    Female 0 - 16  65 - 74 Elementary     White Indepen… Yes   West  \n 8  1952 MI    Male   17 - 33 25 - 34 Highschool +   Black Weak De… Yes   Midwe…\n 9  1952 GA    Female 0 - 16  25 - 34 Some Highscho… White &lt;NA&gt;     No    South \n10  1952 OH    Female 68 - 95 35 - 44 Elementary     White Weak Re… Yes   Midwe…\n# ℹ 46,828 more rows\n# ℹ 8 more variables: pres_appr &lt;chr&gt;, influence &lt;chr&gt;, equality &lt;chr&gt;,\n#   religion &lt;chr&gt;, better_alone &lt;chr&gt;, therm_black &lt;int&gt;, therm_white &lt;int&gt;,\n#   pres_vote &lt;chr&gt;\n\n\nSee the ?nes for details. We only need a small subset of this data:\n\nnes_92 &lt;- nes |&gt; \n  filter(year == 1992) |&gt; \n  select(sex, pres_vote) |&gt; \n  drop_na() |&gt; \n  mutate(pres_vote = case_when(\n    pres_vote == \"Democrat\" ~ \"Clinton\",\n    pres_vote == \"Republican\" ~ \"Bush\",\n    pres_vote == \"Third Party\" ~ \"Perot\",\n  ))\n\n\nShow the codenes_92\n\n# A tibble: 1,658 × 2\n   sex    pres_vote\n   &lt;chr&gt;  &lt;chr&gt;    \n 1 Female Bush     \n 2 Female Bush     \n 3 Female Clinton  \n 4 Male   Bush     \n 5 Female Clinton  \n 6 Female Clinton  \n 7 Female Perot    \n 8 Male   Bush     \n 9 Female Bush     \n10 Male   Perot    \n# ℹ 1,648 more rows\n\n\nIt is worth thinking about the quality of this data. How did NES, how do we, know for whom a surveyed individual voted? The answer is that we don’t. NES does not check the voting records and, even if it did, it could not determine a person’s vote.\nAs always, visualization is a useful tool. Plot your data! The outcome variable usually goes on the y-axis.\n\nnes_92 |&gt; \n  ggplot(aes(x = pres_vote, fill = sex)) +\n    geom_bar(position = \"dodge\") +\n    labs(title = \"Survey of 1992 Presidential Election Votes\",\n         subtitle = \"Men were much more likely to support Ross Perot\",\n         x = NULL,\n         y = \"Count\",\n         caption = \"Source: American National Election Survey\")\n\n\n\n\n\n\n\n\n8.1.3 Validity\nOur Preceptor Table includes the 103,754,865 people who voted for one of the three leading candidates in the 1992 presidential election. Our data includes a (tiny) subset of those people. This is a standard aspect of a “historical” data science project, when the data we want (the Preceptor Table) and the data we have come from the same moment in time. This makes the assumptions of validity and stability much easier to maintain. Of course, there can always be problems. A response to the nice ANES surveyer about one’s vote (or sex) is not the same thing as one’s actual vote (or sex). Indeed, a standard artifact of political surveys is that more people claim to have voted for the winning candidate than actually did. In this case, however, we will assume that validity holds and that we can “stack” the columns from the Preceptor Table and the data on top of each other. In fact, validity allows us to assume that the rows in the data is actually a subset of the rows in the Preceptor Table. To summarize:\n\nUsing data from the National Election Studies survey of US citizens, we seek to understand the relationship between voter preference and sex in the 1992 Presidential election.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Four Parameters: Categorical</span>"
    ]
  },
  {
    "objectID": "four-parameters-categorical.html#justice",
    "href": "four-parameters-categorical.html#justice",
    "title": "\n8  Four Parameters: Categorical\n",
    "section": "\n8.2 Justice",
    "text": "8.2 Justice\nJustice concerns four topics: the Population Table, stability, representativeness, and unconfoundedness.\n\n8.2.1 Population Table\nBecause this project is historical, the Population Table has the same number of rows as the Preceptor Table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation Table\n\n\nSource\nID\nOutcome\nCovariate\n\n\nVote\nSex\n\n\n\n\nPT/Data\n1\nDemocrat\nM\n\n\nPT/Data\n2\nThird Party\nF\n\n\nPT\n3\nRepublican\nM\n\n\nPT\n4\nDemocrat\nF\n\n\nPT\n5\nDemocrat\nF\n\n\nPT\n6\nDemocrat\nM\n\n\n…\n…\n…\n…\n\n\nPT/Data\n10\nRepublican\nF\n\n\nPT/Data\n11\nDemocrat\nF\n\n\nPT\n12\nDemocrat\n…\n\n\nPT\n13\nRepublican\nF\n\n\n…\n…\n…\n…\n\n\nPT/Data\n103,754,865\nRepublican\nM\n\n\n\n\n\n\n\nWe are not interested in these voters outside of the 1992 election. So, this Population Table, unlike most, does not require a Time column. The Preceptor Table and the data come from the same moment in time. Of course, this is not literally true. Recall: The Time column is always a lie. Some voters cast their ballots weeks before Election Day. Some NES participants were surveyed right after the election. Some were survey later. We sweep all these complications under the mythical moment in time which we assert is the same for both the data and the Preceptor Table.\n\n8.2.2 Stability\nIf the assumption of stability holds, then the relationship between the columns in the Population Table is the same for three categories of rows: the data, the Preceptor Table, and the larger population from which both are drawn. In this case, there is no larger population. Or, rather, the Preceptor Table is the larger population. And, since the data and the Preceptor Table come from the same moment in time, stability holds by definition.\n\n8.2.3 Representativeness\nThe NES is a highly professional organization so their survey does a good of capturing a random sample of all voters. But no real world survey is perfect! There are always problems. In particular, there are (somewhat) unobserved differences between the sort of people who respond to surveys and the sort of people who don’t. NES is more likely to capture the votes of cooperative people than the votes of misanthropes. If, among misanthropes, the relationship between sex and Presidential candidate is different than the relationship among cooperative voters, we might have problems. The technical term for the problem of people who don’t respond to surveys is “non-response.”\n\n8.2.4 Unconfoundedness\nBecause this is a predictive, not causal, model, the assumption of unconfoundedness is irrelevant.\n\nUsing data from the National Election Studies (NES) survey of US citizens, we seek to understand the relationship between voter preference and sex in the 1992 Presidential election. Our results might be biased by differential non-response among different categories of voters.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Four Parameters: Categorical</span>"
    ]
  },
  {
    "objectID": "four-parameters-categorical.html#courage",
    "href": "four-parameters-categorical.html#courage",
    "title": "\n8  Four Parameters: Categorical\n",
    "section": "\n8.3 Courage",
    "text": "8.3 Courage\nJustice gave us the Population Table. Courage selects the data generating mechanism. We first specify the mathematical formula which connects the outcome variable we are interested in with the other data that we have.\nBecause we have three possible outcomes, and because there is no natural ordering among the outcomes, a categorical model is the most natural choice. The binomial model which we used in Chapter 4 is just a special case of a categorical model, one in which there are only two outcomes. Recall:\n\\[ red_i  \\sim Bernoulli(\\rho) \\] A more verbose way to present this same model would be:\n\\[ red_i  \\sim Categorical(\\rho_0, \\rho_1) \\] After all, there are two possible values for \\(red_i\\), 0 and 1. There is, therefore, a probability that the bead comes back not-red and a probability that it comes back red. Label those probabilities \\(\\rho_0\\) and \\(\\rho_1\\). In a Bernoulli model, we don’t bother to consider both \\(\\rho_0\\) and \\(\\rho_1\\) because of the mathematical relationship which must hold between them:\n\\[ \\rho_0 + \\rho_1 = 1 \\] If you know the value of \\(\\rho_1\\), then you can calculate the value of \\(\\rho_0\\) easily. So, in Bernoulli models we don’t bother to even consider both \\(\\rho_0\\) and \\(\\rho_1\\). We just worry about the value of \\(\\rho_1\\), and define it as \\(\\rho\\), with the 1 underscore understood by convention.\nThings get more complex, however, with more than two possible outcomes.\n\\[ vote_i  \\sim Categorical(\\rho_{bush}, \\rho_{clinton}, \\rho_{perot}) \\] The vote of person \\(i\\) takes on one of three possible values: Bush (Republican), Clinton (Democrat) or Perot (Third Party). The probability of each outcome is given by the respective \\(\\rho\\). And, by definition:\n\\[ \\rho_{bush} + \\rho_{clinton} + \\rho_{perot} = 1 \\] So, once we know two of the probabilities, we know the third. That means that we can’t estimate them separately. If we do, we run the risk of violating this equality. We must estimate them together. Fortunately, brms takes care of all the details.\nRecall the logit link function from the Bernoulli model in Chapter 4:\n\\[\n\\rho = \\frac{e^{\\beta_0}}{1 + e^{\\beta_0}}\n\\]\nWe need to expand this in two ways: First, we have to allow for variables (in this case, the sex of the voter) to influence the probability.\n\n\\[\n\\rho = \\frac{e^{\\beta_0 + \\beta_1 {male}}}{1 + e^{\\beta_0 + \\beta_1 {male}}}\n\\] The logit link function can be expanded just like a simple linear regression. We just add more terms, along with their associated coefficients. Regardless of how complex this term becomes, \\(\\rho\\) will remain bounded between 0 and 1.\nSecond, we need to allow for three different \\(\\rho\\)’s. This is the Categorical model:\n\\[\n\\begin{aligned}\n\\rho_{clinton} &=& \\frac{e^{\\beta_{0, clinton} + \\beta_{1, clinton} male}}{1 + e^{\\beta_{0, clinton} + \\beta_{1, clinton} male}}\\\\\n\\rho_{perot} &=& \\frac{e^{\\beta_{0, perot} + \\beta_{1, perot} male}}{1 + e^{\\beta_{0, perot} + \\beta_{1, perot} male}}\\\\\n\\rho_{bush}  &=& 1 - \\rho_{clinton} - \\rho_{perot}\n\\end{aligned}\n\\]\nThere is no longer just one \\(\\beta_0\\) and one \\(\\beta_1\\). Instead, there is \\(\\beta_0\\) for Clinton, labeled as \\(\\beta_{0, clinton}\\) and a \\(\\beta_0\\) for Perot, labeled as \\(\\beta_{0, perot}\\). The same applies to \\(\\beta_1\\). These parameters need to be different because being male has a different connection to the probability for voting for Clinton than it does for the probability for voting for Perot. We can’t have a separate \\(\\beta_0\\) and \\(\\beta_1\\) for Bush because \\(\\rho_{bush}\\) is fully defined once we know \\(\\rho_{clinton}\\) and \\(\\rho_{perot}\\).\n\n8.3.1 Models\nFitting a categorical model with brms is very similar to fitting the other models we have learned.\n\nfit_nes &lt;- brm(formula = pres_vote ~ sex,\n               data = nes_92,\n               family = categorical(),\n               silent = 2,\n               refresh = 0,\n               seed = 76)\n\nWe use silent = 2 and refresh = 0 to hide all the messages produced by brm(). We don’t want to clutter the Primer. But those messages can be important. So, we constructing your models interactively, show the messages, at least until you confirm that there is nothing to worry about.\n\nfit_nes\n\n Family: categorical \n  Links: muClinton = logit; muPerot = logit \nFormula: pres_vote ~ sex \n   Data: nes_92 (Number of observations: 1658) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nmuClinton_Intercept     0.45      0.07     0.31     0.60 1.00     2827     2802\nmuPerot_Intercept      -0.86      0.11    -1.08    -0.64 1.00     2577     2687\nmuClinton_sexMale      -0.25      0.11    -0.48    -0.04 1.00     2894     2704\nmuPerot_sexMale         0.42      0.14     0.14     0.71 1.00     3036     2666\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe most relevant parts of the fitted model are the parameter estimates.\n\nfixef(fit_nes)\n\n                      Estimate  Est.Error       Q2.5       Q97.5\nmuClinton_Intercept  0.4546458 0.07467146  0.3080325  0.59727294\nmuPerot_Intercept   -0.8553572 0.10854045 -1.0764757 -0.64397943\nmuClinton_sexMale   -0.2529694 0.11176079 -0.4759733 -0.03690656\nmuPerot_sexMale      0.4244909 0.14498279  0.1399061  0.71336963\n\n\nThe term muClinton_Intercept refers to \\(\\beta_{0, clinton}\\). The term muClinton_sexMale refers to \\(\\beta_{1, clinton}\\). And so on. The more complex our models become, the less interested we are in the values of any particular parameter. Also, the parameters in more complex models rarely have simple interpretations, as they sometimes do in simple linear models. So, ignore the specific values. But do note how all 4 variables have confidence intervals which exclude zero. This suggests, but does not prove, that they are all worth including in our model.\nWe can use add_epred_draws() to explore the posterior distribution of the expected values of \\(\\rho_{clinton}\\), \\(\\rho_{bush}\\), and \\(\\rho_{perot}\\) for women.\n\nfit_nes |&gt; \n  add_epred_draws(newdata = tibble(sex = \"Female\")) |&gt; \n  ggplot(aes(x = .epred, fill = .category)) +\n    geom_histogram(bins = 100) +\n    scale_x_continuous(labels = scales::percent_format()) +\n    labs(title = \"Posterior for Expected Probability of Candidate Support Among Women\",\n         subtitle = \"Women are most likely to support Clinton\",\n         x = \"Posterior Probability of Expected Vote Probability\",\n         y = \"Count\",\n         fill = \"Candidate\") +\n  scale_fill_manual(values = c(\"Clinton\" = \"blue\", \n                               \"Bush\" = \"red\", \n                               \"Perot\" = \"green\"),\n                    limits = c(\"Perot\", \"Bush\", \"Clinton\"))\n\n\n\n\n\n\n\n\n\n8.3.2 Tests\nAs usual, we use pp_check() to examine if the “fake data” created from our DGM matches our actual data.\n\npp_check(fit_nes, type = \"bars\")\n\nUsing 10 posterior draws for ppc type 'bars' by default.\n\n\n\n\n\n\n\n\n\n8.3.3 Data Generating Mechanism\nPutting the mathematics together with the parameter estimates gives us the data generating mechanism\n\\[ vote_i  \\sim Categorical(\\rho_{bush}, \\rho_{clinton}, \\rho_{perot}) \\]\n\\[\n\\begin{aligned}\n\\rho_{bush}  &=& 1 - \\rho_{clinton} - \\rho_{perot}\\\\\n\\rho_{clinton} &=& \\frac{e^{0.45 - 0.25 male}}{1 + e^{0.45 - 0.25 male}}\\\\\n\\rho_{perot} &=& \\frac{e^{-0.86 + 0.42 male}}{1 + e^{-0.86 + 0.42 male}}\\\\\n\\end{aligned}\n\\] This is the last time we are going to go to the trouble of combining the mathematical formula of the DGM with the specific estimated values. First, these formulas are misleading! The value of \\(\\beta_{0, clinton}\\), for example, is not exactly 0.45. In fact, we don’t know what the value is! Being Bayesians, we calculated a posterior probability distribution for \\(\\beta_{0, clinton}\\). We draw from that distribution when we calculate quantities of interest, as we did above with add_epred_draws(). Second, you probably did not even look that closely at these complex formulas. And we don’t blame you! Being a good race car driver means focussing on how to drive better, not on the physics of carburetors. Similar, being a good data scientist means focusing on the Cardinal Virtues as a method for using data to answer questions. Leave the math to the computer.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Four Parameters: Categorical</span>"
    ]
  },
  {
    "objectID": "four-parameters-categorical.html#temperance",
    "href": "four-parameters-categorical.html#temperance",
    "title": "\n8  Four Parameters: Categorical\n",
    "section": "\n8.4 Temperance",
    "text": "8.4 Temperance\nCourage produced the data generating mechanism. Temperance guides us in the use of the DGM — or the “model” — we have created to answer the questions with which we began. We create posteriors for the quantities of interest. We should be modest in the claims we make. The posteriors we create are never the “truth.” The assumptions we made to create the model are never perfect. Yet decisions made with flawed posteriors are almost always better than decisions made without them.\n\n8.4.1 Questions and Answers\nRecall the question with which we began:\n\nWhat was the relationship between sex and voting in the 1992 US Presidential election?\n\nThe more you progress in your data science education, the more general the questions with which you will be expected to handle. There is no single answer to this question! There are many true things one could say in response.\nOften, the best answers to broad questions are graphics. Consider:\n\nfit_nes |&gt; \n  add_epred_draws(newdata = tibble(sex = c(\"Female\", \"Male\"))) |&gt;\n  select(sex, .category, .epred) |&gt; \n  ggplot(aes(x = .epred, fill = sex)) +\n    geom_histogram(bins = 100) +\n    facet_grid(~ .category) +\n    scale_x_continuous(breaks = c(0.05, 0.3, 0.6),\n      labels = scales::percent_format())\n\nAdding missing grouping variables: `.row`\n\n\n\n\n\n\n\n  #   labs(title = \"Posterior for Expected Probability of Candidate Support Among Women\",\n  #        subtitle = \"Women are most likely to support Clinton\",\n  #        x = \"Posterior Probability of Expected Vote Probability\",\n  #        y = \"Count\",\n  #        fill = \"Candidate\") +\n  # scale_fill_manual(values = c(\"Clinton\" = \"blue\", \n  #                              \"Bush\" = \"red\", \n  #                              \"Perot\" = \"green\"),\n  #                   limits = c(\"Perot\", \"Bush\", \"Clinton\"))\n\n\n\n8.4.2 Humility\nWe can never know the truth.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Four Parameters: Categorical</span>"
    ]
  },
  {
    "objectID": "four-parameters-categorical.html#summary",
    "href": "four-parameters-categorical.html#summary",
    "title": "\n8  Four Parameters: Categorical\n",
    "section": "\n8.5 Summary",
    "text": "8.5 Summary",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Four Parameters: Categorical</span>"
    ]
  },
  {
    "objectID": "five-parameters.html",
    "href": "five-parameters.html",
    "title": "9  Five Parameters",
    "section": "",
    "text": "9.1 Wisdom\nConsider the following question:\nHow long do political candidates live after the election?",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Five Parameters</span>"
    ]
  },
  {
    "objectID": "five-parameters.html#wisdom",
    "href": "five-parameters.html#wisdom",
    "title": "9  Five Parameters",
    "section": "",
    "text": "Wisdom requires the creation of a Preceptor Table, an examination of our data, and a determination, using the concept of validity, as to whether or not we can (reasonably!) assume that the two come from the same population.\n\n\n\n\n9.1.1 Preceptor Table\n\nA Preceptor Table is smallest possible table with rows and columns such that, if there is no missing data, our question is easy to answer.\n\nTo create our Preceptor Table, we examine:\nCausal or predictive model: We have a predictive model since there is just one outcome: a candidate’s longevity.\nUnits: Our units for this scenario would be candidates because the questions are about the life expectancy of unique individuals.\nOutcome: The number of years a candidate lives after the election.\nTreatment: Since this is a predictive model there is no treatment per se. Any variable which one might consider a treatment is just another covariate in this context.\nCovariates: We know a variety of information about each candidate.\nMoment in Time: It is unclear to what moment in time the question refers. Is it interested in all candidates who are running in the next election? All those running in the future? Let’s choose to focus in candidate longevity in this century.\n\nHow many years would we expect political candidates in election years after 2000 to live?\n\nOur Preceptor Table:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\nOutcome\nCovariates\n\n\nYears Lived After\nSex\nYear of Election\nAge at Election\n\n\n\n\nCandidate 1\n12\nFemale\n2000\n63\n\n\nCandidate 2\n7\nMale\n2012\n47\n\n\n…\n…\n…\n…\n…\n\n\nCandidate 10\n10\nFemale\n2012\n52\n\n\nCandidate 11\n11\nFemale\n2024\n75\n\n\n…\n…\n…\n…\n…\n\n\nCandidate N\n6\nMale\n2050\n68\n\n\n\n\n\n\n\nWith the Preceptor Table, we can calculate anything related to longevity because we know how many years each candidate lived after an election. But, in reality, many of these candidates are still alive, so we don’t know how long they will live.\nNote also that the Preceptor Table stops with elections in 2050. So, implicitly, we have changed our question to:\n\nHow many years would we expect political candidates in election years between 2000 and 2050 to live?\n\nThe question, the Preceptor Table and the data evolve together in an iterative process.\n\n9.1.2 EDA of governors\n\nThe primer.data package includes the governors data set which features demographic information about candidates for governor in the United States. Barfort, Klemmensen, and Larsen (2020) gathered this data and concluded that winning a gubernatorial election increases a candidate’s lifespan.\n\nShow the codeglimpse(governors)\n\nRows: 1,092\nColumns: 14\n$ state        &lt;chr&gt; \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"A…\n$ year         &lt;int&gt; 1946, 1946, 1950, 1954, 1954, 1958, 1962, 1966, 1966, 197…\n$ first_name   &lt;chr&gt; \"James\", \"Lyman\", \"Gordon\", \"Tom\", \"James\", \"William\", \"G…\n$ last_name    &lt;chr&gt; \"Folsom\", \"Ward\", \"Persons\", \"Abernethy\", \"Folsom\", \"Long…\n$ party        &lt;chr&gt; \"Democrat\", \"Republican\", \"Democrat\", \"Republican\", \"Demo…\n$ sex          &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"…\n$ died         &lt;date&gt; 1987-11-21, 1948-12-17, 1965-05-29, 1968-03-07, 1987-11-…\n$ status       &lt;chr&gt; \"Challenger\", \"Challenger\", \"Challenger\", \"Challenger\", \"…\n$ win_margin   &lt;dbl&gt; 77.334394, -77.334394, 82.206564, -46.748166, 46.748166, …\n$ region       &lt;chr&gt; \"South\", \"South\", \"South\", \"South\", \"South\", \"South\", \"So…\n$ population   &lt;dbl&gt; 2906000, 2906000, 3058000, 3014000, 3014000, 3163000, 332…\n$ election_age &lt;dbl&gt; 38.07255, 78.54894, 48.74743, 46.54620, 46.07255, 33.2703…\n$ death_age    &lt;dbl&gt; 79.11567, 80.66530, 63.31006, 59.88227, 79.11567, 87.8193…\n$ lived_after  &lt;dbl&gt; 41.043121, 2.116359, 14.562628, 13.336071, 33.043121, 54.…\n\n\nThere are 14 variables and 1,092 observations. In this Chapter, we will only be looking at the variables last_name, year, state, sex, lived_after, and election_age.\n\nch7_a &lt;- governors |&gt; \n  select(last_name, year, state, sex, lived_after, election_age)\n\nelection_age and lived_after are how many years a candidate lived before and after the election, respectively. As a consequence, only politicians who are already deceased are included in this data set. This means that there are only a handful of observations from elections in the last 20 years. Most candidates from that time period are still alive and are, therefore, excluded.\nOne subtle issue: Should the same candidate be included multiple times? For example:\n\nch7_a |&gt; \n  filter(last_name == \"Cuomo\")\n\n# A tibble: 4 × 6\n  last_name  year state    sex   lived_after election_age\n  &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n1 Cuomo      1982 New York Male         32.2         50.4\n2 Cuomo      1986 New York Male         28.2         54.4\n3 Cuomo      1990 New York Male         24.2         58.4\n4 Cuomo      1994 New York Male         20.2         62.4\n\n\nFor now, we leave in multiple observations for a single person.\nFirst, let’s sample from our data set.\n\nShow the codech7_a |&gt; \n  slice_sample(n = 5)\n\n# A tibble: 5 × 6\n  last_name  year state        sex    lived_after election_age\n  &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 Ristine    1964 Indiana      Male          44.6         44.8\n2 Sundlun    1986 Rhode Island Male          24.7         66.8\n3 Richards   1990 Texas        Female        15.9         57.2\n4 Turner     1946 Oklahoma     Male          26.6         52.0\n5 Williams   1948 Michigan     Male          39.2         37.7\n\n\nAs we might expect, sex is more often “Male”. To be more precise in inspecting our data, let’s skim() the data set.\n\nShow the codeskim(ch7_a)\n\n\nData summary\n\n\nName\nch7_a\n\n\nNumber of rows\n1092\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nlast_name\n0\n1\n3\n11\n0\n615\n0\n\n\nstate\n0\n1\n4\n14\n0\n50\n0\n\n\nsex\n0\n1\n4\n6\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nyear\n0\n1\n1964.85\n13.38\n1945.00\n1954.00\n1962.00\n1974.00\n2011.00\n▇▆▃▂▁\n\n\nlived_after\n0\n1\n28.23\n13.38\n0.13\n17.57\n29.60\n38.67\n60.42\n▃▆▇▆▂\n\n\nelection_age\n0\n1\n51.72\n8.71\n31.35\n45.34\n51.36\n57.48\n83.87\n▂▇▆▂▁\n\n\n\n\n\nskim() groups the variables together by type and provides some analysis for each variable. We are also given histograms of the numeric data.\nLooking at the histogram for year, we see that it is skewed right — meaning that most of the data is bunched to the left and that there is a smaller tail to the right — with half of the observations from election years between 1945 and 1962. This makes sense logically, because we are only looking at deceased candidates, and candidates from more recent elections are more likely to still be alive.\nIn using this data set, our left-side variable will be lived_after. We are trying to understand/predict how many years a candidate will live after the election.\n\nShow the codech7_a |&gt;\n  ggplot(aes(x = year, y = lived_after)) +\n  geom_point() +\n  labs(title = \"US Gubernatorial Candidate Years Lived Post-Election\",\n       subtitle = \"Candidates who died more recently can't have lived for long post-election\",\n       caption = \"Data: Barfort, Klemmensen and Larsen (2019)\",\n       x = \"Year Elected\",\n       y = \"Years Lived After Election\") +\n  scale_y_continuous(labels = scales::label_number()) +\n  theme_classic() \n\n\n\n\n\n\n\nNote that there is a rough line above which we see no observations. Why might this be? When looking at the year elected and years lived post-election, there is missing data in the upper right quadrant because it is impossible to have been elected post-2000 and lived much more than 11 years. (Note that the last election in this data was in 2012.) Simply put: this “edge” of the data represents, approximately, the most years a candidate could have lived, and still have died, given the year that they were elected.\nThe reason the data is slanted downward is because the maximum value for this scenario is greater in earlier years. That is, those candidates who ran for governor in earlier years could live a long time after the election and still have died prior to the data set creation, giving them higher lived_after values than those who ran for office in more recent years. The edge of the scatter plot is not perfectly straight because, for many election years, no candidate had the decency to die just before data collection. The reason for so few observations in later years is that fewer recent candidates have died.\nTo begin visualizing our lived_after data, we will inspect the difference in years lived post election between male and female candidates.\n\nShow the codech7_a |&gt;\n  ggplot(aes(x = sex, y = lived_after)) +\n  geom_boxplot() +\n  labs(title = \"US Gubernatorial Candidate Years Lived Post-Election\",\n       subtitle = \"Male candidates live much longer after the election\",\n       caption = \"Data: Barfort, Klemmensen and Larsen (2019)\",\n       x = \"Gender\",\n       y = \"Years Lived After Election\") +\n  scale_y_continuous(labels = scales::label_number()) +\n  theme_classic() \n\n\n\n\n\n\n\nThis plot shows that men live much longer, on average, than women after the election. Is there an intuitive explanation for why this might be?\n\nShow the codech7_a |&gt; \n  ggplot(aes(x = election_age, y = lived_after)) +\n    geom_point() +\n    labs(title = \"Longevity of Gubernatorial Candidates\",\n         subtitle = \"Younger candidates live longer\", \n         caption = \"Data Source: Barfort, Klemmensen and Larsen (2019)\",\n         x = \"Age in Years on Election Day\",\n         y = \"Years Lived After Election\") +\n    scale_x_continuous(labels = scales::label_number()) +\n    scale_y_continuous(labels = scales::label_number()) +\n    theme_classic()\n\n\n\n\n\n\n\nYounger candidates tend to live much longer than older candidates after election. This makes sense. It is hard for an 80 year-old candidate to live for another 40 years after the election.\n\n\n9.1.3 Validity\n\nIs the meaning of the columns consistent, i.e., can we assume validity? To put it simply, does the lived_after column in our Preceptor Table equate to same column in our data set. Again, we look to the source of our data: Barfort, Klemmensen, and Larsen (2020).\nThe collection of birth and death dates for winning candidates is well documented. The birth and death dates for losing candidates, however, is not as easily gathered. In fact, Barfort, Klemmensen, and Larsen (2020) had to perform independent research for this information:\n\n“For losing candidates, we use information gathered from several online sources, including Wikipedia, The Political Graveyard…, Find a Grave… and Our Campaigns.”\n\nThis is not nearly as reliable as the data collection for candidates who won their election. And, there was a further complication:\n\n“In a few cases, we are only able to identify the year of birth or death, not the exact date of the event. For these candidates, we impute the date as July 1 of the given year.”\n\nFor these candidates, then, our estimate for longevity will be inaccurate.\nThe assumption of validity seems reasonable. Our data and The Preceptor Table can “stack” on top of each other. We will assume that both are drawn from the same population.\nNote that there is some overlap between our Preceptor Table and our data. For example, Kathleen Babineaux won the 2003 election for governor in Louisana. She died in 2019, so she is both in our data and in our Preceptor Table.\n\nUsing data from all deceased gubernatorial candidates in the United States between 1945 and 2012, we seek to forecast candidate longevity post-election for candidates running in campaigns between 2000 and 2050.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Five Parameters</span>"
    ]
  },
  {
    "objectID": "five-parameters.html#justice",
    "href": "five-parameters.html#justice",
    "title": "9  Five Parameters",
    "section": "\n9.2 Justice",
    "text": "9.2 Justice\n\n\n\n\n\n\n\n\n\nJustice concerns four topics: the Population Table, stability, representativeness, and unconfoundedness.\n\n\n9.2.1 Population Table\nThe Population Table is a structure which includes a row for every unit in the population. We generally break the rows in the Population Table into three categories: the data for units we have (the actual data set), the data for units which we actually want to have (the Preceptor Table), and the data for units we do not care about (the rest of the population, not included in the data or the Preceptor Table).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation Table\n\n\nSource\nOutcome\nCovariates\n\n\nYears Lived After\nSex\nCity\nYear Elected\nElection Age\n\n\n\n\n…\n…\n…\n…\n…\n…\n\n\nData\n20\nMale\nBoston, MA\n1967\n43\n\n\nData\n19\nMale\nBoston, MA\n2012\n67\n\n\n…\n…\n…\n…\n…\n…\n\n\n…\n…\n…\n…\n…\n…\n\n\nPreceptor Table\n?\nFemale\nChicago, IL\n2024\n75\n\n\nPreceptor Table\n?\nMale\nChicago, IL\n2024\n50\n\n\n…\n…\n…\n…\n…\n…\n\n\n\n\n\n\n\nThe Population Table has a different structure than typically.\nFirst, recall that the rows in the Population Table are defined by time/unit combinations. For example, we could have one row for Joe in the year 1950, when he ran (unsuccessfully) and another in 1954 when he ran again. At the same time, we might not have a row for Joe in 1953, when there was no gubernatorial election for him to run in.\nSecond, we have not specified the rows for either the Preceptor Table (or the Population Table) concerning what sort of elections we are interested in. How about Senate races? Those are similar to gubernatorial elections in that they are state-wide and attract series candidates. But are there rows for other elections? What about mayorial elections? What about the town library committee? We need to make some choices here and, given that we only have data for gubernatorial elections, we need to be careful. Update:\n\nUsing data from all deceased gubernatorial candidates in the United States between 1945 and 2012, we seek to forecast candidate longevity in state-wide US races post-election.\n\nThis is a change in our Preceptor Table. We are no longer interested in candiates outside the US, or those running outside of gubernatorial or Senate contests.\nThird, rows from the data and the Preceptor Table are intermixed in the Population Table. This is unusual since, normally, we are interested in now (and the future) in the Preceptor Table and we only have data from the past. In this case, our data includes an election from 2000 in which one candidate has died. We have all her data. But the very previous row is for her opponent, who is still alive and therefore not part of the data but very much a row in the Preceptor Table.\nFourth, every row in the data is part of the Preceptor Table. This is similar to examples we saw when sampling beads from an urn.\n\n9.2.2 Stability\nStability means that the relationship between the columns is the same for three categories of rows: the data, the Preceptor table, and the larger population from which both are drawn.\nLifespan changes over time. In fact, between 1960 and 2015, life expectancy for the total population in the United States increased by almost 10 years. Therefore, our estimates for the future may need some adjustment — that is, to add years to our predicted life expectancy to account for a global change in lifespan over time.\nWhen we are confronted with this uncertainty, we can consider making our timeframe smaller. After all, if we confined the data to candidates post-1980, we would expect more stability in lifespan. This modification may be appropriate, but it limits our inferences.\nThe longer the time period covered by the Preceptor Table (and the data), the more suspect the assumption of stability becomes.\n\n9.2.3 Representativeness\n\nBarfort, Klemmensen, and Larsen (2020) report that:\n\n“We collect data… for all candidates running in a gubernatorial election from 1945 to 2012. We limit attention to the two candidates who received the highest number of votes.”\n\nThis data is, then, highly representative of gubernatorial candidates, as it includes every candidate from 1945 to 2012. However, there is one large caveat: only the two candidates with the most votes are included in the data set. This is unfortunate, as we would ideally look at all gubernatorial candidates (regardless of votes).\nGiven this data-implementation, we can either change the question we are answering (restricting it to just major party candidates) or by generous with regard to the assumption of representativeness — assume that major party candidates are not systematically different from all candidates.\nThe more expansive your Preceptor Table, the more important the assumption of representativeness becomes.\n\n9.2.4 Unconfoundedness\nOur model is predictive, so unconfoundedness is not a concern.\nPutting all the above together, we have:\n\nUsing data from all deceased gubernatorial candidates in the United States from elections held between 1945 and 2012, we seek to forecast candidate longevity in state-wide US races post-election. There is concern that longevity for gubernatorial candidates will differ significantly from that for candidates in Senate and other state-wide elections.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Five Parameters</span>"
    ]
  },
  {
    "objectID": "five-parameters.html#courage",
    "href": "five-parameters.html#courage",
    "title": "9  Five Parameters",
    "section": "\n9.3 Courage",
    "text": "9.3 Courage\n\n\n\n\n\n\n\n\n\nCourage begins with the exploration and testing of different models. It concludes with the creation of a Data Generating Mechanism.\n\nJustice gave us the Population Table. Courage selects the data generating mechanism. We first specify the mathematical formula which connects the outcome variable we are interested in with the other data that we have. We explore different models. We need to decide which variables to include and to estimate the values of unknown parameters. We check our models for consistency with the data we have. We avoid hypothesis tests. We select one model.\nSince our outcome variable is ages lived after an election, a linear model with a normal error term seems reasonable.\n\\[ y_i = \\beta_0  + \\beta_1 x_{1,i} +  \\beta_2 x_{2,i} + \\beta_2 x_{1,i} x_{2,i} + \\epsilon_i\\] with \\(\\epsilon_i \\sim N(0, \\sigma^2)\\). This is an “interaction” model because one of the terms features the multiplication of two variables together.\n\n9.3.1 Models\nDoes sex help us to forecast longevity?\n\n9.3.1.1 sex\nIn this regression, our mathematical formula is:\n\\[ lived\\_after_i = \\beta_0  + \\beta_1 male_i + \\epsilon_i\\]\n\\(\\beta_0\\) is the intercept. In this type of model, our intercept represents the the variable which is not represented in the model. Therefore, the intercept value represents those who are not male (females).\n\\(\\beta_1\\) only affects the outcome when the candidate is male. When the candidate is a male, we add the coefficient for male to the intercept value, which gives us the average lifespan of a male gubernatorial candidate after an election.\nLet’s regress lived_after on sex to see how candidates’ post-election lifespans differ by sex.\n\nfit_sex &lt;- brm(data = ch7_a,\n               formula = lived_after ~ sex,\n               silent = 2,\n               refresh = 0,\n               seed = 76)\n\nTrying to compile a simple C file\n\n\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nusing C compiler: ‘Apple clang version 15.0.0 (clang-1500.3.9.4)’\nusing SDK: ‘’\nclang -arch arm64 -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/opt/R/arm64/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c foo.c -o foo.o\nIn file included from &lt;built-in&gt;:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/Core:19:\n/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: 'cmath' file not found\n#include &lt;cmath&gt;\n         ^~~~~~~\n1 error generated.\nmake: *** [foo.o] Error 1\n\n\n\nprint(fit_sex, detail = FALSE)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: lived_after ~ sex \n   Data: ch7_a (Number of observations: 1092) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    15.97      2.91    10.21    21.54 1.00     3816     2847\nsexMale      12.50      2.95     6.80    18.37 1.00     3804     2929\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    13.29      0.28    12.74    13.87 1.00     3798     2990\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThis suggests that, for female candidates (for whom sexMale equal 0), life expectancy after the election is about 16 years. However, there is a great deal of uncertainty associated with that estimate. The 95% confidence interval ranges from 10 to 22 years.\nFor male candidates, the coefficient of sexMale is the additional number of years which make candidates live after the election, above and beyond the 16 years for female candidates. This is about 12 or 13 years. And that is very surpising! Men, as a group, do not live nearly as long as women. Why would it be different among gubernatorial candidates?\n\n9.3.1.2 election_age\nInstead of sex, we might consider longevity after the election to be a simple function of the candidate’s age at the election. Younger candidates should love longer.\n\\[ lived\\_after_i =  \\beta_0 + \\beta_1 election\\_age_i + \\epsilon_i \\]\nwith \\(\\epsilon_i \\sim N(0, \\sigma^2)\\).\n\n\\(lived\\_after_i\\) is the number of years lived after the election for candidate \\(i\\).\n\\(election\\_age_i\\) is the number of years lived before the election for candidate \\(i\\).\n\\(\\epsilon_i\\) is the “error term,” the difference between the actual years-lived for candidate \\(i\\) and the modeled years-lived. \\(\\epsilon_i\\) is normally distributed with a mean of 0 and a standard deviation of \\(\\sigma\\).\n\nA key distinction is between:\n\nVariables, always scripted with \\(i\\), whose values (potentially) vary across individuals.\nParameters, never scripted with \\(i\\), whose values are constant across individuals.\n\nWhy do we use \\(lived\\_after_i\\) in this formula instead of \\(y_i\\)? The more often we remind ourselves about the variable’s actual substance, the better. But there is another common convention: to always use \\(y_i\\) as the symbol for the dependent variable. It would not be unusual to describe this model as:\n\\[ y_i =  \\beta_0 + \\beta_1 election\\_age_i + \\epsilon_i\\]\nThe mathematics are the same.\nEither way, \\(\\beta_0\\) is the “intercept” of the regression, the average value for the population of \\(lived\\_after\\), among those for whom \\(election\\_age = 0\\).\n\\(\\beta_1\\) is the “coefficient” of \\(election\\_age\\). When comparing two individuals, the first with an \\(election\\_age\\) one year older than the second, we expect the first to have a \\(lived\\_after\\) value \\(\\beta_1\\) different from the second. In other words, we expect the older to have fewer years remaining, because \\(\\beta_1\\) is negative. Again, this is the value for the population from which our data is drawn.\nThere are three unknown parameters — \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma\\) — just as with the models we used in early chapters. Before we get to the five parameter case, it is useful to review this earlier material.\nYou may recall from middle school algebra that the equation of a line is \\(y = m x + b\\). There are two parameters: \\(m\\) and \\(b\\). The intercept \\(b\\) is the value of \\(y\\) when \\(x = 0\\). The slope coefficient \\(m\\) for \\(x\\) is the increase in \\(y\\) for every one unit increase in \\(x\\). When defining a regression line, we use slightly different notation but the fundamental relationship is the same.\nNow we want to use stan_glm() to convert our mathematics into code through a fitted model.\n\nfit_age &lt;- brm(data = ch7_a,\n               formula = lived_after ~ election_age,\n               silent = 2,\n               refresh = 0,\n               seed = 9)\n\nTrying to compile a simple C file\n\n\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nusing C compiler: ‘Apple clang version 15.0.0 (clang-1500.3.9.4)’\nusing SDK: ‘’\nclang -arch arm64 -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/opt/R/arm64/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c foo.c -o foo.o\nIn file included from &lt;built-in&gt;:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/Core:19:\n/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: 'cmath' file not found\n#include &lt;cmath&gt;\n         ^~~~~~~\n1 error generated.\nmake: *** [foo.o] Error 1\n\n\n\nprint(fit_age, detail = FALSE)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: lived_after ~ election_age \n   Data: ch7_a (Number of observations: 1092) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept       72.64      2.03    68.53    76.51 1.00     4767     3466\nelection_age    -0.86      0.04    -0.93    -0.78 1.00     4722     3466\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    11.11      0.23    10.67    11.57 1.00     3772     2847\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nThe variable before the tilde, lived_after, is our outcome.\nThe only explanatory variable is election_age. This variable numeric but, looking at the data, actually consists of just integers.\nRecall that our model is linear. However, we don’t have to specify that the family we are going to use is gaussian because that is the default input that is assumed by brm().\n\nThe most common term for a model like this is a “regression.” We have “regressed” lived_after, our dependent variable, on election_age, our (only) independent variable.\nConsider someone who is about 40 years old on Election Day. We have a score or more data points for candidates around that age. This area is highlighted by the red box on our plot below. As we can see, two died soon after the election. Some of them lived for 50 or more years after the election. Variation fills the world. However, the fitted line tells us that, on average, we would expect a candidate that age to live about 37 years after the election.\nThis is a descriptive model, not a causal model. Remember our motto from Chapter 1: No causation without manipulation. There is no way, for person \\(i\\), to change the years that she has been alive on Election Day. On the day of this election, she is X years old. So, there are not two (or more) potential outcomes. Without more than one potential outcome, there can not be a causal effect.\nGiven that, it is important to monitor our language. We do not believe that changes in election_age “cause” changes in lived_after. That is obvious. But there are some words and phrases — like “associated with” and “change by” — which are too close to causal. (And which we are guilty of using just a few paragraphs ago!) Be wary of their use. Always think in terms of comparisons when using a predictive model. We can’t change election_age for an individual candidate. We can only compare two candidates (or two groups of candidates).\n\n\nWarning in tidy.brmsfit(fit_age): some parameter names contain underscores:\nterm naming may be unreliable!\n\n\n# A tibble: 3 × 2\n  term            estimate\n  &lt;chr&gt;              &lt;dbl&gt;\n1 (Intercept)       72.6  \n2 election_age      -0.859\n3 sd__Observation   11.1  \n\n\n\nShow the codeggplot(ch7_a, aes(y = lived_after, x = election_age)) +\n  geom_point() +\n  geom_abline(intercept = candidate_intercept,\n              slope = candidate_slope, \n              color = \"#F8766D\", \n              size = 1) +\n  labs(title = \"Candidate Slope Model\",\n       x = \"Lifespan Post-Election\", \n       y = \"Age at Election\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n9.3.1.3 election_age and sex\nIn this model, our outcome variable continues to be lived_after, but now we will have two different explanatory variables: election_age and sex. Note that sex is a categorical explanatory variable and election_age is a continuous explanatory variable.\n\n\\[ lived\\_after_i =  \\beta_0 + \\beta_1 male_i + \\beta_2 c\\_election\\_age_i + \\epsilon_i \\] \nBut wait! The variable name is sex, not male. Where does male come from?\nThe answer is that male is an indicator variable, meaning a 0/1 variable. male takes a value of one if the candidate is “Male” and zero otherwise. This is the same as the \\(male_i\\) variable used in the previous two examples. An indicator variable signals that we have something that deals with a factor, category, or ennumerated type. While index variables are those that allow us to stack and align data, eliminating the need to assign specific columns for different variables\n\nThe outcome variable is \\(lived\\_after_i\\), the number of years a person is alive after the election. \\(male_i\\) is one of our explanatory variables. If we are predicting the number of years a male candidate lives after the election, this value will be 1. When we are making this prediction for female candidates, this value will be 0. \\(c\\_election\\_age_i\\) is our other explanatory variable. It is the number of years a candidate has lived before the election, scaled by subtracting the average number of years lived by all candidates.\n\\(\\beta_0\\) is the average number of years lived after the election for women, who on the day of election, have been alive the average number of years of all candidates (i.e. both male and female). \\(\\beta_0\\) is also the intercept of the equation. In other words, \\(\\beta_0\\) is the expected value of \\(lived\\_after_i\\), if \\(male_i = 0\\) and \\(c\\_election\\_age_i = 0\\).\n\\(\\beta_1\\) is almost meaningless by itself. The only time it has meaning is when its value is connected to our intercept (i.e. \\(\\beta_0 + \\beta_1\\)). When the two are added together, you get the average number of years lived after the election for males, who on the day of election, have been alive the average number of years for all candidates.\n\\(\\beta_2\\) is, for the entire population, the average difference in \\(lived\\_after_i\\) between two individuals, one of whom has an \\(c\\_election\\_age_i\\) value of 1 greater than the other.\n\nLet’s translate the model into code.\n\nfit_sex_age &lt;- brm(data = ch7_a,\n                   formula = lived_after ~ sex + election_age,\n                   silent = 2,\n                   refresh = 0,\n                   seed = 12)\n\nTrying to compile a simple C file\n\n\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nusing C compiler: ‘Apple clang version 15.0.0 (clang-1500.3.9.4)’\nusing SDK: ‘’\nclang -arch arm64 -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/opt/R/arm64/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c foo.c -o foo.o\nIn file included from &lt;built-in&gt;:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/Core:19:\n/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: 'cmath' file not found\n#include &lt;cmath&gt;\n         ^~~~~~~\n1 error generated.\nmake: *** [foo.o] Error 1\n\n\n\nprint(fit_sex_age, detail = FALSE)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: lived_after ~ sex + election_age \n   Data: ch7_a (Number of observations: 1092) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept       65.84      3.36    59.28    72.52 1.00     3640     3082\nsexMale          6.28      2.47     1.53    11.15 1.00     3982     2808\nelection_age    -0.85      0.04    -0.92    -0.77 1.00     3934     3193\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    11.09      0.23    10.64    11.55 1.00     4537     2991\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nLooking at our results, you can see that our intercept value is around 66. The average female candidate, who had been alive the average number of years of all candidates, would live another 66 years or so after the election.\nNote that sexMale is around 6. This is our coefficient, \\(\\beta_1\\). We need to connect this value to our intercept value to get something meaningful. Using the formula \\(\\beta_0 + \\beta_1\\), we find out that the number of years the average male candidate — who, on the day of election, is the average age of all candidates — would live is around 72 years.\nNow take a look at the coefficient for \\(c\\_election\\_age_i\\), \\(\\beta_2\\). The median of the posterior, -0.8, represents the slope of the model. When comparing two candidates who differ by one year in election_age, we expect that they will differ by -0.8 years in lived_after. It makes sense that this value is negative. The more years a candidate has lived, the fewer years the candidate has left to live. So, for every extra year a candidate is alive before an election, their lifespan after the election will be 0.8 years lower, on average.\nWe will now show you the parallel slopes model, which was created using the same process explained in the prior chapter. All we’ve done here is extracted the values for our intercepts and slopes, and separated them into two groups. This allows us to create a geom_abline object that takes a unique slope and intercept value, so we can separate the male and female observations.\n\n\nWarning in tidy.brmsfit(fit_sex_age): some parameter names contain underscores:\nterm naming may be unreliable!\n\n\n# A tibble: 4 × 2\n  term            estimate\n  &lt;chr&gt;              &lt;dbl&gt;\n1 (Intercept)       65.8  \n2 sexMale            6.28 \n3 election_age      -0.846\n4 sd__Observation   11.1  \n\n\n\nShow the codeggplot(ch7_a, aes(y = lived_after, x = election_age, color = sex)) +\n  \n  # Use geom_point to show the datapoints. \n  \n  geom_point() +\n  \n  # Create a geom_abline object for the liberal false values. Set the intercept\n  # equal to our previously created female_intercept, while setting slope\n  # equal to our previously created female_slope. The color call is\n  # for coral, to match the colors used by tidyverse for geom_point().\n  \n  geom_abline(intercept = female_intercept,\n              slope = female_slope, \n              color = \"#F8766D\", \n              size = 1) +\n  \n  # Create a geom_abline object for the liberal TRUE values. Set the intercept\n  # equal to our previously created sex_male_intercept, while setting slope\n  # equal to our previously created sex_male_att_slope. The color call is\n  # for teal, to match the colors used by tidyverse for geom_point().\n\n  geom_abline(intercept = sex_male_intercept,\n              slope = sex_male_att_slope,\n              color = \"#00BFC4\", \n              size = 1) +\n  \n  # Add the appropriate titles and axis labels. \n  \n  labs(title = \"Parallel Slopes Model\",\n       x = \"Lifespan Post-Election\", \n       y = \"Age at Election\", \n       color = \"Sex\") +\n  theme_classic()\n\n\n\n\n\n\n\nThe posterior for male/female years lived shows a huge gap between the two values. We want to take into factor the prior knowledge that we have regarding female and male lived after ages. The females tend to live longer than the males based on our prior knowledge which presents a problem with the graph that we have above. With our graph we have a higher tendency of the male candidates living longer simply because of the amount of data that we have. We have more data for the male candidates running for election because they tend to run for election more than the female candidates which in turn skews the lived after predictions that we can make for both sorts of candidates.\n\n9.3.1.4 election_age, sex and election_age*sex\nLet’s create another model. This time, however, the numeric outcome variable of lived_after is a function of the two explanatory variables we used above, election_age and sex, and of their interaction. To look at interactions, we need 5 parameters, which is why we needed to wait until this chapter to introduce the concept.\n\n\\[ lived\\_after_i =  \\beta_0 + \\beta_1 male_i + \\beta_2 c\\_election\\_age_i +\n\\\\ \\beta_3 male_i *  c\\_election\\_age_i + \\epsilon_i \\]\n\nOur outcome variable is still \\(lived\\_after_i\\). We want to know how many years a candidate will live after an election. Our explanatory variables as the same as before. \\(male_i\\) is one for male candidates and zero for female candidates. \\(c\\_election\\_age_i\\) the number of years a candidate has lived before the election, relative to the average value for all candidates. In this model, we have a third predictor variable: the interaction between \\(male_i\\) and \\(c\\_election\\_age_i\\).\n\\(\\beta_0\\) is the average number of years lived after the election for women, who on the day of election, have been alive the average number of years of all candidates. In a sense, this is the same meaning as in the previous model, without an interaction term. But, always remember that the meaning of a parameter is conditional on the model in which it is embedded. Even if a parameter is called \\(\\beta_0\\) in two different regressions does necessitate that it means the same thing in both regressions. Parameter names are arbitrary, or at least simply a matter of convention.\n\\(\\beta_1\\) does not have a simple interpretation as a stand-alone parameter. It is a measure of how different women are from men. However, \\(\\beta_0 + \\beta_1\\) has a straightforward meaning exactly analogous to the meaning of \\(\\beta_0\\). The sum is the average number of years lived after the election for men, who on the day of election, have been alive the average number of years of all candidates.\n\\(\\beta_2\\) is the coefficient of \\(c\\_election\\_age_i\\). It it just the slope for women. It is the average difference in \\(lived\\_after_i\\) between two women, one of whom has an \\(c\\_election\\_age_i\\) value of 1 greater than the other. In our last example, \\(\\beta_2\\) was the slope for the whole population. Now we have different slopes for different genders.\n\\(\\beta_3\\) alone is difficult to interpret. However, when it is added to \\(\\beta_2\\), the result in the slope for men.\n\nWith the help of Courage we can translate all of the math from above into code.\n\nfit_all &lt;- brm(data = ch7_a,\n               formula = lived_after ~ sex*election_age,\n               silent = 2,\n               refresh = 0,\n               seed = 13)\n\nTrying to compile a simple C file\n\n\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nusing C compiler: ‘Apple clang version 15.0.0 (clang-1500.3.9.4)’\nusing SDK: ‘’\nclang -arch arm64 -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/opt/R/arm64/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c foo.c -o foo.o\nIn file included from &lt;built-in&gt;:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/Core:19:\n/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: 'cmath' file not found\n#include &lt;cmath&gt;\n         ^~~~~~~\n1 error generated.\nmake: *** [foo.o] Error 1\n\n\n\nprint(fit_all, detail = FALSE)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: lived_after ~ sex * election_age \n   Data: ch7_a (Number of observations: 1092) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept               18.97     20.76   -22.53    59.27 1.00     1468\nsexMale                 53.63     20.82    13.62    94.63 1.00     1460\nelection_age            -0.05      0.35    -0.72     0.65 1.00     1438\nsexMale:election_age    -0.81      0.35    -1.50    -0.13 1.00     1443\n                     Tail_ESS\nIntercept                1707\nsexMale                  1634\nelection_age             1757\nsexMale:election_age     1778\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    11.07      0.24    10.61    11.55 1.00     1879     2259\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe intercept has increased. \\(\\beta_0\\) is around 19. This is the intercept for females. It still means the average number of years lived after the election for women is 19 or so. Our sexMale coefficient, \\(\\beta_1\\), refers to the value that must be added to the intercept in order to get the average for males. When calculated, the result is 73. Keep in mind, however, that these values only apply if \\(c\\_election\\_age_i = 0\\), if, that is, candidate \\(i\\) is around 52 years old.\nThe coefficient for \\(c\\_election\\_age_i\\), \\(\\beta_2\\), is -0.1. What does this mean? It is the slope for females. So, when comparing two female candidates who differ by one year in age, we expect that the older candidate will live 0.1 years less. Now direct your attention below at the coefficient of sexMale:election_age, \\(\\beta_3\\), which is -0.8. This is the value that must be added to the coefficient of \\(c\\_election\\_age_i\\) (recall \\(\\beta_2 + \\beta_3\\)) in order to find the slope for males. When the two are added together, this value, or slope, is about -0.9. When comparing two male candidates who differ in age by one year, we expect the older candidate to live about 0.9 years less.\nKey point: The interpretation of the intercepts only apply to candidates for whom \\(c\\_election\\_age_i = 0\\). Candidates who are not 52 years-old will have a different expected number of years to live. The interpretation of the slope applies to everyone. In other words, the relationship between \\(lived\\_after_i\\) and \\(c\\_election\\_age_i\\) is the same, regardless of your gender or how old you are.\nMale candidates live longer on average than female candidates. Note, also, that the average years to live after the election for females is about 19 with this model. With the previous model, it was 66 years. Why the difference? The interpretation of “average” is different! In the previous model, it was the average for all women. In this model, it is the average for all 52 years-old women. Those are different things, so we should hardly be surprised by different posteriors.\n\n9.3.2 Tests\n“Tests,” “testing,” “hypothesis tests,” “tests of significance,” and “null hypothesis significance testing” all refer to the same concept. We will refer to this collection of approaches as NHST, a common abbreviation derived from the initials of the last phrase. Wikipedia provides an overview.\nIn hypothesis testing, we have a null hypothesis — this hypothesis represents a particular probability model. We also have an alternative hypothesis, which is typically the alternative to the null hypothesis. Let’s look at an example that is unrelated to statistics first.\nImagine a criminal trial held in the United States. Our criminal justice system assumes “the defendant is innocent until proven guilty.” That is, our initial assumption is that the defendant is innocent.\nNull hypothesis (\\(H_0\\)): Defendent is not guilty (innocent) Alternative hypothesis (\\(H_a\\)): Defendant is guilty\nIn statistics, we always assume the null hypothesis is true. That is, the null hypothesis is always our initial assumption.\nWe then collect evidence — such as finger prints, blood spots, hair samples — with the hopes of finding “sufficient evidence” to make the assumption of innocence refutable.\nIn statistics, the data are the evidence.\nThe jury then makes a decision based on the available evidence:\nIf the jury finds sufficient evidence — beyond a reasonable doubt — to make the assumption of innocence refutable, the jury rejects the null hypothesis and deems the defendant guilty. We behave as if the defendant is guilty. If there is insufficient evidence, then the jury does not reject the null hypothesis. We behave as if the defendant is innocent.\nIn statistics, we always make one of two decisions. We either reject the null hypothesis or we fail to reject the null hypothesis. Rather than collect physical evidence, we test our hypothesis in our model. For example, say that we have a hypothesis that a certain parameter equals zero. The hypotheses are:\n\\(H_0\\): The parameter equals 0. \\(H_a\\): The parameter does not equal 0.\nThe hypothesis that a parameter equals zero (or any other fixed value) can be directly tested by fitting the model that includes the parameter in question and examining the corresponding 95% interval. If the 95% interval excludes zero (or the specified fixed value), then the hypothesis is said to be rejected. If the 95% interval inclues zero, we do not reject the hypothesis. We also do not accept the hypothesis.\nIf this sounds nonsensical, it’s because it is. Our view: Amateurs test. Professionals summarize.\nA Yes/No question throws away too much information to (almost) ever be useful. There is no reason to test when you can summarize by providing the full posterior probability distribution.\nThe same arguments apply in the case of “insignificant” results when we can’t “reject” the null hypothesis. In simple terms: who cares!? We have the full posterior probability distribution for that prediction — also known as the posterior predictive distribution — as graphed above. The fact that result is not “significant” has no relevance to how we use the posterior to make decisions.\nThe same reasoning applies to every parameter we estimate, to every prediction we make. Never test — unless your boss demands a test. Use your judgment, make your models, summarize your knowledge of the world, and use that summary to make decisions.\nWith the fitted model that we have created we are able to perform model checks. Model checks help us understand how accurate our model is to ensure that the fitted model that we have created is reasonably accurate when compared to the actual data. We can view our model through the posterior predictive check that simulates the data upon our fitted model to generate a distribution. With the posterior predictive check we are able to visualize how accurate our data is compared to the actual data ensuring that we have created a great fitted model.\n\npp_check(fit_all, plotfun = \"hist\", nreps = 3, binwidth = 1)\n\nUsing 10 posterior draws for ppc type 'dens_overlay' by default.\n\n\nWarning: The following arguments were unrecognized and ignored: plotfun, nreps,\nbinwidth\n\n\n\n\n\n\n\n\nOur graph in the darker blue represents our actual data. As we can see with the lighter blue graph, our fitted model is able to generate a distribution that is similar when compared to the actual data. However, the “fake-data” produces some values for longevity which are impossible. One important item that we want to note is that for the actual data there is no value that is under 0 or over 60 for longevity of a candidate, however, in the fitted model we are able to see several generated values for which the longevity of a candidate is under 0 or over 60. We know that the longevity of a person can not decrease below 0. This is a flaw in our model. Is it a serious flaw? That is tough to decide. For the most part though our fitted model does a great job in generating a distribution through the “fake-data” simulation when compared to the actual data set.\n\n9.3.3 Data Generating Mechanism\nRecall the parallel slopes model that we created in ?sec-four-parameters. Another visualization we can create, one that also uses slopes and intercepts for our model, is the interaction model. In this model, the slopes for our two groups are different, creating a non-parallel visualization.\nThe process for creating the interaction model is similar to creating the parallel slopes model. Let us begin the same way — by tidying our data and inspecting it.\n\nShow the code# First, we will tidy the data from our model and select the term and estimate.\n# This allows us to create our regression lines more easily.\n\ntidy &lt;- fit_all |&gt; \n  tidy() |&gt; \n  select(term, estimate)\n\nWarning in tidy.brmsfit(fit_all): some parameter names contain underscores:\nterm naming may be unreliable!\n\nShow the codetidy\n\n# A tibble: 5 × 2\n  term                 estimate\n  &lt;chr&gt;                   &lt;dbl&gt;\n1 (Intercept)           19.0   \n2 sexMale               53.6   \n3 election_age          -0.0501\n4 sexMale:election_age  -0.805 \n5 sd__Observation       11.1   \n\n\nAfter tidying our data, we will extract values and assign sensible names for later use. Note that this is identical to the process from ?sec-four-parameters, with the addition of a fourth term (the interaction term):\n\nShow the code# Extract and name the columns of our tidy object. By calling tidy$estimate[1],\n# we are telling R to extract the first value from the estimate column in our\n# tidy object.\n\nintercept &lt;- tidy$estimate[1]\nsex_male &lt;- tidy$estimate[2]\nelection_age &lt;- tidy$estimate[3]\ninteraction_term &lt;- tidy$estimate[4]\n\n\nNow that we have extracted our values, we will create the intercept and slope values for our two different groups, females and males. Recall the following details about finding slopes and intercepts in an interaction model:\n\nThe intercept is the intercept for females. It represents the average number of years lived after the election for females.\nOur sexMale coefficient refers to the value that must be added to the intercept in order to get the average years lived post-election for males.\nThe coefficient for \\(c\\_election\\_age_i\\) is the slope for females.\nThe coefficient of sexMale:election_age is the value that must be added to the coefficient of \\(c\\_election\\_age_i\\) in order to find the slope for males.\n\n\nShow the code# Recall that the intercept and the estimate for election_age act as the\n# estimates for female candidates only. Accordingly, we have assigned those\n# values (from the previous code chunk) to more sensible names: female_intercept\n# and female_slope.\n\nfemale_intercept &lt;- intercept\nfemale_slope &lt;- election_age\n\n# To find the male intercept, we must add the intercept for the estimate for\n# sex_male. To find the male slope, we must add election_age to our\n# interaction term estimate.\n\nmale_intercept &lt;- intercept + sex_male\nmale_slope &lt;- election_age + interaction_term\n\n\nAfter creating objects for our different intercepts and slopes, we will now create the interaction model using geom_abline() for a male and female line.\n\nShow the code# From the ch7_a data, create a ggplot object with election_age as the x-axis\n# and lived_after as the y-axis. We will use color = sex.\n\nggplot(ch7_a, aes(x = election_age, y = lived_after, color = sex)) +\n  \n  # Use geom_point to show the datapoints. \n  \n  geom_point() +\n  \n  # Create a geom_abline object for the female intercept and slope. Set the\n  # intercept qual to our previously created female_intercept, while setting\n  # slope equal to our previously created female_slope. The color call is for\n  # coral, to match the colors used by tidyverse for geom_point().\n  \n  geom_abline(intercept = female_intercept,\n              slope = female_slope, \n              color = \"#F8766D\", \n              size = 1) +\n  \n  # Create a geom_abline object for the male values. Set the intercept equal to\n  # our previously created male_intercept, while setting slope equal to our\n  # previously created male_slope. The color call is for teal, to match the\n  # colors used by tidyverse for geom_point().\n\n  geom_abline(intercept = male_intercept,\n              slope = male_slope,\n              color = \"#00BFC4\", \n              size = 1) +\n  \n  # Add the appropriate titles and axis labels. \n  \n  labs(title = \"Interaction Model\",\n       subtitle = \"Comparing post election lifespan across sex\",\n       x = \"Average Age at Time of Election\", \n       y = \"Years Lived Post-Election\", \n       color = \"Sex\") +\n  theme_classic()\n\n\n\n\n\n\n\nThis is our final interaction model! There are some interesting takeaways. First, we may note that there are far fewer data points for female candidates — a concern we previously mentioned. It makes sense, then, that the slope would be less dramatic when compared with male candidates. We also see that most female candidates run when they are older, as compared with male candidates. This might explain why our intercept for years lived post-election is lower for female candidates.\nThe male line seems more sensible, as we might expect with far more data points. For male candidates, we see a clear (logical) pattern: the older candidates are at the time of election, the less years post-election they live. This makes sense, as we are limited by the human lifespan.\nWe have decided that fit_all is our data generating mechanism. Let’s show its main components in a nice looking table with the help of the gtsummary package. First, load the necessary packages:\n\nlibrary(broom.mixed)\nlibrary(gtsummary)\n\nSecond, create the table:\n\nfit_all |&gt; \n  tbl_regression()\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n\n95% CI1\n\n\n\n\nsex\n\n\n\n\n    sexMale\n54\n14, 95\n\n\nelection_age\n-0.05\n-0.72, 0.65\n\n\nsex * election_age\n\n\n\n\n    sexMale * election_age\n-0.81\n-1.5, -0.13\n\n\n\n\n1 CI = Credible Interval",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Five Parameters</span>"
    ]
  },
  {
    "objectID": "five-parameters.html#temperance",
    "href": "five-parameters.html#temperance",
    "title": "9  Five Parameters",
    "section": "\n9.4 Temperance",
    "text": "9.4 Temperance\n\n\n\n\n\n\n\n\n\nTemperance uses the Data Generating Mechanism to answer the question with which we began. Humility reminds us that this answer is always a lie.\n\n\n9.4.1 Questions and Answers\nRecall the questions with which we began the chapter:\n\nHow long do political candidates live after the election?\n\nBecause our model for longevity includes age and sex, we need to specify the values for those variables if we want to look at specific cases. So:\n\nHow many years would we expect two gubernatorial candidates — one male and one female, both 10 years older than the average candidate — to live after the election?\n\nThese questions are, purposely, less precise than the ones we tackled in earlier chapters, written more in a conversational style. This is how normal people talk.\nHowever, as data scientists, our job is to bring precision to these questions. There are two commonsense interpretations. First, we could be curious about the expected values for these questions. If we averaged the data for a thousand candidates like these, what would the answer be? Second, we could be curious about two specific individuals. How long will they live? Averages involve questions about parameters. The fates of individuals require predictions. Those are general claims, violated too often to be firm rules. Yet, they highlight a key point: expected values are less variable than individual predictions.\nTo calculate expected values, use posterior_epred(). To forecast for individuals, use posterior_predict().\nConsider the “on average” interpretation first. The answer begins with the posterior distributions of the parameters in fit_all.\n\nFirst we want to take a look at our mathematics once again:\n\\[ lived\\_after_i =  \\beta_0 + \\beta_1 male_i + \\beta_2 c\\_election\\_age_i + \\\\\n\\beta_3 male_i *  c\\_election\\_age_i + \\epsilon_i \\]\n\nnewobs = tibble(sex = c(\"Male\", \"Female\"), \n                 election_age = 50)\n\nWith newobs we are want to generate a sample of what a scenario may look like for a candidate’s longevity. Here we establish the two sexes and the election age that we want to deal with for our candidates.\n\nfit_all |&gt; \n  add_epred_draws(newdata = newobs)\n\n# A tibble: 8,000 × 7\n# Groups:   sex, election_age, .row [2]\n   sex   election_age  .row .chain .iteration .draw .epred\n   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;\n 1 Male            50     1     NA         NA     1   29.7\n 2 Male            50     1     NA         NA     2   29.9\n 3 Male            50     1     NA         NA     3   30.0\n 4 Male            50     1     NA         NA     4   29.9\n 5 Male            50     1     NA         NA     5   29.9\n 6 Male            50     1     NA         NA     6   29.3\n 7 Male            50     1     NA         NA     7   29.2\n 8 Male            50     1     NA         NA     8   29.2\n 9 Male            50     1     NA         NA     9   30.0\n10 Male            50     1     NA         NA    10   29.8\n# ℹ 7,990 more rows\n\n\nWhen we put this in terms of our mathematics the \\(beta_1\\) becomes 1 as our \"Male\" term corresponds to 1. The code that we have above is able to use our fitted model from above to generate a similar distribution of longevity for our candidates with the possiblity of how long they may live and whether male candidates will live longer than female candidates.\n\nShow the codefit_all |&gt; \n  add_epred_draws(newdata = newobs) |&gt; \n  ggplot(aes(.epred, fill = sex)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior for Expected Years Lived Post-Election\",\n         subtitle = \"Male candidates live longer\",\n         x = \"Years\",\n         y = \"Probability\") + \n    scale_x_continuous(labels = \n                         scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = \n                         scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\n\n\n\n\nLooking at our posterior probability distributions above, we can see that male candidates are expected to live longer. But how much longer? Does the initial age affect the posterior for expected years after the candidate’s election? Based on our previous knowledge we know that females tend to live longer than the males which is surprising. The more data we have, the more narrow the posteriors are which we can create. Our data includes a great percentage of male candidates that run for election which is why we are more accurate within our posterior check for male candidates and their longevity. As in previous chapters, we can manipulate distributions in, more or less, the same way that we manipulate simple numbers. If we want to know the difference between two posterior distributions, we can simply subtract.\n\n9.4.2 Humility\nWe can never know the truth.\n\nWhen we end Temperance we want to end with the Preceptor’s Posterior. The Preceptor’s Posterior is the posterior you would calculate if all the assumptions you made under Wisdom and Justice were correct. With all of the assumptions that we have analyzed and taken a look at, we can see that there are several possibilities where this may not hold true. Most of the times they never are! So, you can never know the Preceptor’s Posterior. In turn we hope that our posterior will be a close approximation of the Preceptor’s Posterior.\n\n\n\n\nBarfort, Sebastian, Robert Klemmensen, and Erik Gahner Larsen. 2020. “Longevity Returns to Political Office.” Political Science Research and Methods. https://doi.org/10.1017/psrm.2019.63.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Five Parameters</span>"
    ]
  },
  {
    "objectID": "n-parameters.html",
    "href": "n-parameters.html",
    "title": "10  N Parameters",
    "section": "",
    "text": "10.1 Wisdom\nWisdom\nAs you research ways to increase voting, you come across a large-scale experiment showing the effect of sending out a voting reminder that “shames” citizens who do not vote. Perhaps you should pay to send out a shaming voting postcard.\nWe will be looking at the shaming tibble from the primer.data package, sourced from “Social Pressure and Voter Turnout: Evidence from a Large-Scale Field Experiment” (pdf) by Gerber, Green, and Larimer (2008). Familiarize yourself with the data by typing ?shaming.\nRecall our initial question: how can we encourage voters to go out to the polls on election day? We now need to translate this into a more precise question, one that we can answer with data.\nWisdom requires the creation of a Preceptor Table, an examination of our data, and a determination, using the concept of validity, as to whether or not we can (reasonably!) assume that the two come from the same population.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>N Parameters</span>"
    ]
  },
  {
    "objectID": "n-parameters.html#wisdom",
    "href": "n-parameters.html#wisdom",
    "title": "10  N Parameters",
    "section": "",
    "text": "What is the causal effect of postcards on voting?\n\n\n\n10.1.1 Preceptor Table\nCausal or predictive model: The phrasing of the question — especially the phrase “causal effect” — makes it obvious that we need a causal, rather than a predictive, model. There will be at least two potential outcomes for every unit.\nUnits: The units of our Preceptor Table are individual voters in Texas around the time of the next election.\nOutcome: The outcome is voting or not voting. That is, we are researching the question of how to convince people to vote, not how to convince them to vote for a specific candidate. This project is about getting our voters to the polls, not about convincing their voters to switch to our side.\nTreatment: The treatments are postcards of various types.\nCovariates: The covariates include, at least, measures of political engagement, since that attribute was mentioned in the initial question. There may be other covariates as well. We can’t get too detailed until we look at the data.\nMoment in Time: We care about the upcoming Texas election.\nThe Preceptor Table is the smallest possible table such that, if no data were missing, the calculation of all quantities of interest would be trivial. The causal effect, for each individual, is the difference in voting behavior under treatment versus control. Consider:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\nOutcomes\nCovariates\n\n\nVoting After Control\nVoting After Treatment\nTreatment\nEngagement\n\n\n\n\n1\n1\n1\nYes\n1\n\n\n2\n0\n1\nNo\n3\n\n\n…\n…\n…\n…\n…\n\n\n10\n1\n1\nYes\n6\n\n\n11\n1\n0\nYes\n2\n\n\n…\n…\n…\n…\n…\n\n\nN\n0\n1\nNo\n2\n\n\n\n\n\n\n\nComments:\n\nThe more experience you get as a data scientist, the more that you will come to understand that the Four Cardinal Virtues are not a one-way path. Instead, we circle round and round. Our initial question, instead of being fixed forever, is modified as we learn more about the data, and as we experiment with different modeling approaches.\nIndeed, the politician (our boss/client) running for Texas governor probably did not start out with that question. After all, his main question is “How do I win this election?” That big picture goal would lead, over and over again, to more specific questions regarding strategy, including how best to motivate his supporters to vote.\nThe same iterative approach applies to the Preceptor Table. The above is just our first version. Once we look more closely at the data, we will discover that there are multiple treatments. In fact, we have a total of 5 potential outcomes in this example.\nShould we go back and change the Preceptor Table? No! The Preceptor Table is just a tool we use to fix ideas in our own heads. We never talk about Preceptor Tables with anyone else. Indeed, you won’t ever see the words “Preceptor Table” outside of this Primer and related work.\nWhy is there only one covariate (in addition to the treatment)? Because the question only specifies that one. Any covariates mentioned in the question must be included in the Preceptor Table because, without that information for all units, we can’t answer the question. In any real example, we will have other covariates which we might, or might not, include in the model.\nThe outcomes in a Preceptor Table refer to a moment in time. In this case, the outcome occurs on Election Day in 2026. The covariates and treatments must be measured before the outcome, otherwise they can’t be modeled as connected with the outcome.\nWe can update our question:\n\n\nWhat is the causal effect of postcards on voting in the 2026 Texas gubernatorial election? Do those effects vary by political engagement?\n\nAs usual, our question becomes both more and less precise simultaneously. It is more precise in that we only care about a very specific moment in space and time: the election for governor in Texas in 2026. It is less precise in that we are almost always interested in different aspects of the problem, mainly because of the decisions we must make.\nIn this case, we might not have enough money to send postcards to every voter. Maybe we should focus in politically engaged voters. Maybe we should focus on other subsets. We should probably only send postcards to voters who are Republican or are likely to vote Republican, unless, that is, some postcards actually decrease someone’s proposensity to vote. The real world is full of complexity.\n\n10.1.2 EDA of shaming\n\nAfter loading the packages we need, let’s perform an EDA, starting off by running glimpse() on the shaming tibble from the primer.data package.\n\nShow the codelibrary(tidyverse)\nlibrary(primer.data)\nlibrary(ggthemes)\nlibrary(ggdist)\nlibrary(broom.mixed)\nlibrary(gtsummary)\n\n\n\nShow the codeglimpse(shaming)\n\nRows: 344,084\nColumns: 15\n$ cluster       &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"…\n$ primary_06    &lt;int&gt; 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,…\n$ treatment     &lt;fct&gt; Civic Duty, Civic Duty, Hawthorne, Hawthorne, Hawthorne,…\n$ sex           &lt;chr&gt; \"Male\", \"Female\", \"Male\", \"Female\", \"Female\", \"Male\", \"F…\n$ age           &lt;int&gt; 65, 59, 55, 56, 24, 25, 47, 50, 38, 39, 65, 61, 57, 37, …\n$ primary_00    &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"N…\n$ general_00    &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"…\n$ primary_02    &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"…\n$ general_02    &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"…\n$ primary_04    &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"N…\n$ general_04    &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", …\n$ hh_size       &lt;int&gt; 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1,…\n$ hh_primary_04 &lt;dbl&gt; 0.0952381, 0.0952381, 0.0476190, 0.0476190, 0.0476190, 0…\n$ hh_general_04 &lt;dbl&gt; 0.8571429, 0.8571429, 0.8571429, 0.8571429, 0.8571429, 0…\n$ neighbors     &lt;int&gt; 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, …\n\n\nglimpse() gives us a look at the raw data contained within the shaming data set. At the very top of the output, we can see the number of rows and columns, or observations and variables, respectively. We see that there are 344,084 observations, with each row corresponding to a unique respondent. ?shaming provides more details about the variables.\nVariables of particular interest to us are sex, hh_size, and primary_06. The variable hh_size tells us the size of the respondent’s household, sex tells us the sex of the respondent, and primary_06 tells us whether or not the respondent voted in the 2006 primary election.\nThere are a few things to note while exploring this data set. You may – or may not – have noticed that the only response to the general_04 variable is “Yes”. In their published article, the authors note that “Only registered voters who voted in November 2004 were selected for our sample.” After this, the authors found their history then sent out the mailings. Thus, anyone who did not vote in the 2004 general election is excluded, by definition. Keep that fact in mind when we discuss representativeness below.\nThe dependent variable is primary_06, which is coded either 1 or 0 for whether or not the respondent voted in the 2006 primary election. This is the dependent variable because the authors are trying to measure the effect that the treatments have on voting behavior in the 2006 general election. It is also the dependent variable from our point of view since our question also deals with voting behavior.\nThe treatment variable is a factor variable with 5 levels, including the control. Our focus is on those treatments, and on their causal effects, if any. Consider:\n\nShow the codeshaming |&gt;\n  count(treatment)\n\n# A tibble: 5 × 2\n  treatment        n\n  &lt;fct&gt;        &lt;int&gt;\n1 No Postcard 191243\n2 Civic Duty   38218\n3 Hawthorne    38204\n4 Self         38218\n5 Neighbors    38201\n\n\nFour types of treatments were used in the experiment, with voters receiving one of the four types of mailing. All of the mailing treatments carried the message, “DO YOUR CIVIC DUTY - VOTE!”.\nThe first treatment, Civic Duty, also read, “Remember your rights and responsibilities as a citizen. Remember to vote.” This message acted as a baseline for the other treatments, since it carried a message very similar to the one displayed on all the mailings.\nIn the second treatment, Hawthorne, households received a mailing which told the voters that they were being studied and their voting behavior would be examined through public records. This adds a small amount of social pressure.\nIn the third treatment, Self, the mailing includes the recent voting record of each member of the household, placing the word “Voted” next to their name if they did in fact vote in the 2004 election or a blank space next to the name if they did not. In this mailing, the households were also told, “we intend to mail an updated chart” with the voting record of the household members after the 2006 primary. By emphasizing the public nature of voting records, this type of mailing exerts more social pressure on voting than the Hawthorne treatment.\nThe fourth treatment, Neighbors, provides the household members’ voting records, as well as the voting records of those who live nearby. This mailing also told recipients, “we intend to mail an updated chart” of who voted in the 2006 election to the entire neighborhood. Consider:\n\nShow the codeshaming |&gt; \n  sample_frac(0.05) |&gt; \n  ggplot(aes(x = treatment, y = primary_06)) +\n    geom_jitter(alpha = 0.03, height = 0.1) +\n    scale_y_continuous(breaks = c(0, 1), labels = c(\"No\", \"Yes\")) +\n    labs(title = \"Postcards and Voting Behavior in Michigan\",\n         subtitle = \"Most residents did not receive a postcard.\",\n         x = \"Postcard Type Sent to Voter\",\n         y = \"Voted in 2006 Primary Election\",\n         caption = \"Random sample of 5% of the data from Gerber, Green, and Larimer (2008)\")\n\n\n\n\n\n\n\nThis graphic does not tell us as much as we might like. Most residents did not receive a postcard. Those who did get a postcard had about the same chance of receiving any of the four types of postcard. Since the lower bars are darker, more people in the sample did not vote in the election than did vote. However, we can’t really see, much less estimate, a causal effect from this graphic.\n\nThe original question mentions “civic engagement.” Although shaming does not include any variable exactly like this, it does include voting history. Consider:\n\nx &lt;- shaming |&gt; \n    \n  # Converting the Y/N columns to binaries with the function we made \n  # note that primary_06 is already binary and also that we don't \n  # need it to construct a variable based on previous voting behavior.\n  \n  mutate(p_00 = (primary_00 == \"Yes\"),\n         p_02 = (primary_02 == \"Yes\"),\n         p_04 = (primary_04 == \"Yes\"),\n         g_00 = (general_00 == \"Yes\"),\n         g_02 = (general_02 == \"Yes\"),\n         g_04 = (general_04 == \"Yes\")) |&gt; \n  \n  # A sum of the voting action records across the election cycle columns gives\n  # us an idea of the voters general level of civic involvement.\n  \n  mutate(civ_engage = p_00 + p_02 + p_04 + \n                      g_00 + g_02 + g_04) |&gt; \n  \n  # If you look closely at the data, you will note that g_04 is always Yes, so\n  # the lowest possible value of civ_engage is 1. The reason for this is that\n  # the sample was created by starting with a list of everyone who voted in the\n  # 2004 general election. Note how that fact makes the interpretation of the\n  # relevant population somewhat subtle.\n  \n  mutate(voter_class = case_when(civ_engage %in% c(5, 6) ~ \"Always Vote\",\n                                 civ_engage %in% c(3, 4) ~ \"Sometimes Vote\",\n                                 civ_engage %in% c(1, 2) ~ \"Rarely Vote\"),\n         voter_class = factor(voter_class, levels = c(\"Rarely Vote\", \n                                                      \"Sometimes Vote\", \n                                                      \"Always Vote\"))) |&gt; \n  \n  # Centering and scaling the age variable. Note that it would be smart to have\n  # some stopifnot() error checks at this point. For example, if civ_engage &lt; 1\n  # or &gt; 6, then something has gone very wrong.\n  \n  mutate(age_z = as.numeric(scale(age))) |&gt; \n  \n  # voted in a more natural name for our outcome variable\n  \n  rename(voted = primary_06) |&gt; \n  \n  # Just keep the variables that we will be using later.\n  \n  select(voted, treatment, sex, age_z, civ_engage, voter_class) |&gt; \n  \n  # There are only a handful of rows with missing data, so it can't matter much\n  # if we drop them.\n  \n  drop_na()\n\nRead all those comments! Your code should be commented just as thoroughly. Rough guideline: Have as many lines of comments as you have lines of code.\nOf course, when doing the analysis, you don’t know when you start what you will be using at the end. Data analysis is a circular process. We mess with the data. We do some modeling. We mess with the data on the basis of what we learned from the models. With this new data, we do some more modeling. And so on. Consider:\n\nShow the codex |&gt;\n  sample_frac(0.05) |&gt; \n  ggplot(aes(x = civ_engage, y = voted)) +\n    geom_jitter(alpha = 0.03, height = 0.1) +\n    scale_x_continuous(breaks = 1:6) + \n    scale_y_continuous(breaks = c(0, 1), labels = c(\"No\", \"Yes\")) +\n    labs(title = \"Civic Engagement and Voting Behavior in Michigan\",\n         subtitle = \"Past voting predicts future voting.\",\n         x = \"Civic Engagement\",\n         y = \"Voted in 2006 Primary Election\",\n         caption = \"Random sample of 5% of the data from Gerber, Green, and Larimer (2008)\")\n\n\n\n\n\n\n\nAlthough this plot is pleasing, we need to create an actual model with this data in order to answer our questions.\n\n10.1.3 Validity\nValidity is the consistency, or lack there of, in the columns of your data set and the corresponding columns in your Preceptor Table. In order to consider the two data sets to be drawn from the same population, the columns from one must have a valid correspondence with the columns in the other. Validity, if true (or at least reasonable), allows us to construct the Population Table, which is the first step in Justice. Consider two counter-arguments to the assumption of validity in this case.\nFirst, voting in a primary election in 2006 in Michigan is not the same thing as voting in a general election in Texas in 2026. Primary elections are less newsworthy and, usually, less competitive. Our client, the gubernatorial candidate, has already won his primary. The outcome he cares about is voting in a general elections. Treatments which might, or might not, affect participation in a primary are irrelevant. Moreover, the political culture of Texas and Michigan are very different. The meaning and importance of voting is not identical across states, nor over time.\nSecond, our client is interested in using a measure of civic engagement. We build that measure out of past voting behavior. Right now, we aren’t even sure that we can access this data in Texas. And, even if we can, the salience of those elections in the years before 2026 will differ from those in Michigan in the years before 2006.\nFortunately, at least for our continued use of this example, we will assume that validity holds. The outcome variable in our data and in our Preceptor Table are close enough — even though one is for a primary election while the other is for a general election — that we can just stack them.\n\nUsing the results of a voting experiment in Michigan in 2006, we seek to forecast the causal effect on voter participation of sending postcards in the Texas gubernatorial general election of 2026.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>N Parameters</span>"
    ]
  },
  {
    "objectID": "n-parameters.html#justice",
    "href": "n-parameters.html#justice",
    "title": "10  N Parameters",
    "section": "\n10.2 Justice",
    "text": "10.2 Justice\n\n\n\n\nJustice\n\n\n\nJustice includes the creation of the Population Table, followed by a discussion about the assumptions of stability, representativeness and unconfoundedness.\n\n10.2.1 The Population Table\nThe Population Table shows rows from three sources: the Preceptor Table, the data, and the population from which the rows in both the Preceptor Table and the data were drawn.\nThe better we get at data science, the less that we sweat the details of the Population Table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource\nSex\nYear\nState\nOutcomes\n\n\nTreatment\nControl\n\n\n\n\n…\n?\n1990\n?\n?\n?\n\n\n…\n?\n1995\n?\n?\n?\n\n\n…\n…\n…\n…\n…\n…\n\n\nData\nMale\n2006\nMichigan\nDid not vote\n?\n\n\nData\nFemale\n2006\nMichigan\n?\nVoted\n\n\n…\n…\n…\n…\n…\n…\n\n\n…\n?\n2010\n?\n?\n?\n\n\n…\n?\n2012\n?\n?\n?\n\n\n…\n…\n…\n…\n…\n…\n\n\nPreceptor Table\nFemale\n2026\nTexas\n?\n?\n\n\nPreceptor Table\nFemale\n2026\nTexas\n?\n?\n\n\n…\n…\n…\n…\n…\n…\n\n\n…\n?\n2046\n?\n?\n?\n\n\n…\n?\n2050\n?\n?\n?\n\n\n\n\n\n\n\nDoes the underlying population go back to 1990, or only 1995? Does it include elections in 2040? How about 2050? Are states other than Texas and Michigan included? How about town or municipal elections? Other countries?\nThe reason that we don’t worry about those details is that they don’t matter to the problem we are trying to solve. Whether or not the population includes voters in 2040 and/or in Toronto does not really matter to our attempts to draw inferences about Texas in 2026.\nWe also don’t worry about getting other details correct. For example, we know that this experiment includes four post cards, plus the control. That is five potential outcomes, and 10 possible causal effects, since the definition of a causal effect in the difference between any two potential outcomes. We don’t bother to include all those details in the Population Table.\nRecall that the purpose of the Preceptor Table is to force us to answer the key questions. Is the model predictive or causal? What are the units, outcomes and covariates/treatments?\nSimilarly, the purpose of the Population Table is to force us to think hard about the key assumptions of validity, stability, representativeness, and unconfoundedness.\n\n10.2.2 Stability\nStability means that the relationship between the columns is the same for three categories of rows: the data, the Preceptor table, and the larger population from which both are drawn. Of course, what we most need is for the relationship between the columns to be the same between the data and the Preceptor Table. If it isn’t, then we can’t use models estimated on the former to make inferences about the latter. But it is weird to just assume a connection between in Michigan in 2006 and Texas in 2026. Any such assumption is really a broader claim about many years and many jurisdictions. Our description of the underlying population is the glue which connects the data and the Preceptor Table.\nIs data collected in 2006 on voting behavior likely to be the same in 2026? Frankly, we don’t know! We aren’t sure what would impact someone’s response to a postcard encouraging them to vote. It is possible, for instance, that a postcard informing neighbors of voting status has a bigger effect in a world with more social media.\nWhen we are confronted with this uncertainty, we can consider making our timeframe smaller. However, we would still need to assume stability from 2006 (time of data collection) to 2026. Stability allows us to ignore the issue of time.\n\n10.2.3 Representativeness\nEven if the stability assumption holds, we still have problems with representativeness because Michigan and Texas are (very?) different states. In one sense, neither the data nor the Preceptor Table is a sample. Both include, more or less, the entire electorates in their respective states. But, from the point of view of the Population Table, they are both samples from the underling population. Even if we, with the help of the stability assumption, don’t think that anything has changes between 2006 and 2026, we are still trying to use data from Michigan to make an inference about Texas. The obvious problem is that voters in Michigan are (very?) different from voters in Texas, in all sorts of ways which might matter to our analysis.\n\n10.2.4 Unconfoundedness\nThe great advantage of randomized assignment of treatment is that it guarantees unconfoundedness. There is no way for treatment assignment to be correlated with anything, including potential outcomes, if treatment assignment is random.\n\nUsing the results of a voting experiment in Michigan in 2006 primary election, we seek to forecast the causal effect on voter participation of sending postcards in the Texas gubernatorial general election of 2026. There is concern that data from a primary election might not generalize to a general election and that political culture in the two states differ too much to allow for data from one to enable useful forecasts in the other.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>N Parameters</span>"
    ]
  },
  {
    "objectID": "n-parameters.html#courage",
    "href": "n-parameters.html#courage",
    "title": "10  N Parameters",
    "section": "\n10.3 Courage",
    "text": "10.3 Courage\n\n\n\n\nCourage\n\n\n\n\n10.3.1 Models\nJustice gave us the Population Table. Courage selects the data generating mechanism. We first specify the mathematical formula which connects the outcome variable we are interested in with the other data that we have. We explore different models. We need to decide which variables to include and to estimate the values of unknown parameters. We check our models for consistency with the data we have. We avoid hypothesis tests. We select one model, the data generating mechanism or DGM.\n\nGiven that the outcome variable is 0/1, a logistic model is the obvious choice. However, in this case, a linear model with a normal error term provides more or less the same answer.\nWe will need our usual modeling packages:\n\nlibrary(brms)\nlibrary(tidybayes)\n\n\n10.3.1.1 voted ~ treatment + sex\nIn this section, we will look at the relationship between primary voting and treatment + sex.\nThe math:\nWithout variable names:\n\\[ y_{i} = \\beta_{0} + \\beta_{1}x_{i, 1} + \\beta_{2}x_{i,2} ... + \\beta_{n}x_{i,n} + \\epsilon_{i} \\] With variable names:\n\\[ voted_{i} = \\beta_{0} + \\beta_{1}civic\\_duty_i + \\beta_{2}hawthorne_i + \\beta_{3}self_i + \\beta_{4}neighbors_i + \\beta_{5}male_i + \\epsilon_{i} \\]\nThere are two ways to formalize the model used in fit_1: with and without the variable names. The former is related to the concept of Justice as we acknowledge that the model is constructed via the linear sum of n parameters times the value for n variables, along with an error term. In other words, it is a linear model. The only other model we have learned this semester is a logistic model, but there are other kinds of models, each defined by the mathematics and the assumptions about the error term.\nThe second type of formal notation, more associated with the virtue Courage, includes the actual variable names we are using. The trickiest part is the transformation of character/factor variables into indicator variables, meaning variables with 0/1 values. Because treatment has 5 levels, we need 4 indicator variables. The fifth level — which, by default, is the first variable alphabetically (for character variables) or the first level (for factor variables) — is incorporated in the intercept.\nLet’s translate the model into code.\n\nfit_1 &lt;- brm(formula = voted ~ treatment + sex,\n             data = x,\n             family = gaussian(),\n             refresh = 0,\n             silent = 2,\n             seed = 99)\n\nTrying to compile a simple C file\n\n\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nusing C compiler: ‘Apple clang version 15.0.0 (clang-1500.3.9.4)’\nusing SDK: ‘’\nclang -arch arm64 -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/opt/R/arm64/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c foo.c -o foo.o\nIn file included from &lt;built-in&gt;:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/Core:19:\n/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: 'cmath' file not found\n#include &lt;cmath&gt;\n         ^~~~~~~\n1 error generated.\nmake: *** [foo.o] Error 1\n\n\n\nShow the codeprint(fit_1, digits = 3)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: voted ~ treatment + sex \n   Data: x (Number of observations: 344084) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                   Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept             0.291     0.001    0.288    0.293 1.000     4077     3737\ntreatmentCivicDuty    0.018     0.003    0.013    0.023 1.001     2724     2960\ntreatmentHawthorne    0.026     0.003    0.021    0.031 0.999     2932     2925\ntreatmentSelf         0.049     0.003    0.043    0.054 1.000     3168     2972\ntreatmentNeighbors    0.081     0.003    0.076    0.086 1.000     3201     2742\nsexMale               0.012     0.002    0.009    0.015 1.001     5215     3188\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma    0.464     0.001    0.463    0.465 1.001     6852     2997\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWe will now create a table that nicely formats the results of fit_1 using the tbl_regression() function from the gtsummary package. It will also display the associated 95% confidence interval for each coefficient.\n\nShow the codetbl_regression(fit_1, \n               intercept = TRUE, \n               estimate_fun = function(x) style_sigfig(x, digits = 3)) |&gt;\n  \n  # Using Beta as the name of the parameter column is weird.\n  \n  as_gt() |&gt;\n  tab_header(title = md(\"**Likelihood of Voting in the Next Election**\"),\n             subtitle = \"How Treatment Assignment and Age Predict Likelihood of Voting\") |&gt;\n  tab_source_note(md(\"Source: Gerber, Green, and Larimer (2008)\")) |&gt; \n  cols_label(estimate = md(\"**Parameter**\"))\n\n\n\n\n\n\nLikelihood of Voting in the Next Election\n    \n\nHow Treatment Assignment and Age Predict Likelihood of Voting\n    \n\nCharacteristic\n      Parameter\n      \n95% CI1\n\n    \n\n\n\n(Intercept)\n0.291\n0.288, 0.293\n\n\ntreatment\n\n\n\n\n    No Postcard\n—\n—\n\n\n    treatmentCivicDuty\n0.018\n0.013, 0.023\n\n\n    Hawthorne\n0.026\n0.021, 0.031\n\n\n    Self\n0.049\n0.043, 0.054\n\n\n    Neighbors\n0.081\n0.076, 0.086\n\n\nsex\n\n\n\n\n    sexMale\n0.012\n0.009, 0.015\n\n\n\nSource: Gerber, Green, and Larimer (2008)\n    \n\n\n1 CI = Credible Interval\n    \n\n\n\n\nInterpretation:\n\nThe intercept of this model is the expected value of the probability of someone voting in the 2006 primary given that they are part of the control group and are female. In this case, we estimate that women in the control group will vote ~29% of the time.\nThe coefficient for sexMale indicates the difference in likelihood of voting between a male and female. In other words, when comparing men and women, the 0.012 implies that men are ~1.2% more likely to vote than women. Note that, because this is a linear model with no interactions between sex and other variables, this difference applies to any male, regardless of the treatment he received. Because sex can not be manipulated (by assumption), we should not use a causal interpretation of the coefficient.\nThe coefficients of the treatments, on the other hand, do have a causal interpretation. For a single individual, of either sex, being sent the Self postcard increases your probability of voting by 4.9%. It appears that the Neighbors treatment is the most effective at ~8.1% and Civic Duty is the least effective at ~1.8%.\n\n10.3.1.2 voted ~ age_z + sex + treatment + voter_class + voter_class*treatment\nIt is time to look at interactions! Create another model named fit_2 that estimates voted as a function of age_z, sex, treatment, voter_class, and the interaction between treatment and voter classification.\nThe math:\n\\[y_{i} = \\beta_{0} + \\beta_{1} age\\_z + \\beta_{2}male_i + \\beta_{3}civic\\_duty_i + \\\\ \\beta_{4}hawthorne_i + \\beta_{5}self_i + \\beta_{6}neighbors_i + \\\\ \\beta_{7}Sometimes\\ vote_i + \\beta_{8}Always\\ vote_i + \\\\ \\beta_{9}civic\\_duty_i Sometimes\\ vote_i + \\beta_{10}hawthorne_i Sometimes\\ vote_i + \\\\ \\beta_{11}self_i Sometimes\\ vote_i + \\beta_{11}neighbors_i Sometimes\\ vote_i + \\\\ \\beta_{12}civic\\_duty_i Always\\ vote_i + \\beta_{13}hawthorne_i Always\\ vote_i + \\\\ \\beta_{14}self_i Always\\ vote_i + \\beta_{15}neighbors_i Always\\ vote_i + \\epsilon_{i}\\] Translate into code:\n\nfit_2 &lt;- brm(formula = voted ~ age_z + sex + treatment + voter_class + \n                            treatment*voter_class,\n             data = x,\n             family = gaussian(),\n             refresh = 0,\n             silent = 2,\n             seed = 19)\n\nTrying to compile a simple C file\n\n\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nusing C compiler: ‘Apple clang version 15.0.0 (clang-1500.3.9.4)’\nusing SDK: ‘’\nclang -arch arm64 -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/opt/R/arm64/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c foo.c -o foo.o\nIn file included from &lt;built-in&gt;:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/Core:19:\n/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: 'cmath' file not found\n#include &lt;cmath&gt;\n         ^~~~~~~\n1 error generated.\nmake: *** [foo.o] Error 1\n\n\n\nShow the codeprint(fit_2, digits = 3)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: voted ~ age_z + sex + treatment + voter_class + treatment * voter_class \n   Data: x (Number of observations: 344084) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                                            Estimate Est.Error l-95% CI\nIntercept                                      0.153     0.003    0.147\nage_z                                          0.035     0.001    0.034\nsexMale                                        0.008     0.002    0.005\ntreatmentCivicDuty                             0.010     0.007   -0.003\ntreatmentHawthorne                             0.008     0.007   -0.006\ntreatmentSelf                                  0.023     0.007    0.010\ntreatmentNeighbors                             0.044     0.007    0.031\nvoter_classSometimesVote                       0.114     0.003    0.108\nvoter_classAlwaysVote                          0.294     0.004    0.287\ntreatmentCivicDuty:voter_classSometimesVote    0.014     0.008   -0.000\ntreatmentHawthorne:voter_classSometimesVote    0.019     0.008    0.004\ntreatmentSelf:voter_classSometimesVote         0.030     0.007    0.016\ntreatmentNeighbors:voter_classSometimesVote    0.042     0.007    0.028\ntreatmentCivicDuty:voter_classAlwaysVote      -0.001     0.009   -0.018\ntreatmentHawthorne:voter_classAlwaysVote       0.025     0.009    0.008\ntreatmentSelf:voter_classAlwaysVote            0.025     0.009    0.009\ntreatmentNeighbors:voter_classAlwaysVote       0.046     0.008    0.030\n                                            u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept                                      0.159 1.000     3155     2804\nage_z                                          0.037 1.000     4474     3058\nsexMale                                        0.011 1.000     4514     2678\ntreatmentCivicDuty                             0.023 1.001     2741     2837\ntreatmentHawthorne                             0.021 1.003     3052     2808\ntreatmentSelf                                  0.036 1.001     3125     2848\ntreatmentNeighbors                             0.057 1.002     2901     2652\nvoter_classSometimesVote                       0.121 1.000     3113     2702\nvoter_classAlwaysVote                          0.301 1.000     3266     2930\ntreatmentCivicDuty:voter_classSometimesVote    0.029 1.002     2695     2832\ntreatmentHawthorne:voter_classSometimesVote    0.034 1.001     3055     2787\ntreatmentSelf:voter_classSometimesVote         0.044 1.001     3318     2910\ntreatmentNeighbors:voter_classSometimesVote    0.057 1.001     2998     2678\ntreatmentCivicDuty:voter_classAlwaysVote       0.015 1.000     2888     2807\ntreatmentHawthorne:voter_classAlwaysVote       0.043 1.002     3157     2618\ntreatmentSelf:voter_classAlwaysVote            0.041 1.001     3249     2620\ntreatmentNeighbors:voter_classAlwaysVote       0.063 1.001     2972     2643\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma    0.451     0.001    0.450    0.452 1.002     4727     2580\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nAs we did with our first model, create a regression table to observe our findings:\n\nShow the codetbl_regression(fit_2, \n               intercept = TRUE, \n               estimate_fun = function(x) style_sigfig(x, digits = 3)) |&gt;\n  as_gt() |&gt;\n  tab_header(title = md(\"**Likelihood of Voting in the Next Election**\"),\n             subtitle = \"How Treatment Assignment and Other Variables Predict Likelihood of Voting\") |&gt;\n  tab_source_note(md(\"Source: Gerber, Green, and Larimer (2008)\")) |&gt; \n  cols_label(estimate = md(\"**Parameter**\"))\n\n\n\n\n\n\nLikelihood of Voting in the Next Election\n    \n\nHow Treatment Assignment and Other Variables Predict Likelihood of Voting\n    \n\nCharacteristic\n      Parameter\n      \n95% CI1\n\n    \n\n\n\n(Intercept)\n0.153\n0.147, 0.159\n\n\nage_z\n0.035\n0.034, 0.037\n\n\nsex\n\n\n\n\n    sexMale\n0.008\n0.005, 0.011\n\n\ntreatment\n\n\n\n\n    No Postcard\n—\n—\n\n\n    treatmentCivicDuty\n0.010\n-0.003, 0.023\n\n\n    Hawthorne\n0.008\n-0.006, 0.021\n\n\n    Self\n0.023\n0.010, 0.036\n\n\n    Neighbors\n0.044\n0.031, 0.057\n\n\nvoter_class\n\n\n\n\n    Rarely Vote\n—\n—\n\n\n    voter_classSometimesVote\n0.114\n0.108, 0.121\n\n\n    voter_classAlwaysVote\n0.294\n0.287, 0.301\n\n\ntreatment * voter_class\n\n\n\n\n    treatmentCivicDuty * voter_classSometimesVote\n0.014\n0.000, 0.029\n\n\n    Hawthorne * voter_classSometimesVote\n0.019\n0.004, 0.034\n\n\n    Self * voter_classSometimesVote\n0.030\n0.016, 0.044\n\n\n    Neighbors * voter_classSometimesVote\n0.042\n0.028, 0.057\n\n\n    treatmentCivicDuty * voter_classAlwaysVote\n-0.001\n-0.018, 0.015\n\n\n    Hawthorne * voter_classAlwaysVote\n0.025\n0.008, 0.043\n\n\n    Self * voter_classAlwaysVote\n0.025\n0.009, 0.041\n\n\n    Neighbors * voter_classAlwaysVote\n0.046\n0.030, 0.063\n\n\n\nSource: Gerber, Green, and Larimer (2008)\n    \n\n\n1 CI = Credible Interval\n    \n\n\n\n\nNow that we have a summarized visual for our data, let’s interpret the findings:\n\nThe intercept of fit_2 is the expected probability of voting in the upcoming election for a woman of average age (~ 50 years old in this data), who is assigned to the No Postcard group, and is a Rarely Voter. The estimate is 15.3%.\nThe coefficient of age_z, 0.035, implies a change of ~3.5% in likelihood of voting for each increment of one standard deviation (~ 14.45 years). For example: when comparing someone 50 years old with someone 65, the latter is about 3.5% more likely to vote.\nExposure to the Neighbors treatment shows a ~4.4% increase in voting likelihood for someone in the Rarely Vote category. Because of random assignment of treatment, we can interpret that coefficient as an estimate of the average treatment effect.\nIf someone were from a different voter classification, the calculation is more complex because we need to account for the interaction term. For example, for individuals who Sometimes Vote, the treatment effect of Neighbors is 8.6%. For Always Vote Neighbors, it is 9%.\n\n10.3.2 Tests\n\n10.3.3 Data Generating Mechanism\nShould we use fit_1 or fit_2? Reasonable people will differ.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>N Parameters</span>"
    ]
  },
  {
    "objectID": "n-parameters.html#temperance",
    "href": "n-parameters.html#temperance",
    "title": "10  N Parameters",
    "section": "\n10.4 Temperance",
    "text": "10.4 Temperance\n\n\n\n\nTemperance\n\n\n\nCourage produced the data generating mechanism. Temperance guides us in the use of the DGM — or the “model” — we have created to answer the questions with which we began. We create posteriors for the quantities of interest. We should be modest in the claims we make. The posteriors we create are never the “truth.” The assumptions we made to create the model are never perfect. Yet decisions made with flawed posteriors are almost always better than decisions made without them.\n\n10.4.1 Questions and Answers\nRecall our initial question: What is the causal effect, on the likelihood of voting, of different postcards on voters of different levels of political engagement?\nTo answer the question, we want to look at different average treatment effects for each treatment and type of voting behavior. In the real world, the treatment effect for person A is almost always different than the treatment effect for person B.\nIn this section, we will create a plot that displays the posterior probability distributions of the average treatment effects for men of average age across all combinations of 4 treatments and 3 voter classifications. This means that we are making a total of 12 inferences.\nImportant note: We could look at lots of ages and both Male and Female subjects. However, that would not change our estimates of the treatment effects. The model is linear, so terms associated with age_z and sex disappear when we do the subtraction. This is one of the great advantages of linear models.\nTo begin, we will need to create our newobs object.\n\n# Because our model is linear, the terms associated with age_z and sex disappear\n# when we perform subtraction. The treatment effects calculated thereafter will\n# not only apply to males of the z-scored age of ~ 50 years. The treatment\n# effects apply to all participants, despite calling these inputs.\n\nsex &lt;- \"Male\"\nage_z &lt;- 0\ntreatment &lt;- c(\"No Postcard\",\n               \"Civic Duty\",\n               \"Hawthorne\",\n               \"Self\",\n               \"Neighbors\")\nvoter_class &lt;- c(\"Always Vote\",\n                 \"Sometimes Vote\",\n                 \"Rarely Vote\")\n\n# This question requires quite the complicated tibble! Speaking both\n# hypothetically and from experience, keeping track of loads of nondescript\n# column names while doing ATE calculations leaves you prone to simple, but\n# critical, errors. expand_grid() was created for cases just like this - we want\n# all combinations of treatments and voter classifications in the same way that\n# our model displays the interaction term parameters.\n\nnewobs &lt;- expand_grid(sex, age_z, treatment, voter_class) |&gt; \n  \n  # This is a handy setup for the following piece of code that allows us to\n  # mutate the ATE columns with self-contained variable names. This is what\n  # helps to ensure that the desired calculations are indeed being done. If you\n  # aren't familiar, check out the help page for paste() at `?paste`.\n  \n  mutate(names = paste(treatment, voter_class, sep = \"_\"))\n\nTake a look the newobs object before trying to use it.\n\nnewobs\n\n# A tibble: 15 × 5\n   sex   age_z treatment   voter_class    names                     \n   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;          &lt;chr&gt;                     \n 1 Male      0 No Postcard Always Vote    No Postcard_Always Vote   \n 2 Male      0 No Postcard Sometimes Vote No Postcard_Sometimes Vote\n 3 Male      0 No Postcard Rarely Vote    No Postcard_Rarely Vote   \n 4 Male      0 Civic Duty  Always Vote    Civic Duty_Always Vote    \n 5 Male      0 Civic Duty  Sometimes Vote Civic Duty_Sometimes Vote \n 6 Male      0 Civic Duty  Rarely Vote    Civic Duty_Rarely Vote    \n 7 Male      0 Hawthorne   Always Vote    Hawthorne_Always Vote     \n 8 Male      0 Hawthorne   Sometimes Vote Hawthorne_Sometimes Vote  \n 9 Male      0 Hawthorne   Rarely Vote    Hawthorne_Rarely Vote     \n10 Male      0 Self        Always Vote    Self_Always Vote          \n11 Male      0 Self        Sometimes Vote Self_Sometimes Vote       \n12 Male      0 Self        Rarely Vote    Self_Rarely Vote          \n13 Male      0 Neighbors   Always Vote    Neighbors_Always Vote     \n14 Male      0 Neighbors   Sometimes Vote Neighbors_Sometimes Vote  \n15 Male      0 Neighbors   Rarely Vote    Neighbors_Rarely Vote     \n\n\nNow that we have newobs, we just use the same call to add_epred_draws() as usual.\n\nfit_2 |&gt; \n  add_epred_draws(newdata = newobs)\n\n# A tibble: 60,000 × 10\n# Groups:   sex, age_z, treatment, voter_class, names, .row [15]\n   sex   age_z treatment  voter_class names  .row .chain .iteration .draw .epred\n   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt; &lt;int&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;\n 1 Male      0 No Postca… Always Vote No P…     1     NA         NA     1  0.454\n 2 Male      0 No Postca… Always Vote No P…     1     NA         NA     2  0.456\n 3 Male      0 No Postca… Always Vote No P…     1     NA         NA     3  0.453\n 4 Male      0 No Postca… Always Vote No P…     1     NA         NA     4  0.454\n 5 Male      0 No Postca… Always Vote No P…     1     NA         NA     5  0.454\n 6 Male      0 No Postca… Always Vote No P…     1     NA         NA     6  0.455\n 7 Male      0 No Postca… Always Vote No P…     1     NA         NA     7  0.453\n 8 Male      0 No Postca… Always Vote No P…     1     NA         NA     8  0.451\n 9 Male      0 No Postca… Always Vote No P…     1     NA         NA     9  0.456\n10 Male      0 No Postca… Always Vote No P…     1     NA         NA    10  0.455\n# ℹ 59,990 more rows\n\n\n\nThis data is not in a format which is easy to work with. We want to calculate causal effects, which are the difference between two potential outcomes. This is easiest to do when all potential outcomes are on the same line. Consider:\n\ndraws &lt;- fit_2 |&gt; \n  add_epred_draws(newdata = newobs) |&gt; \n  ungroup() |&gt; \n  select(names, .epred) |&gt;  \n  pivot_wider(names_from = names, \n              values_from = .epred, \n              values_fn = list) |&gt;\n  unnest(cols = everything()) |&gt; \n  janitor::clean_names()\n\ndraws\n\n# A tibble: 4,000 × 15\n   no_postcard_always_vote no_postcard_sometimes_vote no_postcard_rarely_vote\n                     &lt;dbl&gt;                      &lt;dbl&gt;                   &lt;dbl&gt;\n 1                   0.454                      0.274                   0.161\n 2                   0.456                      0.272                   0.159\n 3                   0.453                      0.273                   0.158\n 4                   0.454                      0.274                   0.159\n 5                   0.454                      0.275                   0.161\n 6                   0.455                      0.274                   0.162\n 7                   0.453                      0.276                   0.162\n 8                   0.451                      0.274                   0.165\n 9                   0.456                      0.274                   0.162\n10                   0.455                      0.273                   0.163\n# ℹ 3,990 more rows\n# ℹ 12 more variables: civic_duty_always_vote &lt;dbl&gt;,\n#   civic_duty_sometimes_vote &lt;dbl&gt;, civic_duty_rarely_vote &lt;dbl&gt;,\n#   hawthorne_always_vote &lt;dbl&gt;, hawthorne_sometimes_vote &lt;dbl&gt;,\n#   hawthorne_rarely_vote &lt;dbl&gt;, self_always_vote &lt;dbl&gt;,\n#   self_sometimes_vote &lt;dbl&gt;, self_rarely_vote &lt;dbl&gt;,\n#   neighbors_always_vote &lt;dbl&gt;, neighbors_sometimes_vote &lt;dbl&gt;, …\n\n\nEach column has 4,000 rows, representing 4,000 draws from the posterior probability distribution of average voting likelihood for a person with that level of civic engagement and subject to that treatment. So, no_postcard_always_vote is the posterior for the average (or predicted) likelihood of voting for someone who did not receive a postcard, i.e., they were a control, and who “always votes.”\nRecall that, when calculating a treatment effect, we need to subtract the estimate for each category from the control group for that category. For example, if we wanted to find the treatment effect for the neighbors_always_vote group, we would need: neighbors_always_vote minus no_postcard_always_vote.\nTherefore, we will use mutate() twelve times, for each of the treatments and voting frequencies. After, we will pivot_longer in order for the treatment effects to be sensibly categorized for plotting. If any of this sounds confusing, read the code comments carefully.\n\nShow the codeplot_data &lt;- draws |&gt; \n  \n  # Using our cleaned naming system, ATE calculations are simple enough. Note\n  # how much easier the code reads because we have taken the trouble to line up\n  # the columns.\n  \n  mutate(`Always Civic-Duty`    = civic_duty_always_vote     - no_postcard_always_vote,\n         `Always Hawthorne`     = hawthorne_always_vote      - no_postcard_always_vote,\n         `Always Self`          = self_always_vote           - no_postcard_always_vote,\n         `Always Neighbors`     = neighbors_always_vote      - no_postcard_always_vote,\n         `Sometimes Civic-Duty` = civic_duty_sometimes_vote  - no_postcard_sometimes_vote,\n         `Sometimes Hawthorne`  = hawthorne_sometimes_vote   - no_postcard_sometimes_vote,\n         `Sometimes Self`       = self_sometimes_vote        - no_postcard_sometimes_vote,\n         `Sometimes Neighbors`  = neighbors_sometimes_vote   - no_postcard_sometimes_vote,\n         `Rarely Civic-Duty`    = civic_duty_rarely_vote     - no_postcard_rarely_vote,\n         `Rarely Hawthorne`     = hawthorne_rarely_vote      - no_postcard_rarely_vote,\n         `Rarely Self`          = self_rarely_vote           - no_postcard_rarely_vote,\n         `Rarely Neighbors`     = neighbors_rarely_vote      - no_postcard_rarely_vote) |&gt; \n  \n  # This is a critical step, we need to be able to reference voter\n  # classification separately from the treatment assignment, so pivoting in the\n  # following manner reconstructs the relevant columns for each of these\n  # individually. \n  \n  pivot_longer(names_to = c(\"Voter Class\", \"Group\"),\n               names_sep = \" \",\n               values_to = \"values\",\n               cols = `Always Civic-Duty`:`Rarely Neighbors`) |&gt; \n  \n    # Reordering the factors of voter classification forces them to be displayed\n    # in a sensible order in the plot later.\n  \n    mutate(`Voter Class` = fct_relevel(factor(`Voter Class`),\n                                     c(\"Rarely\",\n                                       \"Sometimes\",\n                                       \"Always\")))\n\n\nFinally, we will plot our data! Read the code comments for explanations on aesthetic choices, as well as a helpful discussion on fct_reorder().\n\nShow the codeplot_data  |&gt; \n  \n  # Reordering the y axis values allows a smoother visual interpretation - \n  # you can see the treatments in sequential ATE.\n  \n  ggplot(aes(x = values, y = fct_reorder(Group, values))) +\n  \n  # position = \"dodge\" is the only sure way to see all 3 treatment distributions\n  # identity, single, or any others drop \"Sometimes\" - topic for further study\n  \n    stat_slab(aes(fill = `Voter Class`),\n              position = 'dodge') +\n    scale_fill_calc() +\n  \n    # more frequent breaks on the x-axis provides a better reader interpretation\n    # of the the shift across age groups, as opposed to intervals of 10%\n    \n    scale_x_continuous(labels = scales::percent_format(accuracy = 1),\n                       breaks = seq(-0.05, 0.11, 0.01)) +\n    labs(title = \"Treatment Effects on the Probability of Voting\",\n         subtitle = \"Postcards work less well on those who rarely vote\",\n         y = \"Postcard Type\",\n         x = \"Average Treatment Effect\",\n         caption = \"Source: Gerber, Green, and Larimer (2008)\") +\n    theme_clean() +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nThis is interesting! It shows us a few valuable bits of information:\n\nWe are interested in the average treatment effect of postcards. There are 4 different postcards, each of which can be compared to what would have happened if the voter did not receive any postcard.\nThese four treatment effects, however, are heterogeneous. They vary depending on an individual’s voting history, which we organize into three categories: Rarely Vote, Sometimes Vote and Always Vote. So, we have 12 different average treatment effects, one for each possible combination of postcard and voting history.\nFor each of these combinations, the graphic shows our posterior distribution.\n\nWhat does this mean for us, as we consider which postcards to send? * Consider the highest yellow distribution, which is the posterior distribution for the average treatment effect of receiving the Neighbors postcard (compared to not getting a postcard) for Always Voters. The posterior is centered around 9% with a 95% confidence interval of, roughly, 8% to 10%. * Overall, the Civic Duty and Hawthorne postcards had small average treatment effects, across all three categories of voter. The causal effect on Rarely Voters was much smaller, regardless of treatment. It was also much less precisely estimated because there were many fewer Rarely Voters in the data. *The best way to increase turnover, assuming there are limits to how many postcards you can send, is to focus on Sometimes/Always voters and to use the Neighbors postcard.\nConclusion: If we had a limited number of postcards, we would send the Neighbors postcard to citizens who already demonstrate a tendency to vote.\nHow confident are we in these findings? If we needed to convince our boss that this is the right strategy, we need to explain how confident we are in our assumptions. To do that, we must understand the three levels of knowledge in the world of posteriors.\n\n10.4.2 Humility\nWe can never know the truth.\nThere exist three primary levels of knowledge possible knowledge in our scenario: the Truth (the ideal Preceptor Table), the DGM Posterior, and Our Posterior.\nIf we know the Truth (with a capital “T”), then we know the ideal Preceptor Table. With that knowledge, we can directly answer our question precisely. We can calculate each individual’s treatment effect, and any summary measure we might be interested in, like the average treatment effect.\nThis level of knowledge is possible only under an omniscient power, one who can see every outcome in every individual under every treatment. The Truth would show, for any given individual, their actions under control, their actions under treatment, and each little factor that impacted those decisions.\nThe Truth represents the highest level of knowledge one can have — with it, our questions merely require algebra. There is no need to estimate a treatment effect, or the different treatment effects for different groups of people. We would not need to predict at all — we would know.\nThe DGM posterior is the next level of knowledge, which lacks the omniscient quality of The Truth. This posterior is the posterior we would calculate if we had perfect knowledge of the data generating mechanism, meaning we have the correct model structure and exact parameter values. This is often falsely conflated with “Our posterior”, which is subject to error in model structure and parameter value estimations.\nWith the DGM posterior, we could not be certain about any individual’s causal effect, because of the Fundamental Problem of Causal Inference. In other words, we can never measure any one person’s causal effect because we are unable to see a person’s resulting behavior under treatment and control; we only have data on one of the two conditions.\nWhat we do with the DGM posterior is the same as Our posterior — we estimate parameters based on data and predict the future with the latest and most relevant information possible. The difference is that, when we calculate posteriors for an unknown value in the DGM posterior, we expect those posteriors to be perfect.\nIf we go to our boss with our estimates from this posterior, we would expect our 95% confidence interval to be perfectly calibrated. That is, we would expect the true value to lie within the 95% confidence interval 95% of the time. In this world, we would be surprised to see values outside of the confidence interval more than 5% of the time.\nUnfortunately, Our posterior possesses even less certainty! In the real world, we don’t have perfect knowledge of the DGM: the model structure and the exact parameter values. What does this mean?\nWhen we go to our boss, we tell them that this is our best guess. It is an informed estimate based on the most relevant data possible. From that data, we have created a 95% confidence interval for the treatment effect of various postcards. We estimate that the treatment effect of the Neighbors postcard to be between 8% to 10%.\nDoes this mean we are certain that the treatment effect of Neighbors is between these values? Of course not! As we would tell our boss, it would not be shocking to find out that the actual treatment effect was less or more than our estimate.\nThis is because a lot of the assumptions we make during the process of building a model, the processes in Wisdom, are subject to error. Perhaps our data did not match the future as well as we had hoped. Ultimately, we try to account for our uncertainty in our estimates. Even with this safeguard, we aren’t surprised if we are a bit off.\nFor instance, would we be shocked if the treatment effect of the Neighbors postcard to be 7%? 12%? Of course not! That is only slightly off, and we know that Our posterior is subject to error. Would we be surprised if the treatment effect was found to be 20%? Yes. That is a large enough difference to suggest a real problem with our model, or some real world change that we forgot to factor into our predictions.\nBut, what amounts to a large enough difference to be a cause for concern? In other words, how wrong do we have to be in a one-off for our boss to be suspicious? When is “bad luck” a sign of stupidity? We will delve into this question in the next section of our chapter.\n\nThe world is always more uncertain than our models would have us believe.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>N Parameters</span>"
    ]
  },
  {
    "objectID": "key-concepts.html",
    "href": "key-concepts.html",
    "title": "Cardinal Virtues",
    "section": "",
    "text": "Question\nStart with the question. These generally start very broadly. Example:\nIf you are working in a specific fields, then the questions you and your colleagues/competitors examine will be very similar. In baseball, you will all care about the relationship between minor league statistics and major league performance. In political campaigns, you will all want to know what factors influence both donations and voting.\nIn order to make progress, you will then drill down to a more specific question, one with a numerical answer. Example:\nSpecifics help you to fix ideas as you start to work on a project. Just because you start looking for this number does not mean that we can’t consider other questions. We might estimate dozens of specific numbers when exploring the connection between income and ideology.\nYour goal is to provide an answer to the question, along with your uncertainty about that answer. The most common form for the answer is a posterior probability distribution. This PPD is either the answer itself, or the tool we use to answer the question. The answer to the second type of question will generally involves many posterior probability distributions.\nThe question should be indented and italicized.\nOf course, every data science problem does not start with a question. It actually starts with a decision. The world confronts us. Make decisions we must. Yet, in this introductory textbook, starting with a decision to make is too hard. So, we simplify and start with a question. But we should always at least mention the sort of decision which the answer to the question might help us to make. In baseball, we use statistics to help us decide which players to draft. In politics, we use statistics to craft political ads.\nNote that the sophistication of these discussions increases as we go further into the book. Your discussion should be more sophisticated than the one found in the previous chapter and less sophisticated than what comes later.\nEach chapter features all the same sections and sub-sections as we use below. That is, there are three sub-sections to Wisdom, four sub-sections for Justice, three for Courage and two for Temperance.\nQuantity of Interest is the number which you want to estimate. It is the answer to a specific question. You will almost always calculate a posterior probability distribution for your Quantity of Interest since, in the real world, you will never know your QoI precisely. More sophisticated problems involve more than one QoI.\nOnce we have our question, we can start with the Cardinal Virtues. Each section begins with a one sentence summary about the component steps of the relevant virtue. These will, obviously, be highly similar from chapter to chapter. But that is OK! We want to reinforce the steps in the path over and over again.\nNo battle plan survives first contact with the enemy intact. The same applies to questions and the data we use to answer them. Our questions evolve as our analysis continues. Ultimately, we can only answer some questions with the data we have. Those questions may be close to the ones with which we started, but they will rarely be identical.",
    "crumbs": [
      "Cardinal Virtues"
    ]
  },
  {
    "objectID": "key-concepts.html#steps-to-take-in-every-data-science-problem",
    "href": "key-concepts.html#steps-to-take-in-every-data-science-problem",
    "title": "Key Concepts",
    "section": "",
    "text": "Question\nStart with the question. This will generally be something simple, easily expressed in just a sentence, using colloquial language. Every data science problem starts with a question. Your goal is to provide an answer to the question, along with your uncertainty about that answer. The most common form for the answer is a posterior probability distribution. This PPD is either the answer itself, or the tool we use to answer the question. Example:\n\nWhat proportion of people who make $100,000 are liberal?\n\nThe question should be indented and italicized. As we progress, questions become less precise and more general. We don’t know what to focus on when we start. We narrow as we proceed.\nOf course, every data science problem does not start with a question. It actually starts with a decision. The world confronts us. Make decisions we must. Yet, in this introductory textbook, starting with a decision to make is too hard. So, we simplify and start with a question. But we should always at least mention the sort of decision which the answer to the question might help us to make.\nNote that the sophistication of these discussions increases as we go further into the book. Your discussion should be more sophisticated than the one found in the previous chapter and less sophisticated than what comes later. Later one, we don’t so much start with a specific question as with a variable in which we are interested.\nEach chapter features all the same sections and sub-sections as we use below. That is, there are three sub-sections to Wisdom, four sub-sections for Justice, and so on.\nOnce we have our question, we can start with the Cardinal Virtues. Each section begins with a one sentence summary about the component steps of the relevant virtue. These will, obviously, be highly similar from chapter to chapter. But that is OK! We want to reinforce the steps in the path over and over again.\nNo battle plan survives first contact with the enemy intact. The same applies to questions and the data we use to answer them. Our questions evolve as our analysis continues. Ultimately, we can only answer some questions with the data we have. Those questions may be close to the ones that we started with, but they will rarely be identical.\nWisdom\n\nWisdom requires the creation of a Preceptor Table, an examination of our data, and a determination, using the concept of validity, as to whether or not we can (reasonably!) assume that the two come from the same population.\n\nPreceptor Table\nThe Preceptor Table always begins with a restatement of the definition: “A Preceptor Table is smallest possible table with rows and columns such that, if there is no missing data, our question is easy to answer.”\nTo create the Preceptor Table, we answer a series of questions. (Don’t ask these questions rhetorically. Just describe the answer. There is also no need to number them, although you should always use this order.)\n\nIs the question causal? Look for verbs like “cause” or “affect” or “influence.” Look for a question which implies a comparison, for a single individual unit, between two states of the world, one in which the unit receives treatment \\(X\\) and one in which the unit gets treatment \\(Y\\). Look for a discussion of something which we can manipulate. Remember the motto: No causation without manipulation. We look to see if the question seeks to compare two potential outcomes within the same unit, rather than the same outcome between two different units.\n\nIf none of this is present, use a predictive model. If all you need to know to answer the question is the outcome under one value of the treatment, then the model is predictive. In that case, the treatment is not truly a “treatment.” It is just a covariate. Example: What is the att_end for all women if they were to get the treatment? This is a predictive question, not a causal one, because we do not need to know the outcome under treatment and under control for any individual woman.\n\nWhat is the moment in time to which the question refers? Every question refers to a moment in time, even if that moment stretches a bit. The set of adults today is different from the set 10 years ago, or even yesterday. We need to refine the original question. Assume that we are referring to July 1, 2020 even though, in most cases, people are interested in now. We have changed the original question from:\n\n“What proportion of people who make $100,000 are liberal?”\nto\n“On July 1, 2020, what proportion of people who make $100,000 were liberal?”\n\nWhat are the units? The question often makes this fairly clear, at least in terms of what each row corresponds to, whether it be individuals, classrooms, countries, or whatever. But, questions often fail to make clear the total number of the rows. Our example question above does not specify the relevant population. Is it about all the people in the world? All the adults? All the adults in the United States? The purpose of this paragraph is to refine the question, to make it more specific. Assume that we are interested in all the adults in Chicago. Our question now is:\n\n“On July 1, 2020, what proportion of the adults in Chicago who make $100,000 were liberal?”\nThis back-and-forth between the question and the analysis is a standard part of data science. We rarely answer the exact question we started with, especially because that question is never specific enough to answer without further qualifications. Furthermore, the data we have may not allow us to answer that question, but it may be enough to answer a related question. Is that good enough for the boss/client/colleague who asked the original question? Maybe? You won’t know until you ask.\nOur job as data scientists is not to simply answer the question we have been asked, but to help the questioner determine a question which can be answered with the data we have, a question which helps them to make the decision which they face.\n\nWhat are the outcomes? (If the model is causal, then there must be at least two potential outcomes. If you can’t figure them out, then the model is probably predictive.) If the model is predictive, then there is only one outcome. This paragraph does more than just name the relevant variable. It also starts the discussion about how exactly we might measure this variable. We consider both the underlying concept, “liberal,” and the process by which we might operationalize the concept. Perhaps we are using a written survey with a YES/NO answer. Perhaps it is an in-person interview with a 1-7 Likert scale, in which answers of 1 or 2 are coded, by us, as “liberal.” The details may or may not matter, but we at least need to discuss the issue.\n\nWhat are the covariates? Discussing covariates in the context of the Preceptor Table is different than discussing covariates in the context of the data. Recall that the Preceptor Table is the smallest possible table, so we don’t need to include every relevant variable. We only need to discuss variables that are necessary to answer the question.\nWhat are the treatments, if any? (There are no “treatments” in predictive models. There are only covariates.) A treatment is a covariate which, at least in theory, we can manipulate and the manipulation of which is necessary to answer our question.\nWith all the above, create the Preceptor Table. In this case, our Preceptor Table includes N rows, one for every adult in Chicago on July 1, 2020. It includes two columns: the outcome (liberal) and a single covariate (income).\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\nOutcome\nCovariate\n\n\nLiberal\nIncome\n\n\n\n\n1\n0\n150000\n\n\n2\n0\n50000\n\n\n…\n…\n…\n\n\n10\n1\n65000\n\n\n11\n1\n35000\n\n\n…\n…\n…\n\n\nN\n1\n78000\n\n\n\n\n\n\n\nIf we have the Preceptor Table, with no missing data, then it is trivial to calculate the percentage of adults (who make more than $100,000) who are liberal.\nEDA\nThere is always short section devoted to exploratory data analysis. You can never look at the data too much. Each EDA will include at least one textual look at the data, usually using summary(), but with skim(), glimpse(), print() and slice_sample() also available. It will also include at least one graphic, almost always with the outcome variable on the y-axis and one of the covariates on the x-axis. The data set will often include columns and rows which are irrelevant to the question. Those columns and rows are removed, creating a tibble which will be used in the Courage section. The name of that tibble will often be something convenient like ch_7.\nIt also makes sense to include some discussion about where this data comes from. What are the definitions of the variables? Who chose the sample? Where is the documentation? This sort of background sets the stage for examining validity.\nValidity\nValidity discussions always have one (short) paragraph about each relevant variable (the outcome and any relevant covariates), with examples of why validity might not hold. Validity discussion finishes with a brief discussion along the lines of: “Despite these concerns, we will assume that validity does hold.”\nThese section can be longer of course, depending on how many details you discussed during the EDA. The central point is that we have two (potentially!) completely different things: the Preceptor Table and the data. Just because two columns have the same name does not mean that they are the same thing. Indeed, they will often be quite different! But because we control the Preceptor Table and, to a lesser extent, the original question, we can adjust those variables to be “closer” to the data that we actually have. This is another example of the iterative nature of data science. If the data is not close enough to the question, then we check with our boss/colleague/customer to see if we can modify the question in order to make the match between the data and the Preceptor Table close enough for validity to hold.\nWe conclude the Wisdom section by summarizing how we hope to use the data we have to answer the question we started with. Example:\n\nUsing data from a 2012 survey of Boston-area commuters, we seek to understand the relationship between income and political ideology in Chicago and similar cities in 2020. In particular, what percentage of individuals who make more than $100,000 per year are liberal?\n\nNote how the specific question has morphed into a general examination of the “relationship” between income and political ideology. In order to answer any specific question, we always have to examine a more general relationship. We always have to build a model. We can then use this model to answer both the question we started with as well as other related questions.\nBy thinking hard about the original question and the data, we have come up with a question which may be possible to answer with the data we have. Note that each Cardinal Virtue section finishes with a sentence or two summarizing what you have learned. Those sentences are combined at the end of the analysis. One of the key products of a data science project is a paragraph which summarizes the key conclusions.\nJustice\n\n\nJustice concerns four topics: the Population Table, stability, representativeness, and unconfoundedness.\n\nPopulation Table\nIf validity holds, then we can create a Population Table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation Table\n\n\nSource\nYear\nOutcome\nCovariates\n\n\nIncome\nAge\nCity\n\n\n\n\n…\n…\n…\n…\n…\n\n\nData\n2012\n150000\n43\nBoston\n\n\nData\n2012\n50000\n52\nBoston\n\n\n…\n…\n…\n…\n…\n\n\n…\n…\n…\n…\n…\n\n\nPreceptor Table\n2020\n…\n…\nChicago\n\n\nPreceptor Table\n2020\n…\n…\nChicago\n\n\n…\n…\n…\n…\n…\n\n\n\n\n\n\n\n\nThe “Source” column highlights that the Population Table includes three categories of rows: The data, the Preceptor Table, and the rest of the population, from which both the data and the Preceptor Table are drawn. The ... indicates rows from the population which are not included in either the data or the Preceptor Table.\nThe “ID” column is implicit, and often not included. After all, it should be obvious that each row refers to a specific unit. If we don’t really care about the individual units, there is no need to label them.\nThere should always be a column, in this case “Year,” which indicates the moment in time at which the covariates were recorded. A given unit may appear in multiple rows, with each row providing the data at a different time. In this example, we will have a row for Sarah in 2012, when she was 43, and a row for Sarah in 2020, when she was 51, and so on. Note that Sarah might just be member of the population, neither in the data we have nor in the Preceptor Table. Or she might be in one or the other. We are rarely concerned with any specific individual.\nEach row in the Population Table represents a unique Unit/Time combination.\nThe “Outcome” column is the variable which we are trying to understand/explain/predict. There is always an outcome column, although it will often just be labelled with the variable name, as here with “Income.”\nThe “Covariates” are all the columns other than those already discussed.\nStability\nIf the assumption of stability holds, then the relationships between the columns in the Population Table is the same across time. First, the relationship among columns from the same moment in time as the data is the same as the relationship among columns for the entire table. Second, the relationship among columns from the same moment in time as the Preceptor Table is the same as the relationship among columns for the entire table.\nStability, if true, allows is us to go from the data to the population, and from the population to the Preceptor Table.\nWe discuss at one example of why stability might not hold in this case. These examples are almost always connected to the passage of time. Whatever the relationship between political ideology and income that might have held in 2012, when we gathered our data, might not be true either before or afterwards. Provide specific speculations about what might have changed in the world.\nRegardless of those concerns, we always conclude that, although the assumption of stability might not hold perfectly, the world is probably stable enough over this time period to make inference possible.\nThe longer the time period covered by the Preceptor Table (and the data), the more suspect the assumption of stability becomes.\nRepresentativeness\nAs with all our assumptions, we always begin by restating the definition. Representativeness, or the lack thereof, concerns two relationship, among the rows in the Population Table. The first is between the Preceptor Table and the other rows from the same moment in time. The second is between our data and the other rows of equivalent vintage.\nStability looks across time periods. Reprentativeness looks within time periods.\nWe mention specific examples of two potential problems. First, is our data representative of the population? Rarely! Second, are the rows associated with the Preceptor Table representative of the population? Again, almost never!\nProvide specific examples of how a lack of representativeness might be a problem, one large enough to affect your ability to answer the question.\nBut, to continue the analysis, we always assume/pretend that the rows from both the data and the Preceptor Table are representative enough of the relevant time period from within the larger population from which both are drawn.\n\nThe more expansive your Preceptor Table, the more important the assumption of representativeness becomes.\nUnconfoundedness\nIf the model is predictive, then unconfoundedness is not a concern. Just mention that fact in a sentence at the end of the section on representativeness. But, if the model is causal, then we need a section devoted to this topic.\nAs always, start by restating the definition, something along the lines of “Unconfoundedness means that the treatment assignment is independent of the potential outcomes, when we condition on pre-treatment covariates. A model is confounded if this is not true.”\nIf treatment assignment was random, then unconfoundedness is guaranteed, although experienced researchers often worry about the exact process involved in such “random” assignment. If, however, treatment assignment was not random, then there will always be a concern that it is correlated with potential outcomes. Discuss at least two scenarios in which this might be a concern. But then, as usual, conclude that, although there might be some issues with confoundedness, they are probably small enough to not worry about.\nThe Justice section concludes with a sentence or two about how, despite any problems with the core assumptions of stability, representativeness and unconfoundedness, we can still proceed to next steps because the assumptions hold enough.\nThe last step may be to revisit the key sentences from the Wisdom section. Recall:\n\nUsing data from a 2012 survey of Boston-area commuters, we seek to understand the relationship between income and political ideology in Chicago and similar cities in 2020. In particular, what percentage of individuals who make more than $100,000 per year are liberal?\n\nAre these sentences still correct, or does a serious consideration of the assumptions of stability, representativeness and unconfoundedness require us to modify them? The answer, of course, is that the assumptions are never perfect! So, we have an obligation to add a sentence or two which highlights (no more than) one or two concerns. Examples:\n\nThere is some concern that survey participants may not be perfectly representative of the underlying population.\n\n\nThe relationship between income and ideology may have changed over that eight year period.\n\nThere is no need to use technical terms like “stability.” However, most readers will understand what “representative” means. The key point is honesty. We have an obligation to at least mention some possible concerns. At the end, we conclude with an update as to how the concluding paragraph looks now:\n\nUsing data from a 2012 survey of Boston-area commuters, we seek to understand the relationship between income and political ideology in Chicago and similar cities in 2020. In particular, what percentage of individuals who make more than $100,000 per year are liberal? The relationship between income and ideology may have changed over that eight year period.\n\nCourage\n\nJustice gave us the Population Table. Courage selects the data generating mechanism. We first specify the mathematical formula which connects the outcome variable we are interested in with the other data that we have. We explore different models. We need to decide which variables to include and to estimate the values of unknown parameters. We check our models for consistency with the data we have. We avoid hypothesis tests. We select one model.\n\nThe structure of the Courage section begins with a version of this opening paragraph, followed by a discussion of the functional form we will be using. This is usually straight-forward because it follows directly from the type of the outcome variable: continuous implies a linear model and binary implies logistic. We provide the mathematical formula for this model, using y and x as variables. The rest of the discussion is broken up into three sections: “Models,” “Data Generating Mechanism,” and “Tests.”\nModels\nWhen exploring different models, we need to decide which variables to include and to estimate the values of unknown parameters. We estimate the models and then print out the model results. We do not give another version of the math, or use tbl_regression() yet. The goal is to explore and interpret different models.\nIf a parameter’s estimated value is more than 2 or 3 MAD_SD’s away from zero, we generally keep that parameter (and its associated variable) in the model. This is, probably, a variable which “matters.” The main exception to this rule is a parameter whose value is so close to zero that changes in its associated variable, within the general range of that variable, can’t change the value of the outcome by much.\nDepending on the chapter, we will use different tools to choose among the different possible models.\nTests\nWe check our models for consistency with the data we have using posterior predictive testing. We avoid hypothesis tests.\nData Generating Mechanism\nWe create a final model, the data generating mechanism. We provide the math for this model, using variable names instead of y and x as we did at the start of the chapter. We present the final parameter estimates nicely, using tbl_regression().\nThe model you have made by the end of Courage is almost always too complex to answer the simple question you started with, because the question rarely specifies the values of all the covariates which are included in the model. But any covariates or treatments which are part of the initial question(s) must be included in the model, otherwise we can’t answer any questions about them at all.\nThe DGM section ends with a clear statement in English, in its own paragraph, describing the model. That is, what are the two sentences which a student would say at a presentation describing the model. First sentence specifies the model, including making clear the units, outcome and key covariates. (No need to use the terms “units,” “outcomes,” and so on.) The second sentence tells us something about the model, generally the relationship between one of the covariates and the outcome variable. In general, there is no discussion af specific numbers or their uncertainty. First, who cares? Parameter estimates are boring and irrelevant. Second, the Temperance section is where we answer the original question. Example:\n\nWe modeled liberal, a binary TRUE/FALSE variable, as a logistic function of income. Individuals with higher income were more likely to be liberal.\n\nUpdate our concluding paragraph with this addition:\n\nUsing data from a 2012 survey of Boston-area commuters, we seek to understand the relationship between income and political ideology in Chicago and similar cities in 2020. In particular, what percentage of individuals who make more than $100,000 per year are liberal? The relationship between income and ideology may have changed over that eight year period. We modeled liberal, a binary TRUE/FALSE variable, as a logistic function of income. Individuals with higher income were more likely to be liberal.\n\nFeel free to use “I” instead of “We” if the project is solo.\nTemperance\n\nCourage produced the data generating mechanism. Temperance guides us in the use of the DGM — or the “model” — we have created to answer the questions with which we began. We create posteriors for the quantities of interest. We should be modest in the claims we make. The posteriors we create are never the “truth.” The assumptions we made to create the model are never perfect. Yet decisions made with flawed posteriors are almost always better than decisions made without them.\n\nThe two sub-sections of Temperance are: Questions and Answers, and Humility.\nIt is important to monitor our language. We do not believe that changes in election_age “cause” changes in lived_after. That is obvious. But there are some words and phrases — like “associated with” and “change by” — which are too close to causal. Be wary of their use. Always think in terms of comparisons when using a predictive model. We can’t change election_age for an individual candidate. We can only compare two candidates (or two groups of candidates).\nQuestions and Answers\nWe go back to the question(s) with which we started the journey. We discuss how that question has evolved, in a back-and-forth process by which we try to ensure that the data we have and the question we ask are close enough to make the process plausible.\nWe revisit the Preceptor Table, at least conceptually. We emphasize that the DGM allows us to fill in missing outcomes in the Preceptor Table, thereby allowing us to answer our questions.\nKey issue is the connection between the DGM (either true or estimated) and the Preceptor Table. The connection is tricky! Not even sure I understand it. The DGM can be used to “fill in” all the missing elements of the Preceptor Table, but there will always be some associated uncertainty. Even with the true DGM, we don’t know what att_end Joe would have had under treatment, we just have a posterior for that variable, a way to make draws.\nIdea: Use the DGM to create one complete Preceptor Table. In that draw, Joe is a 6 for att_end. Then, do another draw. Joe is a 5. Do a thousand draws. You then have a thousand Preceptor Tables. Calculate the Quantity of Interest for each Preceptor Table. The 1,000 values are the posterior for your QoI.\nWould be great to make a cool animation of this, perhaps with a simple example. Would be fun to have a similar animation for each chapter. Great summer project!\nWe use the data generating mechanism from Courage to answer the question. This is, obviously, the core of the Temperance section.\nThe section always concludes with a one sentence summary of our final conclusion. This summary does not include any technical terms. It is meant for non-statisticians. It is something which we might say in explaining our take-away conclusion to a non-statistician. It will always feature at least one number, and our uncertainty associated with that number. Example:\n\n55% (\\(\\pm\\) 2%) of the people who make more than $100,000 per year are liberal.\n\nor\n\nOf the people making $100,000 or more per year, about 55% are liberal, although the true number could be as low as 53% or as high as 57%.\n\nSome cases, like these, feature numbers which have a natural interpretation. We know what percentages are. But, often, the key numbers require some more discussion. For example:\n\nThe causal effect of hearing Spanish-speakers is a more conservative attitude toward immigration, a change of about 1.5 (\\(\\pm\\) 0.5) on a 15 point scale.\n\nThis is correct, as far as it goes, but we have no idea if 1.5 is a “big” or “small” change. We need some perspective&gt;\n\nThe causal effect of hearing Spanish-speakers is a more conservative attitude toward immigration, a change of about 1.5 (\\(\\pm\\) 0.5) on a 15 point scale. For perspective, the difference between Democrats and Republicans on that same scale is about 2.1.\n\nDepending on the context, you might have more than one Quantity of Interest to discuss. But there must be at least one. You are now ready to provide the entire concluding paragraph.\n\nUsing data from a 2012 survey of Boston-area commuters, we seek to understand the relationship between income and political ideology in Chicago and similar cities in 2020. In particular, what percentage of individuals who make more than $100,000 per year are liberal? The relationship between income and ideology may have changed over that eight year period. We modeled liberal, a binary TRUE/FALSE variable, as a logistic function of income. Individuals with higher income were more likely to be liberal. Of the people making $100,000 or more per year, about 55% are liberal, although the true number could be as low as 53% or as high as 57%.\n\nThe final result of data science project is a paragraph like this one. We begin with a question and some data. We produce a paragraph.\nOf course, real data science projects never involve a single question. Instead, the starting question leads you to create a DGM which can answer it but which also can answer lots and lots of other questions. Which is cool. In fact, it is often possible to create a graphic which answers lots of questions at once. That is ideal. (The Michigan postcard example is great.) In fact, a really good data science project always ends with a cool graphic which answers lots of questions and a paragraph like the one above which picks out one answer to highlight.\nThe Questions and Answers section ends with that final paragraph.\nHumility\nTemperance guides us in the use of the DGM to answer the questions with which we began.\nThe Humility section always begins with single sentence, something along the lines of:\nWe can never know the truth.\nOver time, we hope to collect a serious of quotations along this theme.\nHaving answered the question, we now (quickly) review all the reasons why our answer might be wrong. Review the specific concerns we had about validity, stability, representativeness, and (if a causal model) unconfoundedness. Those concerns remain.\nReview the three levels of “truth”: Knowing all the entries in the Preceptor Table, knowing the true DGM, and then using our estimated DGM. (This explanation can become more sophisticated as the chapters progress.)\nWe can never know all the entries in the Preceptor Table. That knowledge is reserved for God. If all our assumptions are correct, then our DGM is true, it accurately describes the way in which the world works. There is no better way to predict the future, or to model the past, than to use it. Sadly, this will only be the case with toy examples involving things like coins and dice. We hope that our DGM is close to the true DGM but, since our assumption are never perfectly correct, our DGM will always be different. The estimated magnitude and importance of that difference is a matter of judgment.\nThe problem with our concluding paragraph is that it implies that our DGM is the truth, rather than just an imperfect approximation of the true DGM. There are two main ways in which are DGM might be wrong. First, the central portion of our estimate, 55% in this case, might be wrong. We might be biased low or high. It is hard to know what to do about that, other than to be aware.\nThe second way that our DGM might be wrong, relative to the true DGM, is that our uncertainty interval, the 4% from 53% to 57%, might be off. It might be too narrow or too wide. In reality, however, it is almost certainly too narrow, relative to the true DGM. Problems with our assumptions, which are inevitable, almost always make our confidence intervals too narrow.\nGiven these concerns, we provide a new final paragraph. This paragraph is just like the one with which we ended the Questions and Answers section, but with (perhaps) a different mean estimate and (almost always) a wider confidence interval.\nIn later chapters we should also estimate a different (plausible) DGM and show the answer it provides to our question. That answer will always be different than the one in the concluding paragraph. (Ideally, we choose a small change in the model which produces a large change in the estimates for the QoI.)\nLast line in every chapter is always: “The world is always more uncertain than our models would have us believe.”",
    "crumbs": [
      "Key Concepts"
    ]
  },
  {
    "objectID": "set-up.html",
    "href": "set-up.html",
    "title": "Set Up for Working on The Primer",
    "section": "",
    "text": "Computer Set Up\nI left out the end of the output.\nIf the first part — Git config — seems messed up, execute (with your information):\nuse_git_config(user.name = \"davidkane9\", user.email = \"dave.kane@gmail.com\")\nIf the second part seems messed up, try:\nusethis::create_github_token()\nand read about Github credentials. After you do, restart R and then run git_sitrep() again to make sure that things look like mine, more or less.\nIt is not critical to understand all the details of how renv works. The big picture is that it creates a set of libraries which will be used just for this project and whose versions are kept in sync between you and me.\nAgain, with luck, you will only have to do these steps once.",
    "crumbs": [
      "Set Up for Working on The Primer"
    ]
  },
  {
    "objectID": "set-up.html#computer-set-up",
    "href": "set-up.html#computer-set-up",
    "title": "Set Up for Working on The Primer",
    "section": "",
    "text": "Read the Getting Started chapter from The Primer. Follow the instructions, including installing Rtools if you are using Windows. Read (and watch the videos from) Getting Used to R, RStudio, and R Markdown by Chester Ismay and Patrick C. Kennedy. Note that R Markdown is the predecessor to Quarto. Check out RStudio Essentials Videos. Most relevant for us are “Writing code in RStudio”, “Projects in RStudio” and “Github and RStudio”. The best reference for R/RStudio/Git/Github issues is always Happy Git and GitHub for the useR.\nMake sure that your Git/Github connections are good. If you have gone through the key chapters in Happy Git with R — as you should have — then these may already be OK. If not (or, even if you have), then you need to run usethis::git_sitrep().\n\n&gt; library(usethis)   \n&gt; git_sitrep()    \nGit config (global)   \n● Name: 'David Kane'   \n● Email: 'dave.kane@gmail.com'   \n● Vaccinated: FALSE   \nℹ See `?git_vaccinate` to learn more   \nℹ Defaulting to https Git protocol   \n● Default Git protocol: 'https'   \nGitHub   \n● Default GitHub host: 'https://github.com'   \n● Personal access token for 'https://github.com': '&lt;discovered&gt;'   \n● GitHub user: 'davidkane9'   \n● Token scopes: 'delete_repo, gist, notifications, repo, user, workflow'   \n● Email(s): 'dave.kane@gmail.com (primary)', 'dkane@fas.harvard.edu'   \n...   \n\n\n\n\n\n\n\nInstall the renv package. Read about the renv package here.\n\n\n\nAt this point, you should have all the tools you need to contribute. If you have never done a pull request, however, you will need to learn more. Start by reading the help page. Read the whole thing! Don’t just skim it. These are important concepts for professional-level workflow. The usethis package is mostly providing wrappers around the underlying git commands. If you want to understand what is happening at a lower level, read this, but doing so is optional.\n\n\n\nProve to yourself (and to me) that your set up is working by submitting a pull request to me which simply adds your name to the top of the primer.tutorials TODO.txt file. (See below for how to do this.) Email me to set up our next meeting after you do this.",
    "crumbs": [
      "Set Up for Working on The Primer"
    ]
  },
  {
    "objectID": "set-up.html#project-set-up",
    "href": "set-up.html#project-set-up",
    "title": "Set Up for Working on The Primer",
    "section": "Project Set Up",
    "text": "Project Set Up\nYou will need to do the below steps at least one time. It is more likely, however, that you will do them dozens of times. If things are working, great! If they start not working, you can try to diagnose the problem. But, if you can’t, then you are in a nuke it from orbit scenario, which means that you start by deleting the current version of the package from two places: your computer and your Github account. To delete a project from your computer, put the R Studio project directory in the Trash. Make sure to also close out of the R Studio session after you delete it. If for some reason you cannot completely remove it, consider using the command $sudo rm -r dirname where you replace dirname with the path to the project on your computer. sudo and rm can be extremely dangerous when used together, so make sure to double check the command and/or do additional research. After you successfully remove the R project from your computer, go to your Github account and then go to Settings to delete the repo.\nKey steps:\n\nFork/download the target repo:\n\n\nlibrary(usethis)  \ncreate_from_github(repo_spec = \"PPBDS/INSERT-NAME-OF-PROJECT\",   \n                   fork = TRUE,   \n                   destdir = \"/Users/davidkane/Desktop/projects/\",   \n                   protocol = \"https\")  \n\nObviously, you need to modify the string “INSERT-NAME-OF-PROJECT” to correspond to one of our projects. Most common options are r4ds.tutorials, primer, primer.data, and primer.tutorials. If, for some reason, you are working on a project which does not reside in the PPBDS organization, you should modify the first part of the repo_spec as well.\nYou must change destdir to be a location on your computer. Indeed, professionals will generally have several different RStudio sessions open, each working on a different R project/package, each of which is connected to its own Github repo.\nFor your education, it is worth reading the help page for create_from_github(). The fork and protocal arguments may not really be necessary and, obviously, you should place the project in the location on your computer in which your other projects live. The command first forks a copy of PPBDS/primer to your Github account and then clone/downloads that fork to your computer.\nThis may seem like overkill, but, as Pro Git explains, it is how (essentially) all large projects are organized. With luck, you only have to issue this command once. After that, you are always connected, both to your fork and to the true repos, which live at github/com/PPBDS. Also, note that, if something ever gets totally messed up on your computer, you can just delete the project folder on your computer and the repo on your Github account and then start again. (If you have made changes that you don’t want to lose, just save the files with those changes to one side and then move them back after you have recreated the project.)\nNote that this command should automatically put you in a new RStudio session with the primer (or primer.tutorials or primer.data) RStudio project which resides on your computer\n\nThe next step is to get renv setup so that you are running the same package versions as everyone else. (This does not apply to the primer.tutorials package, which does not use renv.) Run this once:\n\n\nlibrary(renv)\nrenv::restore()\n\nThis will install all the packages you need in the directory of this project. If this command fails, most commonly with a report about problems with compiling a specific package, notify me right away. If I update the renv.lock file by using renv::update(), the problem will often go away. See the Common Problems section below for further discussion.\nrenv::restore() has no effect on your main library of R packages. Restart your R session. Again, this means that you now have two separate installations of, for example, ggplot2. One is in the default place which your R sessions is by default pointed to. (In a different project without a renv directory, you can run .libPaths() to see where that is.) The second place that ggplot2 is installed is in the renv directory associated with this project.\nNote that, for the most part, you won’t do anything with renv after this initial use. If you use error = TRUE in any code chunk, you will also need renv.ignore = TRUE in that code chunk, or you will get an annoying warning because renv can’t parse the code in that chunk.\nHowever, there are three other renv commands you might issue:\nrenv::status() just reports if anything is messed up. It won’t hurt anything.\nrenv::restore() looks at the renv.lock file and installs the packages it specifies. You will need to do this when I make a change to renv.lock, e.g., if I upgrade our version of ggplot2 or add a new package.\nrenv::snapshot() should only be issued if you know what you are doing. This changes the renv.lock file, which is something that, usually, only I do. Most common case for use would be if you need to add a new package to the project.\n\nCreate a branch to work from:\n\n\npr_init(branch = \"chapter-9\")\n\nMake sure the branch name is sensible. Again, this is a command that you only need to issue once, at least for our current workflow. You should always be “on” this branch, never on the default (master) branch. You can check this in the upper right corner of the git panel on R Studio.\nIn more professional settings, you will often work on several different branches at once. So, if you are comfortable, you should feel free to create more than one branch, use it, delete it and so on. Never work on the default branch, however. And, if you use multiple branches, be careful where you are and what you are doing.",
    "crumbs": [
      "Set Up for Working on The Primer"
    ]
  },
  {
    "objectID": "set-up.html#daily-work",
    "href": "set-up.html#daily-work",
    "title": "Set Up for Working on The Primer",
    "section": "Daily Work",
    "text": "Daily Work\n\nAlways use the “Source” view — not the “Visual” view — in RStudio to edit. The Visual view will often reformat your code, without you asking, in unhelpful ways. You don’t want RStudio to make changes in your code.\nPull regularly:\n\n\npr_merge_main()\n\nIssue this command all the time. This is how you make sure that your repo and your computer is updated with the latest changes that have been made in the book. The word “upstream” is associated with the repos at PPBDS. The word “origin” is associated with the fork at your Github account. But, in general, you don’t need to worry about this. Just pull every time you sit down. (Just clicking the pull button is not enough. That only pulls from your repo, to which no changes have been made. It does not pull from PPBDS/primer, et al.) You issue this command multiple times a day.\n\nMake changes in the file you are editing. Knit to make sure the changes work. Commit with a message. Push to the repo on your Github account. And so on.\n\nAt some point, you will be ready to push to the PPBDS organization. However, you can’t do this directly. Instead, you must submit a pull request (PR). Because you are part of a larger project, these commands are slightly different than what you have done before, which has usually just been clicking on the pull (blue) and push (green) arrows in the Git pane in RStudio.\n\nIssue pull requests every few days, depending on how much work you have done and/or whether other people are waiting for something you have done.\n\n\npr_push()\n\nThis command bundles up a bunch of git commands (which you could do by hand) into one handy step. This command does everything needed to create a “pull request” — a request from you to me that I accept the changes you are proposing into the repo at PPBDS/primer — and then opens up the web page to show you. But your are not done! You must PRESS the green button on that web page, sometimes twice. Until then, the PR has not actually been created. pr_push() just does everything before that. The “pr” in pr_push() stands for pull request.\n\nI will leave aside for now issues associated with the back-and-forth discussions we might have around your pull request. I will probably just accept it. Your changes will go into the repos at PPBDS and then be distributed to everyone else when they run pr_merge_main().\nYou can now continue on. There is no need to wait for me to deal with your pull request. There is no need to fork/clone/download again. You don’t need to create a new branch, although many people do, with a branch name which describes what they are working on now. You just keep editing your files, knitting, and committing then pushing to your forked repo. When you feel you have completed another chunk of work, just run pr_push() again.\nRead the usethis setup help page at least once, perhaps after a week or two of working within this framework. It has lots of good stuff!",
    "crumbs": [
      "Set Up for Working on The Primer"
    ]
  },
  {
    "objectID": "set-up.html#common-problems",
    "href": "set-up.html#common-problems",
    "title": "Set Up for Working on The Primer",
    "section": "Common Problems",
    "text": "Common Problems\n\n\n\nIn the immediate aftermath of this creation process, the blue/green arrows (in the Git panel) for pulling/pushing may be grayed out. This is a sign that the connection between your computer and your forked repo has not “settled in.” (I am not sure of the cause or even if this is the right terminology.) I think that just issuing your first pr_merge_main() fixes it. If not, it always goes away. Until it does, however, you can’t pull/push to your repo. That doesn’t really matter, however, since the key commands you need are pr_merge_main() and pr_push(), both of which always work immediately.\nAfter running pr_merge_main(), you will often see a bunch of files in your Git tab in the top right corner of Rstudio marked with an M (for Modified), including files which you know you did not edit. These are the files that have been updated on the “truth” — on PPBDS/primer, for example — since your last pr_merge_main(). Since you pulled them directly from the PPBDS/primer repo, your forked repo sees all the changes other people have made and thinks that you made them. This is easily fixed, however — just commit all the changes to your forked repo. (Strangely, this seems to not always happen. If you don’t see this effect, don’t worry.)\nAlways run pr_merge_main() before committing a file. Otherwise, you may create lots of merge conflicts. If this happens, save a copy of the file(s) you personally were editing off to the side. Then, nuke it from orbit, following the instructions above. Repeat the Project Set Up process. Then move in your file(s) by hand into the new repo, and commit/push them as normal.\nWhen you submit a pull request to merge your work with the PPBDS repo, it won’t always be smiles and sunshine — every once in a while, you’ll run into merge conflicts. When these arise, it is because two parties work on a file separately and submit conflicting changes. This makes it hard for GitHub to “merge” your version with the other version. When this happens, find multiple adjacent “&gt;”, “&lt;”, and “=” signs in your document — these will show you where the conflicts occur. For more background on merge conflicts, read this.\n\nIf you see the above-mentioned conflicts in your document, do not submit a pull request. This will mess things up. Instead, first, go through your document, and make sure all the weird conflict indicators (&lt;, &gt;, and =) are removed. Second, decide what goes in that space. It might be the stuff you wrote. It might be the other stuff. It might be some combination of the two which you decide on. Whatever happens, you are making an affirmative choice about what should appear in the file at that location. Once all the merge conflicts are fixed, run pr_push() again.\n\n\npr_push() can be tricky. First, note that, if I have not accepted a (p)ull (r)equest which you have submitted, then your PR is still open. You can see it on Github. In fact, you can see all the closed/completed pull requests as well. If, while one PR is still open, you submit another pr_push(), then this will just be added to your current PR. And that is OK! We don’t need it to be separate.\n\nBut even if there is not an open PR, pr_push() can be tricky. The key thing to remember is that you must press a green button on Github for a new PR to be created. Normally, this is easy. Running pr_push() automatically (or perhaps after you run pr_view()) puts you in a browser and brings you to the correct Github page. Press the button and – presto! – you have created a PR. But, sometimes, the web page is different. It actually sends you back to an old pull request. When this happens, you need to click on the “Pull Request” tab above. This will take you to a new page, with a green button labeled “Compare & Pull Request”. Press that button.\n\nIf you end up needing to install a new package — which should be rare — just install it with renv::install()and then type renv::status() to confirm than renv is aware of the change. Then, type renv::snapshot(). This will update the renv.lock file to include the new package. You just commit/push the new version of renv.lock, and that shares the information with everyone else on the project. Never commit/push a modified renv.lock unless you know why it has changed. But, for the most part, leave changes in renv.lock to me.\nBe careful of committing garbage files like “.DS_Store”, which is a file created sometimes. Only commit changes which you understand. In the vast majority of cases your PRs will only involve one or two files.\nIf using Windows, make sure you have RTools installed.\nSometimes, your renv library becomes messed up. Note that nuking from orbit may not fix this because renv installs your packages in some private common area which is not impacted when you reinstall the PPBDS package you are working with. In this case, running renv::rebuild() may help. If not, run renv::diagnostics(), find the path to the renv cache directory and delete everything there by hand. Then, run renv::repair()'.\nWhen using renv::restore(), you will occasionally have a problem in which one package, say aprob, fails to install, thereby aborting the entire run. This can be tricky. The most common reason for this problem is that the renv.lock file specifies an older version of aprob than the current version available on CRAN. Since CRAN does not (?) keep binaries for non-current versions, your computer is forced to compile from source. This often works OK, especially if you are using a Mac. But it often doesn’t work. The easiest approach is to update the renv.lock file to use the latest version of aprob.\n\nComplications can also arise if aprob was, in the last day or two, updated on CRAN. If you go to the page for aprob you can see that the source file is version 1.2.7, or whatever, while the binary versions are still 1.2.6. It takes a day or two for new binaries to be created on CRAN, especially for Windows. Again, situations like this are tricky enough that you should reach out to me for guidance.",
    "crumbs": [
      "Set Up for Working on The Primer"
    ]
  },
  {
    "objectID": "set-up.html#style-guide",
    "href": "set-up.html#style-guide",
    "title": "Set Up for Working on The Primer",
    "section": "Style Guide",
    "text": "Style Guide\n\nSection headings (other than Chapter titles) are in sentence case (with only the first word capitalized, unless it is something always capitalized) rather than title case (in which all words except small words like “the” and “of” are capitalized). Chapter titles are in title case. Headings do not end with a period.\nNever hard code stuff like “A tibble with 336,776 rows and 19 columns.” What happens when you update the data? Instead, calculate all numbers on the fly, with “r scales::comma(x)” whenever x is a number in the thousands or greater. Example: “A tibble with ‘r scales::comma(nrow(x))’ rows and ‘r ncol(x)’ columns.”\n“We” are writing this book.\nPackage names are in bold: ggplot2 is a package for doing graphics. In general, we reserve bolding for package names. Use italics for emphasis in other contexts.\nR code, anything you might type in the console, is always within backticks. Example: mtcars is a built-in data set.\nFunction names always include the parentheses: we write pivot_wider(), not pivot_wider.\nAdd lots of memes and videos and cartoons.\nMake ample use of comments, placed with the handy Command/Ctrl + Shift + / shortcut. These are notes for everyone else working on the chapter, and for future you.\nAll tables should be created with the gt package.\nAll images and gifs are loaded with knitr::include_graphics().\nInterim data sets should be called x or something sensible to the situation, like ch7 for a data set you are working with in Chapter 7. Do not use names like data and df, both of which are R commands.\nStudents are sometimes tentative. Don’t be! Edit aggressively. If you don’t like what is there, delete it. (If I disagree with your decision, I can always get the text back from Github.) Move things around. Make the chapter yours, while keeping to the style of the other chapters. Note that 90% of the prose here was not written by me. Cut anything you don’t like.\nIf you make an mp4, you can convert it to .gif using https://convertio.co/mp4-gif.\nEverything is Bayesian. The confidence interval for a regression means that there is a 95% chance that the true value lies within that interval. Use Rubin Causal Model and potential outcomes to define precisely what “true value” you are talking about. And so on.",
    "crumbs": [
      "Set Up for Working on The Primer"
    ]
  },
  {
    "objectID": "style-guide.html",
    "href": "style-guide.html",
    "title": "Style Guide",
    "section": "",
    "text": "Comments\nInclude comments in your code. Easy-to-understand chunks of code should not have comments. The code is the comment. But other code will merit many, many lines of comments, more lines than the code itself. In a given file, you should have about as many total lines of comments as you have lines of code.\nMake your comments meaningful. They should not be a simple description of what your code does. The best comments are descriptions about why you did what you did and which other approaches you tried or considered. (The code already tells us what you did.) Good comments often have a “Dear Diary” quality: “I did this. Then I tried that. I finally chose this other thing because of reasons X, Y and Z. If I work on this again, I should look into this other approach.” Because of this, the structure is often a paragraph of comments followed by several lines of code.\nEach line of a comment should begin with the comment symbol (a “hash”) followed by a single space: #. Code comments must be separated from code by one empty line on both sides. Format your code comments neatly. Place your cursor in the comment block and hit Ctrl-Shift-/ to format the comment automatically. label your R code chunks, without using weird characters or spaces. download_data is a good R code chunk label. Plot #1 is not.\nSpelling matters. Comments should be constructed as sentences, with appropriate capitalization and punctuation.",
    "crumbs": [
      "Style Guide"
    ]
  },
  {
    "objectID": "style-guide.html#graphics",
    "href": "style-guide.html#graphics",
    "title": "Style Guide",
    "section": "Graphics",
    "text": "Graphics\nUse titles, subtitles, axis labels, captions and so on to make it clear what your graphics mean.\nAnytime you make a graphic without a title (explaining what the graphic is), a subtitle (highlighting a key conclusion to draw), a caption (with some information about the source of the data) and axis labels (with information about your variables), you should justify that decision in a code comment. We (try to) always include these items but there are situations in which doing so makes less sense. Ultimately, these decisions are yours, but the readers of your code (including future-you) should understand your reasoning.\nUse your best judgment. For example, sometimes axis labels are unnecessary. Read Data Visualization: A practical introduction by Kieran Healy for guidance on making high quality graphics.",
    "crumbs": [
      "Style Guide"
    ]
  },
  {
    "objectID": "style-guide.html#formating",
    "href": "style-guide.html#formating",
    "title": "Style Guide",
    "section": "Formating",
    "text": "Formating\nLong Lines\nLimit your code to 80 characters per line. This fits comfortably on a printed page with a reasonably sized font. When calling functions, you can omit the argument names for very common arguments (i.e. for arguments that are used in almost every invocation of the function). Short unnamed arguments can also go on the same line as the function name, even if the whole function call spans multiple lines.\nWhitespace\n|&gt; should always have a space before it, and should usually be followed by a new line. After the first step in the pipe, each line should be indented by two spaces. This structure makes it easier to add new steps (or rearrange existing steps) and harder to overlook a step.\n\nShow the code# Good\n\niris |&gt;\n  select(Species, Sepal.Length) |&gt; \n  summarise(avg = mean(Sepal.Length),\n            .by = Species) |&gt; \n  arrange(avg)\n\n# Bad\n\niris |&gt; select(Species, Sepal.Length) |&gt; summarise(avg = mean(Sepal.Length),\n            .by = Species) |&gt; arrange(avg)\n\n\nggplot2 code is handled in a similar fashion. All commands after the initial invocation of ggplot() are indented.\n\nShow the code# Good\n\ndiamonds |&gt; \n  ggplot(aes(x = depth)) +\n    geom_histogram(bins = 100) +\n    labs(title = \"Distribution of Depth\",\n         x = \"Depth\",\n         y = \"Count\")\n\n# Bad\n\ndiamonds |&gt; \nggplot(aes(x = depth)) +\ngeom_histogram(bins = 100) + labs(title = \"Distribution of Depth\",\n         x = \"Depth\",\n         y = \"Count\")\n\n\nCommas\nAlways put a space after a comma, never before, just like in regular English.\n\nShow the code# Good\n\nx[, 1]\n\n# Bad\n\nx[,1]\nx[ ,1]\nx[ , 1]\n\n\nParentheses\nDo not put spaces inside or outside parentheses for regular function calls.\n\nShow the code# Good\n\nmean(x, na.rm = TRUE)\n\n# Bad\n\nmean (x, na.rm = TRUE)\nmean( x, na.rm = TRUE )\n\n\nInfix operators\nMost infix operators (=, ==, +, -, &lt;-, ~, et cetera) should be surrounded by one space.\n\nShow the code# Good\n\nheight &lt;- (feet * 12) + inches\nmean(x, na.rm = TRUE)\ny ~ a + b\n\n\n# Bad\n\nheight&lt;-feet*12+inches\nmean(x, na.rm=TRUE)\ny~a + b\n\n\nOther operators — like ::, :::, $, @, [, [[, ^, and : — should never be surrounded by spaces.\n\nShow the code# Good\n\nsqrt(x^2 + y^2)\ndf$z\nx &lt;- 1:10\n\n# Bad\n\nsqrt(x ^ 2 + y ^ 2)\ndf $ z\nx &lt;- 1 : 10\n\n\nYou may add extra spaces if it improves alignment of = or &lt;-.\n\nShow the codelist(total = a + b + c,\n     mean = (a + b + c) / n)\n\n\nDo not add extra spaces to places where space is not usually allowed.",
    "crumbs": [
      "Style Guide"
    ]
  },
  {
    "objectID": "style-guide.html#messageswarningserrors",
    "href": "style-guide.html#messageswarningserrors",
    "title": "Style Guide",
    "section": "Messages/Warnings/Errors",
    "text": "Messages/Warnings/Errors\nR messages/warnings/errors should never appear in a submitted document. The right way to deal with these issues is to find out their cause and then fix the underlying problem. Students sometimes use “hacks” to make these messages/warnings/errors disappear. The most common hacks involve using code chunk options like message = FALSE, warning = FALSE, results = \"hide\", include = FALSE and others. Don’t do this, in general. A message/warning/error is worth understanding and then fixing. Don’t close your eyes (metaphorically) and pretend that the problem doesn’t exist. There are some situations, however, in which, no matter what you try, you can’t fix the problem. In those few cases, you can use one of these hacks, but you must make a code comment directly below it, explaining the situation. The only exception is the “setup” chunk (included by default in every new Rmd) which comes with include = FALSE. In that chunk, no explanation is necessary, by convention.",
    "crumbs": [
      "Style Guide"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Barfort, Sebastian, Robert Klemmensen, and Erik Gahner Larsen. 2020.\n“Longevity Returns to Political Office.” Political\nScience Research and Methods. https://doi.org/10.1017/psrm.2019.63.\n\n\nBryan, Jenny. 2019. STAT 545: Data Wrangling, Exploration, and\nAnalysis with r. https://stat545.com/.\n\n\nDiez, David M, Christopher D Barr, and Mine Çetinkaya-Rundel. 2014.\nIntroductory Statistics with Randomization and Simulation.\nFirst. Scotts Valley, CA: CreateSpace Independent Publishing Platform.\nhttps://www.openintro.org/stat/textbook.php?stat_book=isrs.\n\n\nDowney, Allen. 2012. Think Bayes: Bayesian Statistics Made\nSimple. Green Tea Press.\n\n\nEnos, Ryan D. 2014. “Causal Effect of Intergroup Contact on\nExclusionary Attitudes.” Proceedings of the National Academy\nof Sciences 111 (10): 3699–3704. https://doi.org/10.1073/pnas.1317670111.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and\nOther Stories. Analytical Methods for Social Research. Cambridge\nUniversity Press. https://doi.org/10.1017/9781139161879.\n\n\nGrolemund, Garrett, and Hadley Wickham. 2017. R for Data\nScience. First. Sebastopol, CA: O’Reilly Media. https://r4ds.had.co.nz/.\n\n\nIrizarry, Rafael A. 2019. Introduction to Data Science: Data\nAnalysis and Prediction Algorithms with r. First. Boca Raton, FL:\nCRC Press.\n\n\nKim, Albert Y., and Chester Ismay. 2019. Statistical Inference via\nData Science: A ModernDive into r and the Tidyverse. First. Boca\nRaton, FL: CRC Press.\n\n\nKuhn, Max, and Julia Silge. 2020. Tidy Modeling with r.\n\n\nLegler, Julie, and Paul Roback. 2019. Broadening Your Statistical\nHorizons: Generalized Linear Models and Multilevel Models.\n\n\nTimbers, Tiffany-Anne, Trevor Campbell, and Melissa Lee. 2021. Data\nScience: A First Introduction. https://ubc-dsci.github.io/introduction-to-datascience/.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "key-concepts.html#question",
    "href": "key-concepts.html#question",
    "title": "Cardinal Virtues",
    "section": "",
    "text": "What is the relationship between income and political ideology?\n\n\n\n\nWhat proportion of people who make $100,000 are liberal?",
    "crumbs": [
      "Cardinal Virtues"
    ]
  },
  {
    "objectID": "key-concepts.html#wisdom",
    "href": "key-concepts.html#wisdom",
    "title": "Cardinal Virtues",
    "section": "Wisdom",
    "text": "Wisdom\n\nWisdom requires the creation of a Preceptor Table, an examination of our data, and a determination, using the concept of validity, as to whether or not we can (reasonably!) assume that the two come from the same population.\n\nWisdom is the first Cardinal Virtue in data science. Begin with the quantity of interest. Is that QoI a causal effect or simply a forecast? Which units and outcomes does it imply? What Preceptor Table would allow you to calculate your QoI easily? Perform an exploratory data analysis (EDA) on the data you have. Is it valid to consider the data you have and the (theoretical) data from the Preceptor Table to have arisen out of the same population? If so, you may continue. If not, your attempt to estimate your QoI ends now.\nPreceptor Table\n\nA Preceptor Table is smallest possible table with rows and columns such that, if there is no missing data, our question is easy to answer.\n\nPredictive Models and Causal Models are different because predictive models have only one outcome column. Causal models have more than one (potential) outcome column because we need more than one potential outcome in order to estimate a causal effect. The first step in a data science problem is to determine if your QoI requires a causal or a predictive model.\nUnits are determined by the original question, which also determines the QoI. They are the rows, both in the Preceptor Table and in the data.\nVariables is the general term for the columns in both the Preceptor Table and the data. In fact, the term is even more general since it may refer to data vectors which we would like to have in order to answer the question but which are, sadly, not available in the data. The columns in the data are a subset of all the variables in which we might be interested.\nThe outcome is the most important variable. It is determined by the question/QoI. By definition, it must be present in both the data and the Preceptor Table. Different problems might be answered with the same data set, with different variables playing the role of the outcome in each case.\nCovariates is the general term for all the variables which are not the outcome. As with variables, there are three different contexts in which we might use the term covariates. First, covariates are all the variables which might have some connection with our outcome, even if they are not included in the data. Second, covariates are all the variables in the data other than the outcome. Third, covariates can refer to just the subset of the variables in the data which we actually use in our model. The second usage is, obviously, a subset of the first, and the third usage is a subset of the second.\nUnits, outcomes and covariates are important parts of every data science model. Causal, but not predictive, models also include at least one treatment, which is just a covariate which we can, at least in theory, manipulate. The QoI determines the units and outcomes for your model.\nPotential Outcome is the outcome for an individual under a specified treatment. A potential outcome is just a regular outcome in the case of a causal model. In a predictive model, we just have an outcome. It is just another variable, the one that, in the context of this problem, we are interested in explaining/modeling/predicting. In a casual model, on the other hand, there are at least two outcomes: the outcome which happens if the unit gets the treatment and the outcome which happens if that same unit gets the control. We refer to both of these outcomes as potential outcomes.\nTo create the Preceptor Table, we answer a series of questions. (Don’t ask these questions rhetorically. Just describe the answer. There is also no need to number them, although you should always use this order.)\nCausal Effect is the difference between two potential outcomes.\nRubin Causal Model is an approach to the statistical analysis of cause and effect based on the framework of potential outcomes.\n\nIs the question causal? Look for verbs like “cause” or “affect” or “influence.” Look for a question which implies a comparison, for a single individual unit, between two states of the world, one in which the unit receives treatment \\(X\\) and one in which the unit gets treatment \\(Y\\). Look for a discussion of something which we can manipulate. Remember the motto: No causation without manipulation. We look to see if the question seeks to compare two potential outcomes within the same unit, rather than the same outcome between two different units.\n\nIf none of this is present, use a predictive model. If all you need to know to answer the question is the outcome under one value of the treatment, then the model is predictive. In that case, the treatment is not truly a “treatment.” It is just a covariate. Example: What is the att_end for all women if they were to get the treatment? This is a predictive question, not a causal one, because we do not need to know the outcome under treatment and under control for any individual woman.\n\nWhat is the moment in time to which the question refers? Every question refers to a moment in time, even if that moment stretches a bit. The set of adults today is different from the set 10 years ago, or even yesterday. We need to refine the original question. Assume that we are referring to July 1, 2020 even though, in most cases, people are interested in now. We have changed the original question from:\n\n\nWhat proportion of people who make $100,000 are liberal?\n\nto\n\nOn July 1, 2020, what proportion of people who made $100,000 were liberal?\n\n\nWhat are the units? The question often makes this fairly clear, at least in terms of what each row corresponds to, whether it be individuals, classrooms, countries, or whatever. But, questions often fail to make clear the total number of the rows. Our example question above does not specify the relevant population. Is it about all the people in the world? All the adults? All the adults in the United States? The purpose of this paragraph is to refine the question, to make it more specific. Assume that we are interested in all the adults in Chicago. Our question now is:\n\n\nOn July 1, 2020, what proportion of the adults in Chicago who made $100,000 were liberal?\n\nThis back-and-forth between the question and the analysis is a standard part of data science. We rarely answer the exact question we started with, especially because that question is never specific enough to answer without further qualifications. Furthermore, the data we have may not allow us to answer that question, but it may be enough to answer a related question. Is that good enough for the boss/client/colleague who asked the original question? Maybe? You won’t know until you ask.\nOur job as data scientists is not to simply answer the question we have been asked, but to help the questioner determine a question which can be answered with the data we have, a question which helps them to make the decision which they face.\n\nWhat are the outcomes? (If the model is causal, then there must be at least two potential outcomes. If you can’t figure them out, then the model is probably predictive.) If the model is predictive, then there is only one outcome. This paragraph does more than just name the relevant variable. It also starts the discussion about how exactly we might measure this variable. We consider both the underlying concept, “liberal,” and the process by which we might operationalize the concept. Perhaps we are using a written survey with a YES/NO answer. Perhaps it is an in-person interview with a 1-7 Likert scale, in which answers of 1 or 2 are coded, by us, as “liberal.” The details may or may not matter, but we at least need to discuss the issue.\n\nWhat are the covariates? Discussing covariates in the context of the Preceptor Table is different than discussing covariates in the context of the data. Recall that the Preceptor Table is the smallest possible table, so we don’t need to include every relevant variable. We only need to discuss variables that are necessary to answer the question.\nWhat are the treatments, if any? (There are no “treatments” in predictive models. There are only covariates.) A treatment is a covariate which, at least in theory, we can manipulate and the manipulation of which is necessary to answer our question.\nWith all the above, create the Preceptor Table. In this case, our Preceptor Table includes N rows, one for every adult in Chicago on July 1, 2020. It includes two columns: the outcome (liberal) and a single covariate (income).\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\nOutcome\nCovariate\n\n\nLiberal\nIncome\n\n\n\n\n1\n0\n150000\n\n\n2\n0\n50000\n\n\n…\n…\n…\n\n\n10\n1\n65000\n\n\n11\n1\n35000\n\n\n…\n…\n…\n\n\nN\n1\n78000\n\n\n\n\n\n\n\nIf we have the Preceptor Table, with no missing data, then it is trivial to calculate the percentage of adults (who make more than $100,000) who are liberal.\nEDA\n\nYou can never look at the data too much.\n\nThere is always short section devoted to exploratory data analysis. Each EDA will include at least one textual look at the data, usually using summary(), but with skim(), glimpse(), print() and slice_sample() also available. It will also include at least one graphic, almost always with the outcome variable on the y-axis and one of the covariates on the x-axis. The data set will often include columns and rows which are irrelevant to the question. Those columns and rows are removed, creating a tibble which will be used in the Courage section. The name of that tibble will often be something convenient like ch_7.\nIt also makes sense to include some discussion about where this data comes from. What are the definitions of the variables? Who chose the sample? Where is the documentation? This sort of background sets the stage for examining validity.\nValidity\nValidity is the consistency, or lack thereof, in the columns of your data set and the corresponding columns in your Preceptor Table. In order to consider the two data sets to be drawn from the same population, the columns from one must have a valid correspondence with the columns in the other. Validity, if true (or at least reasonable), allows us to construct the Population Table, which is the first step in Justice.\nValidity discussions always have one (short) paragraph about each relevant variable (the outcome and any relevant covariates), with examples of why validity might not hold. Validity discussion finishes with a brief discussion along the lines of: “Despite these concerns, we will assume that validity does hold.”\nThese section can be longer of course, depending on how many details you discussed during the EDA. The central point is that we have two (potentially!) completely different things: the Preceptor Table and the data. Just because two columns have the same name does not mean that they are the same thing. Indeed, they will often be quite different! But because we control the Preceptor Table and, to a lesser extent, the original question, we can adjust those variables to be “closer” to the data that we actually have. This is another example of the iterative nature of data science. If the data is not close enough to the question, then we check with our boss/colleague/customer to see if we can modify the question in order to make the match between the data and the Preceptor Table close enough for validity to hold.\nWe conclude the Wisdom section by summarizing how we hope to use the data we have to answer the question we started with. Example:\n\nUsing data from a 2012 survey of Boston-area commuters, we seek to understand the relationship between income and political ideology in Chicago and similar cities in 2020. In particular, what percentage of individuals who make more than $100,000 per year are liberal?\n\nNote how the specific question has morphed into a general examination of the “relationship” between income and political ideology. In order to answer any specific question, we always have to examine a more general relationship. We always have to build a model. We can then use this model to answer both the question we started with as well as other related questions.\nBy thinking hard about the original question and the data, we have come up with a question which may be possible to answer with the data we have. Note that each Cardinal Virtue section finishes with a sentence or two summarizing what you have learned. Those sentences are combined at the end of the analysis. One of the key products of a data science project is a paragraph which summarizes the key conclusions.",
    "crumbs": [
      "Cardinal Virtues"
    ]
  },
  {
    "objectID": "key-concepts.html#justice",
    "href": "key-concepts.html#justice",
    "title": "Cardinal Virtues",
    "section": "Justice",
    "text": "Justice\n\nJustice concerns four topics: the Population Table, stability, representativeness, and unconfoundedness.\n\nJustice is the second Cardinal Virtue in data science. Justice starts with the Population Table – the data we want to have, the data which we actually have, and all the other data from that same population. Each row of the Population Table is defined by a unique unit/time combination. We explore three key issues. First, does the relationship among the variables demonstrate stability, meaning is the model stable across different time periods? Second, are the rows associated with the data and, separately, the rows associated with the Preceptor Table, representative of all the units from the population? Third, for causal models only, we consider unconfoundedness.\nPopulation Table\nThe Population Table includes a row for each unit/time combination in the underlying population from which both the Preceptor Table and the data are drawn. It can be constructed if the validity assumption is (mostly) true. It includes all the rows from the Preceptor Table. It also includes the rows from the data set. It usually has other rows as well, rows which represent unit/time combinations from other parts of the population.\nIf validity holds, then we can create a Population Table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation Table\n\n\nSource\nYear\nOutcome\nCovariates\n\n\nIncome\nAge\nCity\n\n\n\n\n…\n…\n…\n…\n…\n\n\nData\n2012\n150000\n43\nBoston\n\n\nData\n2012\n50000\n52\nBoston\n\n\n…\n…\n…\n…\n…\n\n\n…\n…\n…\n…\n…\n\n\nPreceptor Table\n2020\n…\n…\nChicago\n\n\nPreceptor Table\n2020\n…\n…\nChicago\n\n\n…\n…\n…\n…\n…\n\n\n\n\n\n\n\n\nThe “Source” column highlights that the Population Table includes three categories of rows: The data, the Preceptor Table, and the rest of the population, from which both the data and the Preceptor Table are drawn. The ... indicates rows from the population which are not included in either the data or the Preceptor Table.\nThe “ID” column is implicit, and often not included. After all, it should be obvious that each row refers to a specific unit. If we don’t really care about the individual units, there is no need to label them.\nThere should always be a column, in this case “Year,” which indicates the moment in time at which the covariates were recorded. A given unit may appear in multiple rows, with each row providing the data at a different time. In this example, we will have a row for Sarah in 2012, when she was 43, and a row for Sarah in 2020, when she was 51, and so on. Note that Sarah might just be member of the population, neither in the data we have nor in the Preceptor Table. Or she might be in one or the other. We are rarely concerned with any specific individual.\nEach row in the Population Table represents a unique Unit/Time combination.\nThe “Outcome” column is the variable which we are trying to understand/explain/predict. There is always an outcome column, although it will often just be labelled with the variable name, as here with “Income.”\nThe “Covariates” are all the columns other than those already discussed.\nStability\nStability means that the relationship between the columns in the Population Table is the same for three categories of rows: the data, the Preceptor Table, and the larger population from which both are drawn.\nIf the assumption of stability holds, then the relationships between the columns in the Population Table is the same across time. First, the relationship among columns from the same moment in time as the data is the same as the relationship among columns for the entire table. Second, the relationship among columns from the same moment in time as the Preceptor Table is the same as the relationship among columns for the entire table.\nStability, if true, allows is us to go from the data to the population, and from the population to the Preceptor Table.\nWe discuss at one example of why stability might not hold in this case. These examples are almost always connected to the passage of time. Whatever the relationship between political ideology and income that might have held in 2012, when we gathered our data, might not be true either before or afterwards. Provide specific speculations about what might have changed in the world.\nRegardless of those concerns, we always conclude that, although the assumption of stability might not hold perfectly, the world is probably stable enough over this time period to make inference possible.\nThe longer the time period covered by the Preceptor Table (and the data), the more suspect the assumption of stability becomes.\nRepresentativeness\nRepresentativeness, or the lack thereof, concerns two relationship, among the rows in the Population Table. The first is between the Preceptor Table and the other rows. The second is between our data and the other rows. Ideally, we would like both the Preceptor Table and our data to be random samples from the population. Sadly, this is almost never the case.\nAs with all our assumptions, we always begin by restating the definition. Representativeness, or the lack thereof, concerns two relationship, among the rows in the Population Table. The first is between the Preceptor Table and the other rows from the same moment in time. The second is between our data and the other rows of equivalent vintage.\nStability looks across time periods. Reprentativeness looks within time periods.\nWe mention specific examples of two potential problems. First, is our data representative of the population? Rarely! Second, are the rows associated with the Preceptor Table representative of the population? Again, almost never!\nProvide specific examples of how a lack of representativeness might be a problem, one large enough to affect your ability to answer the question.\nBut, to continue the analysis, we always assume/pretend that the rows from both the data and the Preceptor Table are representative enough of the relevant time period from within the larger population from which both are drawn.\n\nThe more expansive your Preceptor Table, the more important the assumption of representativeness becomes.\nUnconfoundedness\nUnconfoundedness means that the treatment assignment is independent of the potential outcomes, when we condition on pre-treatment covariates. This assumption is only relevant for causal models. We describe a model as “confounded” if this is not true. The easiest way to ensure unconfoundedness is to assign treatment randomly.\nIf the model is predictive, then unconfoundedness is not a concern. Just mention that fact in a sentence at the end of the section on representativeness. But, if the model is causal, then we need a section devoted to this topic.\nAs always, start by restating the definition, something along the lines of “Unconfoundedness means that the treatment assignment is independent of the potential outcomes, when we condition on pre-treatment covariates. A model is confounded if this is not true.”\nIf treatment assignment was random, then unconfoundedness is guaranteed, although experienced researchers often worry about the exact process involved in such “random” assignment. If, however, treatment assignment was not random, then there will always be a concern that it is correlated with potential outcomes. Discuss at least two scenarios in which this might be a concern. But then, as usual, conclude that, although there might be some issues with confoundedness, they are probably small enough to not worry about.\nJust because Wisdom points us toward a Population Table with X rows does not mean we need to keep all X rows, especially if creating a model which covers all rows is hard/impossible. We can just simplify the claims we are making about the world by removing some rows. Getting rid of rows will usually necessitate an adjustment to the question we are trying to answer. Again, data science is an iterative process.\nThe Justice section concludes with a sentence or two about how, despite any problems with the core assumptions of stability, representativeness and unconfoundedness, we can still proceed to next steps because the assumptions hold enough.\nThe last step may be to revisit the key sentences from the Wisdom section. Recall:\n\nUsing data from a 2012 survey of Boston-area commuters, we seek to understand the relationship between income and political ideology in Chicago and similar cities in 2020. In particular, what percentage of individuals who make more than $100,000 per year are liberal?\n\nAre these sentences still correct, or does a serious consideration of the assumptions of stability, representativeness and unconfoundedness require us to modify them? The answer, of course, is that the assumptions are never perfect! So, we have an obligation to add a sentence or two which highlights (no more than) one or two concerns. Examples:\n\nThere is some concern that survey participants may not be perfectly representative of the underlying population.\n\n\nThe relationship between income and ideology may have changed over that eight year period.\n\nThere is no need to use technical terms like “stability.” However, most readers will understand what “representative” means. The key point is honesty. We have an obligation to at least mention some possible concerns. At the end, we conclude with an update as to how the concluding paragraph looks now:\n\nUsing data from a 2012 survey of Boston-area commuters, we seek to understand the relationship between income and political ideology in Chicago and similar cities in 2020. In particular, what percentage of individuals who make more than $100,000 per year are liberal? The relationship between income and ideology may have changed over that eight year period.",
    "crumbs": [
      "Cardinal Virtues"
    ]
  },
  {
    "objectID": "key-concepts.html#courage",
    "href": "key-concepts.html#courage",
    "title": "Cardinal Virtues",
    "section": "Courage",
    "text": "Courage\n\nCourage begins with the exploration and testing of different models. It concludes with the creation of a Data Generating Mechanism.\n\nCourage is the third Cardinal Virtue in data science. Justice gives us the Population Table. Courage creates the data generating mechanism. We first specify the mathematical formula which connects the outcome variable we are interested in with the other data that we have. We explore different models. We need to decide which variables to include and to estimate the values of unknown parameters. We check our models for consistency with the data we have. We avoid hypothesis tests. We select one model, the data generating mechanism.\nCourage begins by a discussion of the functional form we will be using. This is usually straight-forward because it follows directly from the type of the outcome variable: continuous implies a linear model and binary implies logistic. We provide the mathematical formula for this model, using y and x as variables. The rest of the discussion is broken up into three sections: “Models,” “Tests,” and “Data Generating Mechanism.”\nModels\nWhen exploring different models, we need to decide which variables to include and to estimate the values of unknown parameters. We estimate the models and then print out the model results. We do not give another version of the math, or use tbl_regression() yet. The goal is to explore and interpret different models.\nIf a parameter’s estimated value is more than 2 or 3 standard errors away from zero, we generally keep that parameter (and its associated variable) in the model. This is, probably, a variable which “matters.” The main exception to this rule is a parameter whose value is so close to zero that changes in its associated variable, within the general range of that variable, can’t change the value of the outcome by much.\nDepending on the chapter, we will use different tools to choose among the different possible models.\nTests\nWe check our models for consistency with the data we have using posterior predictive testing. We avoid hypothesis tests.\nData Generating Mechanism\nData Generating Mechanism (DGM) is also called the data generating model or the data generating process. The true DGM is the reality of the world, the physical process which actually generates the data which we observe. The estimated DGM is the mathematical formula we create which models the true DGM, which we can never know. In Temperance, we will use the estimated DGM to draw inferences about our Quantities of Interest.\nWe create a final model, the data generating mechanism. We provide the math for this model, using variable names instead of y and x as we did at the start of the chapter. We present the final parameter estimates nicely, using the gtsummary package.\nThe model you have made by the end of Courage is almost always too complex to answer the simple question you started with, because the question rarely specifies the values of all the covariates which are included in the model. But any covariates or treatments which are part of the initial question(s) must be included in the model, otherwise we can’t answer any questions about them at all.\nThe DGM section ends with a clear statement in English, in its own paragraph, describing the model. That is, what are the two sentences which a student would say at a presentation describing the model. The first sentence specifies the model, including making clear the units, outcome and key covariates. (No need to use the terms “units,” “outcomes,” and so on.) The second sentence tells us something about the model, generally the relationship between one of the covariates and the outcome variable. In general, there is no discussion af specific numbers or their uncertainty. First, who cares? Parameter estimates are boring and irrelevant. Second, the Temperance section is where we answer the original question. Example:\n\nWe modeled liberal, a binary TRUE/FALSE variable, as a logistic function of income. Individuals with higher income were more likely to be liberal.\n\nUpdate our concluding paragraph with this addition:\n\nUsing data from a 2012 survey of Boston-area commuters, we seek to understand the relationship between income and political ideology in Chicago and similar cities in 2020. In particular, what percentage of individuals who make more than $100,000 per year are liberal? The relationship between income and ideology may have changed over that eight year period. We modeled liberal, a binary TRUE/FALSE variable, as a logistic function of income. Individuals with higher income were more likely to be liberal.\n\nFeel free to use “I” instead of “We” if the project is solo.",
    "crumbs": [
      "Cardinal Virtues"
    ]
  },
  {
    "objectID": "key-concepts.html#temperance",
    "href": "key-concepts.html#temperance",
    "title": "Cardinal Virtues",
    "section": "Temperance",
    "text": "Temperance\n\nTemperance uses the Data Generating Mechanism to answer the question with which we began. Humility reminds us that this answer is always a lie.\n\nTemperance is the fourth Cardinal Virtue in data science. Courage gave us the data generating mechanism. Temperance guides us in the use of the DGM — or the “model” — we have created to answer the questions with which we began. We create posteriors for the quantities of interest. We should be modest in the claims we make. The posteriors we create are never the “truth.” The assumptions we made to create the model are never perfect. Yet decisions made with flawed posteriors are almost always better than decisions made without them.\nThe two sub-sections of Temperance are: Questions and Answers, and Humility.\nIt is important to monitor our language. We do not believe that changes in election_age “cause” changes in lived_after. That is obvious. But there are some words and phrases — like “associated with” and “change by” — which are too close to causal. Be wary of their use. Always think in terms of comparisons when using a predictive model. We can’t change election_age for an individual candidate. We can only compare two candidates (or two groups of candidates).\nQuestions and Answers\nWe go back to the question(s) with which we started the journey. We discuss how that question has evolved, in a back-and-forth process by which we try to ensure that the data we have and the question we ask are close enough to make the process plausible.\nWe revisit the Preceptor Table, at least conceptually. We emphasize that the DGM allows us to fill in missing outcomes in the Preceptor Table, thereby allowing us to answer our questions.\nKey issue is the connection between the DGM (either true or estimated) and the Preceptor Table. The connection is tricky! Not even sure I understand it. The DGM can be used to “fill in” all the missing elements of the Preceptor Table, but there will always be some associated uncertainty. Even with the true DGM, we don’t know what att_end Joe would have had under treatment, we just have a posterior for that variable, a way to make draws.\nIdea: Use the DGM to create one complete Preceptor Table. In that draw, Joe is a 6 for att_end. Then, do another draw. Joe is a 5. Do a thousand draws. You then have a thousand Preceptor Tables. Calculate the Quantity of Interest for each Preceptor Table. The 1,000 values are the posterior for your QoI.\nWould be great to make a cool animation of this, perhaps with a simple example. Would be fun to have a similar animation for each chapter. Great summer project!\nWe use the data generating mechanism from Courage to answer the question. This is, obviously, the core of the Temperance section.\nThe section always concludes with a one sentence summary of our final conclusion. This summary does not include any technical terms. It is meant for non-statisticians. It is something which we might say in explaining our take-away conclusion to a non-statistician. It will always feature at least one number, and our uncertainty associated with that number. Example:\n\n55% (\\(\\pm\\) 2%) of the people who make more than $100,000 per year are liberal.\n\nor\n\nOf the people making $100,000 or more per year, about 55% are liberal, although the true number could be as low as 53% or as high as 57%.\n\nSome cases, like these, feature numbers which have a natural interpretation. We know what percentages are. But, often, the key numbers require some more discussion. For example:\n\nThe causal effect of hearing Spanish-speakers is a more conservative attitude toward immigration, a change of about 1.5 (\\(\\pm\\) 0.5) on a 15 point scale.\n\nThis is correct, as far as it goes, but we have no idea if 1.5 is a “big” or “small” change. We need some perspective&gt;\n\nThe causal effect of hearing Spanish-speakers is a more conservative attitude toward immigration, a change of about 1.5 (\\(\\pm\\) 0.5) on a 15 point scale. For perspective, the difference between Democrats and Republicans on that same scale is about 2.1.\n\nDepending on the context, you might have more than one Quantity of Interest to discuss. But there must be at least one. You are now ready to provide the entire concluding paragraph.\n\nUsing data from a 2012 survey of Boston-area commuters, we seek to understand the relationship between income and political ideology in Chicago and similar cities in 2020. The relationship between income and ideology may have changed over that eight year period. We modeled liberal, a binary TRUE/FALSE variable, as a logistic function of income. Individuals with higher income were more likely to be liberal. Of the people making $100,000 or more per year, about 55% are liberal, although the true number could be as low as 53% or as high as 57%.\n\nNote that we have deleted the rhetorical question — “In particular, what percentage of individuals who make more than $100,000 per year are liberal?” — from the start of the paragraph. It is no longer necessary.\nThe final result of data science project is a paragraph like this one. Data science begins with a question and some data. It ends with a paragraph and, ideally, some graphics.\nOf course, real data science projects never involve a single question. Instead, the starting question leads you to create a DGM which can answer it but which also can answer lots and lots of other questions. Which is cool. In fact, it is often possible to create a graphic which answers lots of questions at once. That is ideal. (The Michigan postcard example is great.)\nA really good data science project always ends with a cool graphic which answers lots of questions and a paragraph like the one above which picks out one answer to highlight.\nThe Questions and Answers section ends with that final paragraph.\nHumility\nTemperance guides us in the use of the DGM to answer the questions with which we began.\nThe Humility section always begins with single sentence, something along the lines of:\nWe can never know the truth.\nOver time, we hope to collect a serious of quotations along this theme.\nHaving answered the question, we now (quickly) review all the reasons why our answer might be wrong. Review the specific concerns we had about validity, stability, representativeness, and (if a causal model) unconfoundedness. Those concerns remain.\nReview the three levels of “truth”: Knowing all the entries in the Preceptor Table, knowing the true DGM, and then using our estimated DGM. (This explanation can become more sophisticated as the chapters progress.)\nWe can never know all the entries in the Preceptor Table. That knowledge is reserved for God. If all our assumptions are correct, then our DGM is true, it accurately describes the way in which the world works. There is no better way to predict the future, or to model the past, than to use it. Sadly, this will only be the case with toy examples involving things like coins and dice. We hope that our DGM is close to the true DGM but, since our assumption are never perfectly correct, our DGM will always be different. The estimated magnitude and importance of that difference is a matter of judgment.\nThe problem with our concluding paragraph is that it implies that our DGM is the truth, rather than just an imperfect approximation of the true DGM. There are two main ways in which are DGM might be wrong. First, the central portion of our estimate, 55% in this case, might be wrong. We might be biased low or high. It is hard to know what to do about that, other than to be aware.\nThe second way that our DGM might be wrong, relative to the true DGM, is that our uncertainty interval, the 4% from 53% to 57%, might be off. It might be too narrow or too wide. In reality, however, it is almost certainly too narrow, relative to the true DGM. Problems with our assumptions, which are inevitable, almost always make our confidence intervals too narrow.\nGiven these concerns, we provide a new final paragraph. This paragraph is just like the one with which we ended the Questions and Answers section, but with (perhaps) a different mean estimate and (almost always) a wider confidence interval.\nIn later chapters we should also estimate a different (plausible) DGM and show the answer it provides to our question. That answer will always be different than the one in the concluding paragraph. (Ideally, we choose a small change in the model which produces a large change in the estimates for the QoI.)\nLast line in every chapter is always: “The world is always more uncertain than our models would have us believe.”",
    "crumbs": [
      "Cardinal Virtues"
    ]
  }
]